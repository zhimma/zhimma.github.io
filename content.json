{"meta":{"title":"zhimma's blog","subtitle":null,"description":"zhimma's 技术blog","author":"zhimma","url":"https://blog.zhimma.com"},"pages":[{"title":"about","date":"2019-01-24T02:03:56.000Z","updated":"2019-01-24T02:03:56.157Z","comments":true,"path":"about/index.html","permalink":"https://blog.zhimma.com/about/index.html","excerpt":"","text":""},{"title":"categories","date":"2019-01-24T02:03:44.000Z","updated":"2019-01-24T09:08:04.758Z","comments":true,"path":"categories/index.html","permalink":"https://blog.zhimma.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-01-24T02:03:50.000Z","updated":"2019-01-24T09:08:03.439Z","comments":true,"path":"tags/index.html","permalink":"https://blog.zhimma.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"项目压测及调优","slug":"项目压测及调优","date":"2019-12-02T03:56:48.000Z","updated":"2019-12-02T03:58:32.914Z","comments":true,"path":"2019/12/02/项目压测及调优/","link":"","permalink":"https://blog.zhimma.com/2019/12/02/项目压测及调优/","excerpt":"","text":"[TOC] 性能概念TPS、QPS、RPSTPSTPS：Transactions Per Second（每秒事务处理数），指服务器每秒处理的事务次数。一般用于评估数据库、交易系统的基准性能。 QPSQPS：Queries Per Second（查询量/秒），是服务器每秒能够处理的查询次数，例如域名服务器、Mysql查询性能。 RPSRPS：Request Per Second（请求数/秒）RPS（Request Per Second）和QPS可以认为是一回事。RT：Response Time（响应时间）：客户端发一个请求开始计时，到客户端接收到从服务器端返回的响应结果结束所经历的时间，响应时间由请求发送时间、网络传输时间和服务器处理时间三部分组成。也叫Think Time。 并发数与TPS/QPS的关系QPS（TPS）= 并发数/平均响应时间这里的并发数如果为事务处理请求数，则为TPS，如果为查询请求数，则为QPS。 服务器状态https://blog.csdn.net/u011636440/article/details/78611838 操作系统查看当前操作系统发行版信息 12[root@VM_25_122_centos conf.d]# cat /etc/redhat-releaseCentOS Linux release 7.7.1908 (Core) CPU123456789101112131415161718192021222324[root@VM_25_122_centos conf.d]# lscpuArchitecture: x86_64CPU op-mode(s): 32-bit, 64-bitByte Order: Little EndianCPU(s): 8On-line CPU(s) list: 0-7Thread(s) per core: 1Core(s) per socket: 8座： 1NUMA 节点： 1厂商 ID： GenuineIntelCPU 系列： 6型号： 79型号名称： Intel(R) Xeon(R) CPU E5-26xx v4步进： 1CPU MHz： 2394.446BogoMIPS： 4788.89超管理器厂商： KVM虚拟化类型： 完全L1d 缓存： 32KL1i 缓存： 32KL2 缓存： 4096KNUMA 节点0 CPU： 0-7Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx lm constant_tsc rep_good nopl eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch bmi1 avx2 bmi2 rdseed adx xsaveopt 查看物理CPU个数12[root@VM_25_122_centos conf.d]# cat /proc/cpuinfo| grep &quot;physical id&quot;| sort| uniq| wc -l1 查看每个物理CPU中core的个数(即核数)12[root@VM_25_122_centos conf.d]# cat /proc/cpuinfo| grep &quot;cpu cores&quot;| uniqcpu cores : 8 查看逻辑CPU的个数12[root@VM_25_122_centos conf.d]# cat /proc/cpuinfo| grep &quot;processor&quot;| wc -l8 内存查看概要内存使用情况1234[root@VM_25_122_centos conf.d]# free -g total used free shared buff/cache availableMem: 15 1 9 0 4 13Swap: 0 0 0 -g是以GB为单位；也可以使用-m，即以MB为单位 压测和优化未优化压测压测结果预览 PHP-FPM配置 wrk压测结果 Nginx报错 优化PHP-FPMPHP-FPM优化(示例：这是另一台服务器) USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND 格式说明： USER: 行程拥有者 PID: pid %CPU: 占用的 CPU 使用率 %MEM: 占用的记忆体使用率 VSZ: 占用的虚拟记忆体大小 RSS: 占用的记忆体大小 TTY: 终端的次要装置号码 (minor device number of tty) STAT: 该行程的状态，linux的进程有5种状态： ​ D 不可中断 uninterruptible sleep (usually IO) ​ R 运行 runnable (on run queue) ​ S 中断 sleeping ​ T 停止 traced or stopped ​ Z 僵死 a defunct (”zombie”) process 注: 其它状态还包括W(无驻留页), &lt;(高优先级进程), N(低优先级进程), L(内存锁页). START: 行程开始时间 TIME: 执行的时间 COMMAND:所执行的指令 查看进程的消息ps auxf | grep php | grep -v grep grep -v grep 过滤当前命令grep 占用的内存数量123ps auxf | grep php | grep -v grep | grep -v master | awk &apos;&#123;sum+=$6&#125; END &#123;print sum&#125;&apos;18207120 查看进程数量123ps auxf | grep php | grep -v grep | grep -v master | wc -l800 可以看到第6列，每一个子进程的内存占用大概在22M之间（单位为KB）。平均的内存占用为18207120KB/800/1024 = 22.2M。 进程数限制此时如果我们分配全部的内存给PHP-FPM使用，那么进程数可以限制在`16*1000/22 = 744,但是由于我们的服务器同时服务了很多内容，所以我们可以向下调整到256个进程数： 123456process.max = 700pm = dynamicpm.max_children = 700pm.start_servers = 450pm.min_spare_servers = 200pm.max_spare_servers = 700 max_children 这个值原则上是越大越好，php-cgi的进程多了就会处理的很快，排队的请求就会很少。 设置”max_children”也需要根据服务器的性能进行设定 一般来说一台服务器正常情况下每一个php-cgi所耗费的内存在20M左右 假设“max_children”设置成100个，20M*100=2000M 也就是说在峰值的时候所有PHP-CGI所耗内存在2000M以内。 假设“max_children”设置的较小，比如5-10个，那么php-cgi就会“很累”，处理速度也很慢，等待的时间也较长。 如果长时间没有得到处理的请求就会出现504 Gateway Time-out这个错误，而正在处理的很累的那几个php-cgi如果遇到了问题就会出现502 Bad gateway这个错误。 start_servers pm.start_servers的默认值为2。并且php-fpm中给的计算方式也为：{（cpu空闲时等待连接的php的最小子进程数） + （cpu空闲时等待连接的php的最大子进程数 - cpu空闲时等待连接的php的最小子进程数）/ 2}； 用配置表示就是：min_spare_servers + (max_spare_servers - min_spare_servers) / 2； 一般而言，设置成10-20之间的数据足够满足需求了。 max_requests（最大请求数） 最大处理请求数是指一个php-fpm的worker进程在处理多少个请求后就终止掉，master进程会重新respawn一个新的。这个配置的主要目的是避免php解释器或程序引用的第三方库造成的内存泄露。pm.max_requests = 10240 当一个 PHP-CGI 进程处理的请求数累积到 max_requests 个后，自动重启该进程。 502，是后端 PHP-FPM 不可用造成的，间歇性的502一般认为是由于 PHP-FPM 进程重启造成的. 但是为什么要重启进程呢？ 如果不定期重启 PHP-CGI 进程，势必造成内存使用量不断增长（比如第三方库有问题等）。因此 PHP-FPM 作为 PHP-CGI 的管理器，提供了这么一项监控功能，对请求达到指定次数的 PHP-CGI 进程进行重启，保证内存使用量不增长。 正是因为这个机制，在高并发中，经常导致 502 错误 目前我们解决方案是把这个值尽量设置大些，减少 PHP-CGI 重新 SPAWN 的次数，同时也能提高总体性能。PS：刚开始我们是500导致内存飙高，现在改成5120，当然可以再大一些，10240等，这个主要看测试结果，如果没有内存泄漏等问题，可以再大一些。 request_terminate_timeout（最长执行时间）max_execution_time和request_terminate_timeout ; The timeout for serving a single request after which the worker process will; be killed. This option should be used when the ‘max_execution_time’ ini option; does not stop script execution for some reason. A value of ‘0’ means ‘off’.; Available units: s(econds)(default), m(inutes), h(ours), or d(ays); Default Value: 0;request_terminate_timeout = 0＝＝＝＝＝＝＝＝＝＝＝＝设置单个请求的超时中止时间. 该选项可能会对php.ini设置中的’max_execution_time’因为某些特殊原因没有中止运行的脚本有用. 设置为 ‘0’ 表示 ‘Off’.当经常出现502错误时可以尝试更改此选项。 这两项都是用来配置一个PHP脚本的最大执行时间的。当超过这个时间时，PHP-FPM不只会终止脚本的执行，还会终止执行脚本的Worker进程。 Nginx会发现与自己通信的连接断掉了，就会返回给客户端502错误。 优化PHP-FPM压测优化PHP-FPM结果预览 PHP-FPM配置 wrk压测结果 Nginx报错 优化Nginx压测https://www.cyberciti.biz/faq/linux-unix-nginx-too-many-open-files/ http://www.chengweiyang.cn/2015/11/14/how-to-enlarge-linux-open-files-upper-cell/ 错误：24: Too many open files错误详情 错误原因Linux / UNIX对文件句柄和打开文件的数量设置了软限制和硬限制。您可以使用ulimit命令查看这些限制 查看硬值和软值: 12ulimit -Hnulimit -Sn 解决方法在Linux OS级别上增加开放FD限制 编辑文件/etc/sysctl.conf，输入： 1vi /etc/sysctl.conf 追加/修改以下行：fs.file-max = 100000 保存并关闭文件。编辑/etc/security/limits.conf，输入：# vi /etc/security/limits.conf Set soft and hard limit for all users or nginx user as follows: 12nginx soft nofile 100000nginx hard nofile 100000 保存并并关闭文件输入下面命令使之生效sysctl -p Nginx优化https://www.mtyun.com/library/how-to-optimize-nginx https://www.linpx.com/p/11-nginx-parameter-performance-optimization-suggestions.html 对于Nginx的调优，可以大致从如下指令着手 worker_processes worker_connections Buffers Timeouts Gzip Compression Static File Caching logging 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273user nginx;worker_processes 8;worker_cpu_affinity auto;worker_rlimit_nofile 500000;events &#123; use epoll; accept_mutex off; accept_mutex_delay 500ms; worker_connections 10240; multi_accept on;&#125;pid /var/run/nginx.pid;http &#123; include /etc/nginx/mime.types; default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; sendfile on; tcp_nopush on; keepalive_timeout 10; keepalive_requests 5000; tcp_nodelay on; client_body_buffer_size 10m; client_header_buffer_size 32k; client_max_body_size 20m; large_client_header_buffers 4 32k; client_body_timeout 12; client_header_timeout 12; send_timeout 10; access_log off; error_log /var/log/nginx/error.log crit; access_log /var/log/nginx/access.log main; gzip on; gzip_buffers 16 8k; gzip_comp_level 6; gzip_http_version 1.0; gzip_min_length 1000; gzip_proxied any; gzip_vary on; gzip_types text/xml application/xml application/atom+xml application/rss+xml application/xhtml+xml image/svg+xml text/javascript application/javascript application/x-javascript text/x-json application/json application/x-web-app-manifest+json text/css text/plain text/x-component font/opentype application/x-font-ttf application/vnd.ms-fontobject image/x-icon; gzip_disable \"MSIE [1-6]\\.(?!.*SV1)\"; open_file_cache max=100000 inactive=20s; open_file_cache_valid 30s; open_file_cache_min_uses 2; open_file_cache_errors on; proxy_buffering on; proxy_buffer_size 64k; proxy_buffers 12 32k; proxy_busy_buffers_size 256k; include /etc/nginx/conf.d/*.conf;&#125; worker_processes-配置工作进程数量worker_processes表示工作进程的数量，一般情况设置成CPU核的数量即可，一个cpu配置多于一个worker数，对Nginx而言没有任何益处 1grep processor /proc/cpuinfo | wc -l 这个命令会告诉你当前机器是多少核，输出为8即表示8核。 worker_cpu_affinity-配置cpu亲和性设置worker_cpu_affinity，这个配置用于将worker process与指定cpu核绑定，降低由于多CPU核切换造成的寄存器等现场重建带来的性能损耗。 worker_rlimit_nofile-配置 worker 进程的最大打开文件数调整配置 Nginx worker 进程的最大打开文件数，这个控制连接数的参数为worker_rlimit_nofile。该参数的实际配置如下: 1worker_rlimit_nofile 65535; 可设置为系统优化后的 ulimit -HSn 的结果 events块-配置事件处理模型的优化1234567events &#123; use epoll; # accept_mutex off; # accept_mutex_delay 500ms; worker_connections 10240; multi_accept on;&#125; use epoll Nginx 的连接处理机制在不同的操作系统中会采用不同的 I/O 模型，在 linux 下，Nginx 使用 epoll 的 I/O 多路复用模型，在 Freebsd 中使用 kqueue 的 I/O 多路复用模型，在 Solaris 中使用 /dev/poll 方式的 I/O 多路复用模型，在 Windows 中使用 icop，等等。 events 指令是设定 Nginx 的工作模式及连接数上限。use指令用来指定 Nginx 的工作模式。Nginx 支持的工作模式有 select、 poll、 kqueue、 epoll 、 rtsig 和/ dev/poll。当然，也可以不指定事件处理模型，Nginx 会自动选择最佳的事件处理模型。 accept_mutex off/on ? https://blog.huoding.com/2013/08/24/281 https://www.linuxdashen.com/nginx%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96 关闭 accept_mutex ，请求在多个 worker 间的分配更均衡 accept_mutex_delay当accept_mutex功能启用后，只有一个持有mutex锁的worker进程会接受并处理请求，其他worker进程等待。accept_mutex_delay指定的时间就是这些worker进程的等待时间，过了等待时间下一个worker进程便取得mutex锁，处理请求。accept_mutex_delay在events模块中指定，默认的值为500ms。 worker_connections通过调整控制连接数的参数来调整 Nginx 单个进程允许的客户端最大连接数。 worker_connections 也是个事件模块指令，用于定义 Nginx 每个进程的最大连接数，默认是 1024。 最大连接数的计算公式如下： 1max_clients = worker_processes * worker_connections; 如果作为反向代理，因为浏览器默认会开启 2 个连接到 server，而且 Nginx 还会使用fds（file descriptor）从同一个连接池建立连接到 upstream 后端。则最大连接数的计算公式如下： 1max_clients = worker_processes * worker_connections / 4; 另外，进程的最大连接数受 Linux 系统进程的最大打开文件数限制，在执行操作系统命令 ulimit -HSn 65535或配置相应文件后， worker_connections 的设置才能生效。 multi_accept 默认情况下，Nginx 进程只会在一个时刻接收一个新的连接，我们可以配置multi_accept 为 on，实现在一个时刻内可以接收多个新的连接，提高处理效率。该参数默认是 off，建议开启。 tcp优化1234567http &#123; sendfile on; tcp_nopush on; keepalive_timeout 120; tcp_nodelay on;&#125; sendfile 第一行的 sendfile 配置可以提高 Nginx 静态资源托管效率。sendfile 是一个系统调用，直接在内核空间完成文件发送，不需要先 read 再 write，没有上下文切换开销。 tcp_nopush TCP_NOPUSH 是 FreeBSD 的一个 socket 选项，对应 Linux 的 TCP_CORK，Nginx 里统一用 tcp_nopush 来控制它，并且只有在启用了 sendfile 之后才生效。启用它之后，数据包会累计到一定大小之后才会发送，减小了额外开销，提高网络效率。 keepalive_timeout Nginx 使用 keepalive_timeout 来指定 KeepAlive 的超时时间（timeout）。指定每个 TCP 连接最多可以保持多长时间。Nginx 的默认值是 75 秒，有些浏览器最多只保持 60 秒，所以可以设定为 60 秒。若将它设置为 0，就禁止了 keepalive 连接。 keepalive_requests keepalive_requests指令用于设置一个keep-alive连接上可以服务的请求的最大数量，当最大请求数量达到时，连接被关闭。默认是100。 这个参数的真实含义，是指一个keep alive建立之后，nginx就会为这个连接设置一个计数器，记录这个keep alive的长连接上已经接收并处理的客户端请求的数量。如果达到这个参数设置的最大值时，则nginx会强行关闭这个长连接，逼迫客户端不得不重新建立新的长连接。大多数情况下当QPS(每秒请求数)不是很高时，默认值100凑合够用。但是，对于一些QPS比较高（比如超过10000QPS，甚至达到30000,50000甚至更高) 的场景，默认的100就显得太低。简单计算一下，QPS=10000时，客户端每秒发送10000个请求(通常建立有多个长连接)，每个连接只能最多跑100次请求，意味着平均每秒钟就会有100个长连接因此被nginx关闭。同样意味着为了保持QPS，客户端不得不每秒中重新新建100个连接。因此，就会发现有大量的TIME_WAIT的socket连接(即使此时keep alive已经在client和nginx之间生效)。因此对于QPS较高的场景，非常有必要加大这个参数，以避免出现大量连接被生成再抛弃的情况，减少TIME_WAIT。 tcp_nodelay TCP_NODELAY 也是一个 socket 选项，启用后会禁用 Nagle 算法，尽快发送数据，某些情况下可以节约 200ms（Nagle 算法原理是：在发出去的数据还未被确认之前，新生成的小数据先存起来，凑满一个 MSS 或者等到收到确认后再发送）。Nginx 只会针对处于 keep-alive 状态的 TCP 连接才会启用 tcp_nodelay。 优化连接参数这部分更多是更具业务场景来决定的。例如client_max_body_size用来决定请求体的大小，用来限制上传文件的大小。上面列出的参数可以作为起始参数。 123456http &#123; client_body_buffer_size 10m; client_header_buffer_size 32k; client_max_body_size 20m; large_client_header_buffers 4 32k;&#125; Buffers：另一个很重要的参数为buffer，如果buffer太小，Nginx会不停的写一些临时文件，这样会导致磁盘不停的去读写，现在我们先了解设置buffer的一些相关参数： client_body_buffer_size 允许客户端请求的最大单个文件字节数 client_header_buffer_size 用于设置客户端请求的Header头缓冲区大小，大部分情况1KB大小足够 client_max_body_size 设置客户端能够上传的文件大小，默认为1m large_client_header_buffers 该指令用于设置客户端请求的Header头缓冲区大小 超时时间优化client_body_timeout 设定客户端与服务器建立连接后发送request body的超时时间。如果客户端在此时间内没有发送任何内容，那么Nginx返回HTTP 408错误（Request Timed Out）。它的默认值是60秒，在http, server 和 location模块中定义。 client_header_timeout 设定客户端向服务器发送一个完整的request header的超时时间。如果客户端在此时间内没有发送一个完整的request header，那么Nginx返回HTTP 408错误（Request Timed Out）。它的默认值是60秒，在http 和 server模块中定义。 send_timeout 指定了向客户端传输数据的超时时间。默认值为60秒，可以在http, server 和 location模块中定义 压缩优化Gzip 压缩 开启Gzip，gzip可以帮助Nginx减少大量的网络传输工作，另外要注意gzip_comp_level的设置，太高的话，Nginx服务会浪费CPU的执行周期。 1234567891011121314151617http &#123; gzip on; gzip_buffers 16 8k; gzip_comp_level 2; gzip_http_version 1.0; gzip_min_length 1000; gzip_proxied any; gzip_vary on; gzip_types text/xml application/xml application/atom+xml application/rss+xml application/xhtml+xml image/svg+xml text/javascript application/javascript application/x-javascript text/x-json application/json application/x-web-app-manifest+json text/css text/plain text/x-component font/opentype application/x-font-ttf application/vnd.ms-fontobject image/x-icon; gzip_disable \"MSIE [1-6]\\.(?!.*SV1)\";&#125; 静态资源缓存123location ~* .(jpg|jpeg|png|gif|ico|css|js)$ &#123; expires 365d;&#125; 压测总结服务器负载已达到最大，php-fpm+nginx模式进行优化不能再明显的提高PRS Swoole12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364gzip on;gzip_min_length 1024;gzip_comp_level 2;gzip_types text/plain text/css text/javascript application/json application/javascript application/x-javascript application/xml application/x-httpd-php image/jpeg image/gif image/png font/ttf font/otf image/svg+xml;gzip_vary on;gzip_disable \"msie6\";upstream swoole &#123; # 通过 IP:Port 连接 server 127.0.0.1:5200 weight=5 max_fails=3 fail_timeout=30s; # 通过 UnixSocket Stream 连接，小诀窍：将socket文件放在/dev/shm目录下，可获得更好的性能 #server unix:/xxxpath/laravel-s-test/storage/laravels.sock weight=5 max_fails=3 fail_timeout=30s; #server 192.168.1.1:5200 weight=3 max_fails=3 fail_timeout=30s; #server 192.168.1.2:5200 backup; keepalive 16;&#125;server &#123; listen 80; server_name dabao.zhimma.com; index index.php index.html index default; root /data/wwwroot/dabao/api/public; access_log /var/log/nginx/$server_name.access.log main; autoindex off; # Nginx处理静态资源(建议开启gzip)，LaravelS处理动态资源。 location / &#123; try_files $uri @laravels; &#125; # 当请求PHP文件时直接响应404，防止暴露public/*.php #location ~* \\.php$ &#123; # return 404; #&#125; location @laravels &#123; # proxy_connect_timeout 60s; # proxy_send_timeout 60s; # proxy_read_timeout 120s; proxy_http_version 1.1; proxy_set_header Connection \"\"; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Real-PORT $remote_port; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_set_header Scheme $scheme; proxy_set_header Server-Protocol $server_protocol; proxy_set_header Server-Name $server_name; proxy_set_header Server-Addr $server_addr; proxy_set_header Server-Port $server_port; proxy_set_header Set-Cookie \"HttpOnly\"; proxy_set_header Set-Cookie \"Secure\"; proxy_set_header X-Frame-Options \"SAMEORIGIN\"; proxy_pass http://swoole; &#125; location ^~ /backend &#123; alias /data/wwwroot/dabao/web/; if (!-e $request_filename) &#123; rewrite ^/(.*) /backend/index.html last; break; &#125; try_files $uri $uri/ @router; &#125; location @router &#123; rewrite ~.*$ /index.html last; &#125;&#125; 压测结果 php-fpm 配置文件解读1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162pid = /usr/local/var/run/php-fpm.pid#pid设置，一定要开启,上面是Mac平台的。默认在php安装目录中的var/run/php-fpm.pid。比如centos的在: /usr/local/php/var/run/php-fpm.piderror_log = /usr/local/var/log/php-fpm.log#错误日志，上面是Mac平台的，默认在php安装目录中的var/log/php-fpm.log，比如centos的在: /usr/local/php/var/log/php-fpm.loglog_level = notice#错误级别. 上面的php-fpm.log纪录的登记。可用级别为: alert（必须立即处理）, error（错误情况）, warning（警告情况）, notice（一般重要信息）, debug（调试信息）. 默认: notice.emergency_restart_threshold = 60emergency_restart_interval = 60s#表示在emergency_restart_interval所设值内出现SIGSEGV或者SIGBUS错误的php-cgi进程数如果超过 emergency_restart_threshold个，php-fpm就会优雅重启。这两个选项一般保持默认值。0 表示 '关闭该功能'. 默认值: 0 (关闭).process_control_timeout = 0#设置子进程接受主进程复用信号的超时时间. 可用单位: s(秒), m(分), h(小时), 或者 d(天) 默认单位: s(秒). 默认值: 0.daemonize = yes#后台执行fpm,默认值为yes，如果为了调试可以改为no。在FPM中，可以使用不同的设置来运行多个进程池。 这些设置可以针对每个进程池单独设置。listen = 127.0.0.1:9000#fpm监听端口，即nginx中php处理的地址，一般默认值即可。可用格式为: 'ip:port', 'port', '/path/to/unix/socket'. 每个进程池都需要设置。如果nginx和php在不同的机器上，分布式处理，就设置ip这里就可以了。listen.backlog = -1#backlog数，设置 listen 的半连接队列长度，-1表示无限制，由操作系统决定，此行注释掉就行。backlog含义参考：http://www.3gyou.cc/?p=41listen.allowed_clients = 127.0.0.1#允许访问FastCGI进程的IP白名单，设置any为不限制IP，如果要设置其他主机的nginx也能访问这台FPM进程，listen处要设置成本地可被访问的IP。默认值是any。每个地址是用逗号分隔. 如果没有设置或者为空，则允许任何服务器请求连接。listen.owner = wwwlisten.group = wwwlisten.mode = 0666#unix socket设置选项，如果使用tcp方式访问，这里注释即可。user = wwwgroup = www#启动进程的用户和用户组，FPM 进程运行的Unix用户, 必须要设置。用户组，如果没有设置，则默认用户的组被使用。pm = dynamic #php-fpm进程启动模式，pm可以设置为static和dynamic和ondemand#如果选择static，则进程数就数固定的，由pm.max_children指定固定的子进程数。#如果选择dynamic，则进程数是动态变化的,由以下参数决定：pm.max_children = 50 #子进程最大数pm.start_servers = 2 #启动时的进程数，默认值为: min_spare_servers + (max_spare_servers - min_spare_servers) / 2pm.min_spare_servers = 1 #保证空闲进程数最小值，如果空闲进程小于此值，则创建新的子进程pm.max_spare_servers = 3 #，保证空闲进程数最大值，如果空闲进程大于此值，此进行清理pm.max_requests = 10000#设置每个子进程重生之前服务的请求数. 对于可能存在内存泄漏的第三方模块来说是非常有用的. 如果设置为 '0' 则一直接受请求. 等同于 PHP_FCGI_MAX_REQUESTS 环境变量. 默认值: 0.pm.status_path = /status#FPM状态页面的网址. 如果没有设置, 则无法访问状态页面. 默认值: none. munin监控会使用到ping.path = /ping#FPM监控页面的ping网址. 如果没有设置, 则无法访问ping页面. 该页面用于外部检测FPM是否存活并且可以响应请求. 请注意必须以斜线开头 (/)。ping.response = pong#用于定义ping请求的返回相应. 返回为 HTTP 200 的 text/plain 格式文本. 默认值: pong.access.log = log/$pool.access.log#每一个请求的访问日志，默认是关闭的。access.format = \"%R - %u %t \\\"%m %r%Q%q\\\" %s %f %&#123;mili&#125;d %&#123;kilo&#125;M %C%%\"#设定访问日志的格式。slowlog = log/$pool.log.slow#慢请求的记录日志,配合request_slowlog_timeout使用，默认关闭request_slowlog_timeout = 10s#当一个请求该设置的超时时间后，就会将对应的PHP调用堆栈信息完整写入到慢日志中. 设置为 '0' 表示 'Off'request_terminate_timeout = 0#设置单个请求的超时中止时间. 该选项可能会对php.ini设置中的'max_execution_time'因为某些特殊原因没有中止运行的脚本有用. 设置为 '0' 表示 'Off'.当经常出现502错误时可以尝试更改此选项。rlimit_files = 1024#设置文件打开描述符的rlimit限制. 默认值: 系统定义值默认可打开句柄是1024，可使用 ulimit -n查看，ulimit -n 2048修改。rlimit_core = 0#设置核心rlimit最大限制值. 可用值: 'unlimited' 、0或者正整数. 默认值: 系统定义值.chroot =#启动时的Chroot目录. 所定义的目录需要是绝对路径. 如果没有设置, 则chroot不被使用.chdir =#设置启动目录，启动时会自动Chdir到该目录. 所定义的目录需要是绝对路径. 默认值: 当前目录，或者/目录（chroot时）catch_workers_output = yes#重定向运行过程中的stdout和stderr到主要的错误日志文件中. 如果没有设置, stdout 和 stderr 将会根据FastCGI的规则被重定向到 /dev/null . 默认值: 空. Nginx错误日志说明 错误信息 错误说明 upstream prematurely（过早的） closed connection 请求uri的时候出现的异常，是由于upstream还未返回应答给用户时用户断掉连接造成的，对系统没有影响，可以忽略 recv() failed (104: Connection reset by peer) （1）服务器的并发连接数超过了其承载量，服务器会将其中一些连接Down掉； （2）客户关掉了浏览器，而服务器还在给客户端发送数据； （3）浏览器端按了Stop (111: Connection refused) while connecting to upstream 用户在连接时，若遇到后端upstream挂掉或者不通，会收到该错误 (111: Connection refused) while reading response header from upstream 用户在连接成功后读取数据时，若遇到后端upstream挂掉或者不通，会收到该错误 (111: Connection refused) while sending request to upstream Nginx和upstream连接成功后发送数据时，若遇到后端upstream挂掉或者不通，会收到该错误 (110: Connection timed out) while connecting to upstream nginx连接后面的upstream时超时 (110: Connection timed out) while reading upstream nginx读取来自upstream的响应时超时 (110: Connection timed out) while reading response header from upstream nginx读取来自upstream的响应头时超时 (110: Connection timed out) while reading upstream nginx读取来自upstream的响应时超时 (104: Connection reset by peer) while connecting to upstream upstream发送了RST，将连接重置 upstream sent invalid header while reading response header from upstream upstream发送的响应头无效 upstream sent no valid HTTP/1.0 header while reading response header from upstream upstream发送的响应头无效 client intended to send too large body 用于设置允许接受的客户端请求内容的最大值，默认值是1M，client发送的body超过了设置值 reopening logs 用户发送kill -WINCH命令 no servers are inside upstream upstream下未配置server no live upstreams while connecting to upstream upstream下的server全都挂了 SSL_do_handshake() failed SSL握手失败 SSL_write() failed (SSL:) while sending to client (13: Permission denied) while reading upstream (98: Address already in use) while connecting to upstream (99: Cannot assign requested address) while connecting to upstream ngx_slab_alloc() failed: no memory in SSL session shared cache ssl_session_cache大小不够等原因造成 could not add new SSL session to the session cache while SSL handshaking ssl_session_cache大小不够等原因造成 send() failed (111: Connection refused) 错误日志类型 类型1: upstream timed out 类型2: connect() failed 类型3: no live upstreams 类型4: upstream prematurely closed connection 类型5: 104: Connection reset by peer 类型6: client intended to send too large body 类型7: upstream sent no valid HTTP/1.0 header 类型 错误日志 原因 解决办法 1 upstream timed out (110: Connection timed out) while connecting to upstream nginx与upstream建立tcp连接超时，nginx默认连接建立超时为200ms 排查upstream是否能正常建立tcp连接 1 upstream timed out (110: Connection timed out) while reading response header from upstream nginx从upstream读取响应时超时，nginx默认的读超时为20s，读超时不是整体读的时间超时，而是指两次读操作之间的超时，整体读耗时有可能超过20s 排查upstream响应请求为什么过于缓慢 2 connect() failed (104: Connection reset by peer) while connecting to upstream nginx与upstream建立tcp连接时被reset 排查upstream是否能正常建立tcp连接 2 connect() failed (111: Connection refused) while connecting to upstream nginx与upstream建立tcp连接时被拒 排查upstream是否能正常建立tcp连接 3 no live upstreams while connecting to upstream nginx向upstream转发请求时发现upstream状态全都为down 排查nginx的upstream的健康检查为什么失败 4 upstream prematurely closed connection nginx在与upstream建立完tcp连接之后，试图发送请求或者读取响应时，连接被upstream强制关闭 排查upstream程序是否异常，是否能正常处理http请求 5 recv() failed (104: Connection reset by peer) while reading response header from upstream nginx从upstream读取响应时连接被对方reset 排查upstream应用已经tcp连接状态是否异常 6 client intended to send too large body 客户端试图发送过大的请求body，nginx默认最大允许的大小为1m，超过此大小，客户端会受到http 413错误码 调整请求客户端的请求body大小；调大相关域名的nginx配置：client_max_body_size； 7 upstream sent no valid HTTP/1.0 header nginx不能正常解析从upstream返回来的请求行","categories":[{"name":"Nginx","slug":"Nginx","permalink":"https://blog.zhimma.com/categories/Nginx/"},{"name":"PHP-FPM","slug":"Nginx/PHP-FPM","permalink":"https://blog.zhimma.com/categories/Nginx/PHP-FPM/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://blog.zhimma.com/tags/Nginx/"},{"name":"PHP-FPM","slug":"PHP-FPM","permalink":"https://blog.zhimma.com/tags/PHP-FPM/"}]},{"title":"","slug":"Lumen接入Azure的blob存储配置","date":"2019-08-16T06:42:41.000Z","updated":"2019-08-16T07:15:56.405Z","comments":true,"path":"2019/08/16/Lumen接入Azure的blob存储配置/","link":"","permalink":"https://blog.zhimma.com/2019/08/16/Lumen接入Azure的blob存储配置/","excerpt":"","text":"[TOC] 最近在做的一个项目使用的是Azure服务器，其中数据库、redis、和对象存储和国内厂商都有不同，最直观的区别就是Azure基本都是SSL方式进行连接，相对国内云服务器厂商的服务接入没有那么无脑，今天简单记录下Azure提供的blob存储在lumen框架的接入 几个轮子12composer require league/flysystemcomposer require league/flysystem-azure-blob-storage 配置Laravel/Lumen 提供了很强大的文件管理系统和云存储功能的集成 /Users/zhimma/Data/www/lumen/vendor/laravel/lumen-framework/config目录复制一份到项目根目录下,主要检查/Users/zhimma/Data/www/lumen/config/filesystems.php这个文件是否存在 下面进行配置： /Users/zhimma/Data/www/lumen/config/filesystems.php 在配置s3下方添加如下内容： 1234567...'azure' =&gt; [ 'driver' =&gt; 'azure', 'name' =&gt; env('AZURE_STORAGE_NAME', 'xxx'), 'key' =&gt; env('AZURE_STORAGE_KEY', 'xxx'), 'container' =&gt; env('AZURE_STORAGE_CONTAINER', 'xxx'),], /Users/zhimma/Data/www/lumen/bootstrap/app.php 添加$app-&gt;configure(&#39;filesystems&#39;);表示加载该配置文件项 添加$app-&gt;register(\\Illuminate\\Filesystem\\FilesystemServiceProvider::class);加载该服务提供者 /Users/zhimma/Data/www/lumen/app/Providers/AppServiceProvider.php 新增下面的方法 1234567891011public function boot()&#123; Storage::extend('azure', function ($app, $config) &#123; $endpoint = sprintf('DefaultEndpointsProtocol=https;AccountName=%s;AccountKey=%s;EndpointSuffix=core.chinacloudapi.cn', $config['name'], $config['key'], $config['url'] ?? null, $config['prefix'] ?? null); $client = BlobRestProxy::createBlobService($endpoint); $adapter = new AzureBlobStorageAdapter($client, $config['container'], $config['prefix'] ?? null); return new Filesystem($adapter); &#125;);&#125; 参考文档：https://learnku.com/docs/laravel/5.5/filesystem/1319#custom-filesystems 上传测试新增好路由后，我们进行上传测试 123456789101112131415161718192021222324252627class UploadController extends Controller&#123; /** * 文件上传 * * @param Request $request * * @return \\Illuminate\\Http\\JsonResponse * @throws \\Exception * @author zhimma * @date 2019/5/27 12:00 PM */ public function upload(Request $request) &#123; if (!$request-&gt;hasFile('file')) &#123; throw new \\Exception(\"文件不存在\"); &#125; $file = $request-&gt;file('file'); if (!$file-&gt;isValid()) &#123; throw new \\Exception($file-&gt;getErrorMessage()); &#125; $path = Storage::put(date('Ymd'), $file); $url = env('AZURE_BLOB_URL').'/'.env('AZURE_STORAGE_CONTAINER').'/'.$path; return $this-&gt;success(['url' =&gt; $url]); &#125;&#125; 返回如下 12345678&#123; \"status\": \"success\", \"httpCode\": 200, \"statusCode\": 0, \"data\": &#123; \"url\": \"https://xxx.blob.core.chinacloudapi.cn/xxx/20190816/FKRJQXqo1Rdm77mAW2biuBSaVx12mH4U52NtKlZI.png\" &#125;&#125; 至此配置完成 参考 https://matthewdaly.co.uk/blog/2016/10/24/creating-an-azure-storage-adapter-for-laravel/ https://stackoverflow.com/questions/56267900/how-to-use-azure-blob-in-lumen","categories":[{"name":"Azure","slug":"Azure","permalink":"https://blog.zhimma.com/categories/Azure/"}],"tags":[{"name":"Azure","slug":"Azure","permalink":"https://blog.zhimma.com/tags/Azure/"}]},{"title":"RabbitMQ","slug":"RabbitMQ","date":"2019-07-15T07:42:32.000Z","updated":"2019-07-15T07:43:16.000Z","comments":true,"path":"2019/07/15/RabbitMQ/","link":"","permalink":"https://blog.zhimma.com/2019/07/15/RabbitMQ/","excerpt":"","text":"1234567Fatal error: Uncaught Error: Class &apos;AMQPConnection&apos; not found in /Users/zhimma/Data/www/rabbitMQ/send.php:21Stack trace:0 &#123;main&#125; thrown in /Users/zhimma/Data/www/rabbitMQ/send.php on line 21[Finished in 0.2s]","categories":[],"tags":[]},{"title":"Jenkins初始化及常见问题整理","slug":"Jenkins初始化及常见问题整理","date":"2019-06-19T02:21:41.000Z","updated":"2019-06-19T09:39:52.885Z","comments":true,"path":"2019/06/19/Jenkins初始化及常见问题整理/","link":"","permalink":"https://blog.zhimma.com/2019/06/19/Jenkins初始化及常见问题整理/","excerpt":"","text":"[TOC] 错误异常处理1. Failed to connect to repository : Error performing command: git ls-remote -h产生原因: Jenkins服务器没有安装git 解决方式： 1. Jenkins服务器上查看git是否已安装及安装位置 1234567[root@ci ~]# git version-bash: git: command not found[root@ci ~]# yum install git -yComplete![root@ci ~]# whereis gitgit: /usr/bin/git 2. 打开Jenkins的 主页面 &gt; 系统管理 &gt; Global Tool Configuration 配置下git 的path 2. stdout: stderr: Host key verification failed. fatal: Could not read from remote repository.Please make sure you have the correct access rights and the repository exists.产生原因: 代码仓库没有添加允许jenkins服务器用户拉取代 解决方式： 创建秘钥可以参考第三个错误的解决步骤 检查jenkins服务器ssh key 12345[jenkins@ci ~]# cd .ssh[jenkins@ci ~]# lltotal 8-rw-------. 1 root root 1675 Jun 19 11:06 id_rsa-rw-r--r--. 1 root root 389 Jun 19 11:06 id_rsa.pub 如果没有文件，则使用下面命令进行创建 1234[jenkins@ci ~]$ssh-keygenGenerating public/private rsa key pair.Enter file in which to save the key (/var/lib/jenkins/.ssh/id_rsa):// 一路回车即可 在代码仓库添加jenkins服务器用户的公钥 在jenkins管理端创建全局凭证并使用 创建全局凭证 使用全局凭证 3. Load key “/var/lib/jenkins/.ssh/id_rsa”: Permission denied参考1 这里需要对Jenkins密钥进行配置 3-1 切换jenkins用户不成功jenkins系统本身有bug的，下面切换用户会失败 1su jenkins 解决办法如下： /etc/passwd文件中的/bin/bash被yum安装的时候变成了/bin/false 然后我执行cat /etc/passwd命令，果然被改成了/bin/false cat /etc/passwd 接着执行sudo vim /etc/passwd命令,把false改为bash sudo vim /etc/passwd 改完后使用grep jenkins /etc/passwd 查看下是否成功 1jenkins:x:997:995:Jenkins Automation Server:/var/lib/jenkins:/bin/bash 3-2 su jenkins bash-4.2问题当我们切换到jenkins用户后，命令提示符的用户名不是jenkins而变成了-bash-4.2# 网上一查，原因是在安装jenkins时，jenkins只是创建了jenkins用户，并没有为其创建home目录。所以系统就不会在创建用户的时候，自动拷贝/etc/skel目录下的用户环境变量文件到用户家目录，也就导致这些文件不存在，出现-bash-4.2#的问题了以下命令是在切换到jenkins用户下执行的！（只是用户现在显示的是-bash-4.2） 这个时候呢，参考网上的做法我执行下面步骤： vim ~/.bash_profile执行上面的命令，即使没有.bash_profile文件，linux会自动创建。 然后再添加这句 export PS1=&#39;[\\u@\\h \\W]\\$&#39; 我们最后再刷新.bash_profile文件，使其起作用 source ~/.bash_profile 3-2 su jenkins bash-4.2再解决参考 在上面操作后，我使用su jenkins 还是会有bash-4.2界面，只需把命令变为su - jenkins可以解决，同事给了一个其他的方法 ​ 将/etc/skel/目录下 .bashrc和 .bash_profile 文件拷贝到用户的家目录下，然后对复制过去的文件授予该用户、用户组权限。 1234cp /etc/skel/.bashrc /var/lib/jenkins/cp /etc/skel/.bash_profile /var/lib/jenkins/chown jenkins.jenkins /var/lib/jenkins/.bashrcchown jenkins.jenkins /var/lib/jenkins/.bash_profile ​ 然后退出重新登录，就恢复正常了。 /etc/skel/目录作用： ​ 用来存放新用户环境变量文件，添加新用户时，将该目录下文件拷贝到新用户家目录中。 ​ 默认情况下该目录下都是隐藏文件（以.点开头的文件）； ​ 通过修改、添加、删除该目录下的文件，可为新添加的用户提供统一的、标准的、初始化用户环境。 ####3-3 su jenkins 需要输密码 上面我们解决了bash-4.2，下次从root切换jenkins用户的时候，发现需要输入密码，下面解决下这个错误 1vi /etc/sudoers 添加jenkins ALL=(ALL) NOPASSWD: ALL 然后执行/etc/init.d/jenkins restart 3-4 Jenkins密钥配置su - jenkins切换到jenkins用户后，使用下面命令生成秘钥 1234[jenkins@ci ~]$ssh-keygenGenerating public/private rsa key pair.Enter file in which to save the key (/var/lib/jenkins/.ssh/id_rsa):// 一路回车即可 生成后， 把公钥添加到代码管理工具gitlab或者其他平台，使得jenkins用户有权限拉取代码 把公钥添加到需要发布代码的服务器，使得jenkins所在服务器有权限进行scp 或 ssh操作","categories":[{"name":"Jenkins","slug":"Jenkins","permalink":"https://blog.zhimma.com/categories/Jenkins/"}],"tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"https://blog.zhimma.com/tags/Jenkins/"}]},{"title":"Go零零碎碎","slug":"Go零零碎碎","date":"2019-06-17T09:08:40.000Z","updated":"2019-07-22T08:26:17.708Z","comments":true,"path":"2019/06/17/Go零零碎碎/","link":"","permalink":"https://blog.zhimma.com/2019/06/17/Go零零碎碎/","excerpt":"","text":"[TOC] 常见数据类型值类型值类型：基本数据类型int、float、bool、string、数组和结构体struct都属于值类型,使用这些类型的变量是直接使用指向存在内存中的值，值类型的变量的值通常存储在栈中 引用类型引用类型：指针、slice切片、map、管道chan、interface等都是引用类型 字符串golang的统一编码为utf-8，golang中string底层是通过byte数组实现的。中文字符在unicode下占2个字节，在utf-8编码下占3个字节，而golang默认编码正好是utf-8，所以字母数字占一字节，汉字占3字节 要获取一个字符串的长度，有以下2种方式： 1234567891011121314151617181920package mainimport ( \"fmt\" \"unicode/utf8\")func main() &#123; str := \"hello 芝麻开门\" // 返回字符串的长度，相当于PHP的strlen fmt.Println(\"str 的长度:\", len(str)) fmt.Println(\"str 的RuneCountInString长度:\", utf8.RuneCountInString(str)) fmt.Println(\"str 的rune convert长度：\", len([]rune(str))) /** str 的长度: 18 str 的RuneCountInString长度: 10 str 的rune convert长度： 10 */&#125; golang中还有一个byte数据类型，与rune相似，它们都是用来表示字符类型的变量类型。它们的不同在于： byte 等同于int8，常用来处理ascii字符 rune 等同于int32，常用来处理unicode或utf-8字符 循环一个字符串时，如果有中文，需要转切片，不然会出现乱码，因为是按照字符串的字节长度遍历 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687package mainimport ( \"fmt\" \"unicode/utf8\")func main() &#123; str := \"hello 芝麻开门\" // 返回字符串的长度，相当于PHP的strlen fmt.Println(\"str 的长度:\", len(str)) fmt.Println(\"str 的RuneCountInString长度:\", utf8.RuneCountInString(str)) fmt.Println(\"str 的rune convert长度：\", len([]rune(str))) /** str 的长度: 18 str 的RuneCountInString长度: 10 str 的rune convert长度： 10 */ runeStr := []rune(str) for i := 0; i &lt; len(runeStr); i++ &#123; fmt.Printf(\"%c\\n\", runeStr[i]) &#125; /** str 的长度: 18 str 的RuneCountInString长度: 10 str 的rune convert长度： 10 h e l l o 芝 麻 开 门 */ // 字符串函数 // 判断一个字符串是否包含指定的字符串 fmt.Println(strings.Contains(\"hello world\", \"hello\")) //true // 统计一个字符串有几个指定的字符，判断字符重复次数 fmt.Println(strings.Count(\"hello world\", \"o\")) // 2 // 字符串比较：不区分大小写== fmt.Println(strings.EqualFold(\"hello world\", \"Hello World\")) // true // 返回一个字符串在另一个字符串第一次出现的index值 fmt.Println(strings.Index(\"hello world\", \"o\")) // 4 // 返回一个字符串在另一个字符串最后一次出现的位置 fmt.Println(strings.LastIndex(\"hello world\", \"o\")) // 7 // 字符串替换 n=-1表示全部替换 fmt.Println(strings.Replace(\"hello world\", \"hello\", \"你好\", 1)) // 你好 world // 查分字符串为数组，类似PHP中的explode strArr := strings.Split(\"hello world\", \"o w\") fmt.Printf(\"strArr 类型 %T,值%v\\n\", strArr, strArr) // strArr 类型 []string,值[hell orld] for i := 0; i &lt; len(strArr); i++ &#123; fmt.Println(strArr[i]) /** hell orld */ &#125; // 对字符串大小写转换 fmt.Println(strings.ToLower(\"HELLO WORLD\")) // hello world fmt.Println(strings.ToUpper(\"hello world\")) // HELLO WORLD // 去除字符串两边的字符 fmt.Println(strings.TrimRight(\"~hello world~\", \"~\")) // ~hello world fmt.Println(strings.TrimLeft(\"~hello world~\", \"~\")) // hello world~ fmt.Println(strings.Trim(\"~hello world~\", \"~\")) // hello world // 评断字符串头尾是否存在指定字符 fmt.Println(strings.HasPrefix(\"hello world\", \"hello\")) // true fmt.Println(strings.HasSuffix(\"hello world\", \"hello\")) // false // join联合组合字符串 fmt.Println(strings.Join(strArr, \"o w\")) // hello world // 重复字符串 fmt.Println(strings.Repeat(\"go \", 3)) // go go go&#125; 数组声明数组1234567891011121314151617181920212223242526func arrayDemo() &#123; // 定义并初始化一个长度为3的数组 var arr [3]int = [3]int&#123;1, 2, 3&#125; arr1 := [3]int&#123;1, 2, 3&#125; fmt.Println(arr, arr1) // [1 2 3] [1 2 3] // 声明一个长度为6的数组，最后一个元素值为-1 arr2 := [...]int&#123;5: -1&#125; fmt.Println(arr2) // [0 0 0 0 0 -1] // 定义一个长度为3的int型数组，初始值是3个0,数组“零值”状态 var arr3 [3]int fmt.Println(arr3) // [0 0 0] // 二维数组 arr4 := [3][2]int&#123;&#125; arr5 := [3][2]string&#123;&#123;\"a\", \"b\"&#125;, &#123;\"AA\", \"BB\"&#125;, &#123;\"AAA\", \"BBB\"&#125;&#125; fmt.Println(arr4, arr5) // [[0 0] [0 0] [0 0]] [[a b] [AA BB] [AAA BBB]] // 使用new声明数组 arr6 := new([3]int) fmt.Printf(\"arr6类型%T，值%v\", arr6, arr6) // arr6类型*[3]int，值&amp;[0 0 0] arr7 := new([2]int) fmt.Printf(\"arr7类型%T，值%v\" , arr7 , arr7) // arr7类型*[2]int，值&amp;[0 0]&#125; 遍历数组1234567func for_range() &#123; arr := [...]string&#123;\"go\", \"php\", \"java\"&#125; for key, value := range arr &#123; fmt.Printf(\"key = %v , value = %v\", key, value) // key = 0 , value = gokey = 1 , value = phpkey = 2 , value = java &#125;&#125; slice切片定义和创建12345678910111213141516171819202122232425262728package mainimport &quot;fmt&quot;func main() &#123; sliceDemo1() sliceDemo2() sliceDemo3()&#125;func sliceDemo1() &#123; var arr [5]int = [...]int&#123;1, 2, 3, 4, 5&#125; var slice = arr[1:3] // slice value=[2 3] , cap=4 , len=2 fmt.Printf(&quot;slice value=%v , cap=%v , len=%v&quot;, slice, cap(slice), len(slice))&#125;func sliceDemo2() &#123; var slice []int = make([]int, 5, 10) fmt.Println(slice) // [0 0 0 0 0] slice[1] = 5 slice[3] = 10 // slice value=[0 5 0 10 0] , cap=10 , len=5 fmt.Printf(&quot;slice value=%v , cap=%v , len=%v&quot;, slice, cap(slice), len(slice))&#125;func sliceDemo3() &#123; // 声明和初始化一个切片 var slice []string = []string&#123;&quot;hello&quot;, &quot;world&quot;, &quot;!&quot;&#125; fmt.Printf(&quot;slice value=%v , cap=%v , len=%v&quot;, slice, cap(slice), len(slice))&#125; 三种方式说明及区别说明sliceDemo1：让切片引用一个已经存在的数组创建切片 sliceDemo2：使用make来创建，可以指定切片的大小和容量，如果没有给切片赋值，则会使用默认值，(int、float=&gt;0, strint=&gt;””, bool=&gt;false)；make方式创建的切片对应的数组由make底层维护，对外不开见，只能通过slice访问各个元素 sliceDemo3：定义一个切片直接指定具体数组 区别sliceDemo1方式是直接引用数组，这个数组是事先存在的，对程序员可见 sliceDemo2通过make创建切片，make也会创建一个数组，是由切片在底层维护，该数组对程序员不可见 冒泡排序1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package mainimport \"fmt\"func main() &#123; arr := [8]int&#123;22, 31, 23, 44, 1, 55, 54, 33&#125; /** [1 22 23 31 33 44 54 55] [1 22 23 31 33 44 54 55] [1 22 23 31 33 44 54 55] **/ fmt.Println(bubble1(arr)) fmt.Println(bubble2(arr)) fmt.Println(bubble3(arr))&#125;func bubble1(data [8]int) [8] int &#123; for i := 0; i &lt; len(data)-1; i++ &#123; fmt.Println(data[i]) for j := 0; j &lt; len(data)-1-i; j++ &#123; if data[j] &gt; data[j+1] &#123; tmp := data[j] data[j] = data[j+1] data[j+1] = tmp &#125; &#125; &#125; return data&#125;func bubble2(data [8]int) [8]int &#123; for i := 0; i &lt; len(data); i++ &#123; for j := i + 1; j &lt; len(data); j++ &#123; if data[i] &gt; data[j] &#123; tmp := data[i] data[i] = data[j] data[j] = tmp &#125; &#125; &#125; return data&#125;func bubble3(data [8]int) [8]int &#123; for i := 0; i &lt; len(data); i++ &#123; for j := 0; j &lt; len(data)-1; j++ &#123; if data[j] &gt; data[j+1] &#123; tmp := data[j] data[j] = data[j+1] data[j+1] = tmp &#125; &#125; &#125; return data&#125; mapmap相当于PHP中的关联数组 12345678910111213141516171819202122232425262728package mainimport \"fmt\"func main() &#123; mapDemo()&#125;func mapDemo() &#123; // 声明方式1 var map1 map[string]string map1 = make(map[string]string, 5) map1[\"name\"] = \"zhimma\" map1[\"address\"] = \"xian\" fmt.Println(map1) // map[name:zhimma address:xian] // 声明方式2 map2 := make(map[string]string) map2[\"name\"] = \"zhimma\" map2[\"address\"] = \"xian\" fmt.Println(map2) // map[name:zhimma address:xian] // 声明方式3 map3 := map[string]string&#123; \"name\": \"zhimma\", \"address\": \"xian\", &#125; fmt.Println(map3) // // map[name:zhimma address:xian]&#125; 指针相关概念变量：变量是基本类型，变量存的就是值，也叫值类型 地址：用于引用计算机的内存地址，可理解为内存地址的标签，通俗一点讲就是一个房子在小区门牌号 123456789package mainimport \"fmt\"func main() &#123; // 声明变量name并赋值 name := \"zhimma\" fmt.Println(\"name 的地址是\", &amp;name) // name 的地址是 0xc000092030&#125; 指针：指针变量存的是一个地址，这个地址指向的空间存的才是值，指针是一个指向另一个变量内存地址的值 1234567891011121314151617181920package mainimport \"fmt\"func main() &#123; var i int = 1 fmt.Println(\"i 的地址是\", &amp;i) var ptr *int = &amp;i fmt.Printf(\"ptr=%v\\n\", ptr) fmt.Printf(\"ptr的地址是%v\\n\", &amp;ptr) fmt.Printf(\"ptr指向的值是%v\\n\", *ptr) /** i 的地址是 0xc00001a080 ptr=0xc00001a080 ptr的地址是0xc00000c030 ptr指向的值是1 */&#125; &amp;:取地址符，指针 := &amp;变量 :取值符， 变量 := 指针 匿名函数定义匿名函数有2种方式 定义时直接使用（这种方式只使用一次） 12345678910package mainimport \"fmt\"func main() &#123; totalNum := func(a int, b int) int &#123; return a + b &#125;(5, 3) fmt.Println(totalNum) // 8&#125; 将匿名函数赋值给变量 123456789101112package mainimport \"fmt\"func main() &#123; // 将匿名函数赋值给变量sunFunc sumFunc := func(a int, b int) int &#123; return a + b &#125; total := sumFunc(1, 2) fmt.Println(total)&#125; 全局匿名函数 123456789101112package mainimport \"fmt\"var globalFunc = func(params string) string &#123; return \"this is globalFunc and receive params str is'\" + params + \"'\"&#125;func main() &#123; str := globalFunc(\"hello world\") fmt.Println(str) // this is globalFunc and receive params str is'hello world'&#125; 闭包new函数new函数用来分配内存，主要分配值类型，比如int、float32、struct等，返回的是指针 1234567891011func newDemo() &#123; num := 100 fmt.Printf(\"num 的类型是%T,值是%v,地址是%v\", num, num, &amp;num) // num 的类型是int,值是100,地址是0xc00001a178 num1 := new(int) fmt.Printf(\"num1 的类型是%T,值是%v,地址是%v\", num1, num1, &amp;num1) // num1 的类型是*int,值是0xc00001a190,地址是0xc00000c030 fmt.Printf(\"num1 的类型是%T,值是%v,地址是%v,指向的值是%v\", num1, num1, &amp;num1, *num1) // num1 的类型是*int,值是0xc00001a098,地址是0xc00000c030,指向的值是0&#125;","categories":[{"name":"Go","slug":"Go","permalink":"https://blog.zhimma.com/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://blog.zhimma.com/tags/Go/"}]},{"title":"Docker数据管理-volume/bind mount/tmpfs mount","slug":"Docker数据管理-volume-bind-mount-tmpfs-mount","date":"2019-04-10T03:11:26.000Z","updated":"2019-04-10T03:39:30.009Z","comments":true,"path":"2019/04/10/Docker数据管理-volume-bind-mount-tmpfs-mount/","link":"","permalink":"https://blog.zhimma.com/2019/04/10/Docker数据管理-volume-bind-mount-tmpfs-mount/","excerpt":"","text":"[TOC] 数据卷是一个可供一个或多个容器使用的特殊目录，它绕过UFS，可以提供很多有用的特性： 数据卷 可以在容器之间共享和重用 对 数据卷 的修改会立马生效 对 数据卷 的更新，不会影响镜像 数据卷 默认会一直存在，即使容器被删除 -v/-mount 如何选择最初，-v 和 -volume 用于独立的容器，--mount 用于 swarm server。但 docker 17.06 之后，也可以使用 --mount。两者的区别在于，-v 将所有选项组合在一个字段中，--mount 则将它们分开。 新用户应使用 --mount 语法，老用户推荐使用 --mount。 -v/--volume，由（:）分隔的三个字段组成，&lt;卷名&gt;:&lt;容器路径&gt;:&lt;选项列表&gt;。选项列表，如：ro只读。 --mount，由多个键值对组成，由,分隔，每个由一个&lt;key=&lt;value&gt;&gt;元组组成。 type，值可以为 bind，volume，tmpfs。 source，对于命名卷，是卷名。对于匿名卷，这个字段被省略。可能被指定为 source 或 src。 destination，文件或目录将被挂载到容器中的路径。可以指定为 destination，dst 或 target。 volume-opt 可以多次指定。 选择合适的挂载方式Docker提供了3种方法将数据从Docker宿主机挂载（mount）到容器：volumes，bind mounts和tmpfs mounts。一般来说，volumes总是最好的选择。 不管你选择哪种挂载方式，从容器中看都是一样的。数据在容器的文件系统中被展示为一个目录或者一个单独的文件。 一个简单区分volumes，bind mounts和tmpfs mounts不同点的方法是：思考数据在宿主机上是如何存在的。 Volumes由Docker管理，存储在宿主机的某个地方（在linux上是/var/lib/docker/volumes/）。非Docker应用程序不能改动这一位置的数据。Volumes是Docker最好的数据持久化方法。 Bind mounts的数据可以存放在宿主机的任何地方。数据甚至可以是重要的系统文件或目录。非Docker应用程序可以改变这些数据。 tmpfs mounts的数据只存储在宿主机的内存中，不会写入到宿主机的文件系统。 详细对比Volumes：由Docker创建和管理。你可以通过docker volume create命令显式地创建volume，Docker也可以在创建容器或服务是自己创建volume。 例如下面： 12345678910111213☁ Docker docker volume lsDRIVER VOLUME NAMElocal my-vol☁ Docker docker run -d \\ -it \\ --name demo \\ --mount type=volume,target=/app \\ nginx:latest7eb04d34a87ecd4a7996f8ad4da83bd613d2e992f6860faf730b876a66efd19f☁ Docker docker volume lsDRIVER VOLUME NAMElocal 0c06952c157c3b6550e7f71a9c6463a1b70c893c15b0d8e0b8d05ad398ced427local my-vol 第一行：先查看下volume列表 第四行：创建container，并使用type=volume类型，指向container内部app目录 第十行：查看最新的volume列表 当你创建了一个volume，它会被存放在宿主机的一个目录下。当你将这个volume挂载到某个容器时，这个目录就是挂载到容器的东西。这一点和bind mounts类似，除了volumes是由Docker创建的，和宿主机的核心（core functionality）隔离。 一个volume可以同时被挂载到几个容器中。即使没有正在运行的容器使用这个volume，volume依然存在，不会被自动清除。可以通过docker volume prune清除不再使用的volumes。 volumes也支持volume driver，可以将数据存放在另外的机器或者云上。 Bind mounts：Docker早期就支持这个特性。与volumes相比，Bind mounts支持的功能有限。使用bind mounts时，可以将你主机上的任何文件或目录（绝对路径）挂载到容器中。 警告：使用Bind mounts的一个副作用是，容器中运行的程序可以修改宿主机的文件系统，包括创建，修改，删除重要的系统文件或目录。这个功能可能会有安全问题。 tmpfs mounts：tmpfs mounts的数据不会落盘。在容器的生命周期内，它可以被用来存储一些不需要持久化的状态或敏感数据。例如，swarm服务通过tmpfs mounts来将secrets挂载到一个服务的容器中去。 如何选择？适合volume的场景 在不同的容器中共享数据，如果未显式创建它，则会在第一次将其装入容器时创建。当该容器停止或被移除时，该卷仍然存在。多个容器可以同时安装相同的卷，可以是读写也可以是只读。仅在您明确删除卷时才会删除卷 当Docker主机不能保证具有给定的目录或文件结构时。卷可帮助您将Docker主机的配置与容器运行时分离。 如果要将容器的数据存储在远程主机或云提供程序上，而不是本地存储。 当你需要备份或迁移数据的时候，当您需要能够将数据从一个Docker主机备份，还原或迁移到另一个Docker主机时，卷是更好的选择。您可以使用卷停止容器，然后备份卷的目录（例如/var/lib/docker/volumes/） 适合bind mounts的场景 宿主机和容器共享配置文件。Docker提供的DNS解决方案就是如此，将宿主机的/etc/resolv.conf挂载到每个容器中。 开发环境需要在宿主机和容器中共享代码。docker的开发就是如此，毕竟容器中一般是没有编辑器的 当Docker主机的文件或目录结构保证与容器所需的绑定安装一致时 适合tmpfs mounts的场景tmpfs mounts主要用在你既不想在容器内，又不想在宿主机文件系统保存数据的时候。这可能是出于安全原因，也可能是你的应用需要写非常多的非持久化数据，tmpfs mounts这时候可以保证容器性能。 ##本文转自这里","categories":[{"name":"容器化服务","slug":"容器化服务","permalink":"https://blog.zhimma.com/categories/容器化服务/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://blog.zhimma.com/tags/Docker/"}]},{"title":"Go语言学习笔记8-Go语言并发","slug":"Go语言学习笔记8-Go语言并发","date":"2019-03-22T06:33:29.000Z","updated":"2019-03-22T07:53:18.522Z","comments":true,"path":"2019/03/22/Go语言学习笔记8-Go语言并发/","link":"","permalink":"https://blog.zhimma.com/2019/03/22/Go语言学习笔记8-Go语言并发/","excerpt":"","text":"Go 语言通过编译器运行时（runtime）,从语言上支持了并发的特性。Go 语言的并发通过goroutine 特性完成。goroutine 类似于线程，但可以根据需要创建多个goroutine并发工作goroutine 是由Go 语言的运行时调度完成，而线程是由操作系统调度完成 Go 语言还提供channel 在多个goroutine 间进行通信。goroutine 和 channel是Go 语言秉承CSP并发模式的重要实现基础。 Go 语言goroutine在编写 Socket 网络程序时，需要提前准备一个线程池为每一个 Socket 的收发包分配一个线程。开发人员需要在线程数量和 CPU 数量间建立一个对应关系，以保证每个任务能及时地被分配到 CPU 上进行处理，同时避免多个任务频繁地在线程间切换执行而损失效率。 如果面对随时随地可能发生的并发和线程处理需求，线程池就不是非常直观和方便了。能否有一种机制：使用者分配足够多的任务，系统能自动帮助使用者把任务分配到 CPU 上，让这些任务尽量并发运作。这种机制在 Go 语言中被称为 goroutine。 goroutine 的概念类似于线程，但 goroutine 是由Go 程序运行时进行调度和管理。Go 程序会智能地将goroutine 中的任务合理地分配给每个CPU Go 程序从main 包的main() 函数开始，在程序启动时，Go 程序就会为 main() 函数创建一个默认的goroutine 使用普通函数创建 goroutineGo 语言程序中使用go关键字为一个函数创建一个goroutine。一个函数可以被创建多个goroutine，一个goroutine必定对应一个函数 格式 为一个普通函数创建goroutine的写法如下： 1go 函数名(参数列表) 使用 go关键字创建goroutine时，被调用函数的返回值会被忽略 如果需要在goroutine中返回数据，需要使用channel特性，通过通道(channel)把数据从goroutine中作为返回值传出 例子 使用 go 关键字，将 running() 函数并发执行，每隔一秒打印一次计数器，而 main 的 goroutine 则等待用户输入，两个行为可以同时进行。请参考下面代码： 123456789101112131415func main() &#123; go running() var input string fmt.Scanln(&amp;input) // 接受用户输入，直到按 Enter 键时将输入的内容写入 input 变量中并返回，整个程序终止。&#125;func running()&#123; var times int for &#123; times++ fmt.Println(\"tick\" , times) time.Sleep(time.Second) &#125;&#125; 命令行输出如下：tick 1tick 2tick 3tick 4tick 5 代码执行后，命令行会不断地输出 tick，同时可以使用 fmt.Scanln() 接受用户输入。两个环节可以同时进行。 这个例子中，Go 程序在启动时，运行时（runtime）会默认为 main() 函数创建一个 goroutine。在 main() 函数的 goroutine 中执行到 go running 语句时，归属于 running() 函数的 goroutine 被创建，running() 函数开始在自己的 goroutine 中执行。此时，main() 继续执行，两个 goroutine 通过 Go 程序的调度机制同时运作。 使用匿名函数创建goroutinego 关键字后也可以为匿名函数或者闭包启动goroutine 使用匿名函数创建goroutine 使用匿名函数或者闭包创建goroutine时，除了将函数定义部分卸载go 的后面之外，还需要加上匿名函数的调用参数，格式如下： 123go func(参数列表)&#123; 函数体&#125;(调用参数列表) 使用匿名函数创建goroutine例子 在main() 函数中创建一个匿名函数并未匿名函数启动goroutine。 12345678910111213func main() &#123; go func() &#123; var times int for &#123; times ++ fmt.Println(\"tick\" , times) time.Sleep(time.Second) &#125; &#125;() var input string fmt.Scanln(&amp;input)&#125; goroutine 虽然类似于线程概念，但是从调度性能上没有线程细致，而细致程度取决于 Go 程序的 goroutine 调度器的实现和运行环境。 终止 goroutine 的最好方法就是自然返回 goroutine 对应的函数 Go语言GOMAXPROCS(调整并发的运行性能)在 Go 程序运行时（runtime）实现了一个小型的任务调度器。这套调度器的工作原理类似于操作系统调度线程，Go 程序调度器可以高效地将 CPU 资源分配给每一个任务。传统逻辑中，开发者需要维护线程池中线程与 CPU 核心数量的对应关系。同样的，Go 地中也可以通过 runtime.GOMAXPROCS() 函数做到，格式为： 1runtime.GOMAXPROCS(逻辑CPU数量) 这里的逻辑CPU数量可以有以下几种数值： &lt;1不修改任何数值 =1单核心执行 >1多喝并发执行 一般情况下，可以使用 runtime.NumCPU()查询 CPU 数量，并使用runtime.GOMAXPROCS() 函数进行设置，例如： 1runtime.GOMAXPROCS(runtime.NumCPU()) GOMAXPROCS 同时也是一个环境变量，在应用程序启动前设置环境变量也可以起到相同的作用。 并发和并行的区别下面让我们来了解并发和并行之间的区别： 并发：把任务在不同的时间点交给处理器进行处理，在同一时间点，任务并不会同时运行 并行：把每一个任务分配给每一个处理器独立完成。在同一时间点，任务一定是同时运行 两个概念的区别是：任务是否同时执行。举一个生活中的例子：打电话和吃饭。 吃饭时，电话来了，需要停止吃饭去接电话。电话接完后回来继续吃饭，这个过程是并发执行。 吃饭时，电话来了，边吃饭边接电话。这个过程是并行执行。 GO 语言在 GOMAXPROCS 数量与任务数量相等时，可以做到并行执行，但一般情况下都是并发执行。 goroutine 和 coroutine的区别coroutine 与 goroutine 在名字上类似，都可以将函数或者语句在独立的环境中运行，但是它们之间有两点不同： goroutine 可能发生并行执行； 但 coroutine 始终顺序执行。 狭义的说， goroutine 可能发生在多线程环境下，goroutine无法控制自己获取高优先度支持 coroutine始终发生在单线程，coroutine 程序需要主动交出控制器，宿主才能获得控制权并将控制权交给其他coroutine goroutine 间使用 channel 通信，coroutine 使用 yield 和 resume 操作。 coroutine 的运行机制属于协作式任务处理，早期的操作系统要求每一个应用必须遵守操作系统的任务处理规则，应用程序在不需要使用 CPU 时，会主动交出 CPU 使用权。如果开发者无意间或者故意让应用程序长时间占用 CPU，操作系统也无能为力，表现出来的效果就是计算机很容易失去响应或者死机。 goroutine 属于抢占式任务处理，已经和现有的多线程和多进程任务处理非常类似。应用程序对 CPU 的控制最终还需要由操作系统来管理，操作系统如果发现一个应用程序长时间大量地占用 CPU，那么用户有权终止这个任务。 Go语言通道(chan)","categories":[{"name":"Go","slug":"Go","permalink":"https://blog.zhimma.com/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://blog.zhimma.com/tags/Go/"}]},{"title":"Go语言学习笔记7-包(package)","slug":"Go语言学习笔记7-包-package","date":"2019-03-20T08:07:57.000Z","updated":"2019-03-20T10:29:08.045Z","comments":true,"path":"2019/03/20/Go语言学习笔记7-包-package/","link":"","permalink":"https://blog.zhimma.com/2019/03/20/Go语言学习笔记7-包-package/","excerpt":"","text":"[TOC] 终于学到包了，前面有忘得差不多了，特别是接口，晕晕乎乎的，期待学完之后的项目实战 Go 语言的源码复用建立在包(package) 基础之上。Go 语言的入口main() 函数所在的包叫main，main 包想要引用别的代码，必须同样以包的方式进行。 Go 于洋的包与文件夹一一对应，所有与包相关的操作，必须依赖工作目录（GOPATH） GOPATH详解GOPATH 是Go 语言中使用的一个环境变量，它使用绝对路径提供项目的工作目录 1234567891011121314151617181920212223242526☁ ~ go env // 执行 go env 指令，将输出当前 Go 开发包的环境变量状态。 GOARCH=\"amd64\" // GOARCH 表示目标处理器架构GOBIN=\"\" // GOBIN 表示编译器和链接器的安装位置。 GOCACHE=\"/Users/zhimma/Library/Caches/go-build\"GOEXE=\"\"GOFLAGS=\"\"GOHOSTARCH=\"amd64\"GOHOSTOS=\"darwin\"GOOS=\"darwin\" // GOOS 表示目标操作系统 GOPATH=\"/Users/zhimma/go\" // GOPATH 表示当前工作目录GOPROXY=\"\"GORACE=\"\"GOROOT=\"/usr/local/go\" // GOROOT 表示 Go 开发包的安装目录GOTMPDIR=\"\"GOTOOLDIR=\"/usr/local/go/pkg/tool/darwin_amd64\"GCCGO=\"gccgo\"CC=\"clang\"CXX=\"clang++\"CGO_ENABLED=\"1\"GOMOD=\"\"CGO_CFLAGS=\"-g -O2\"CGO_CPPFLAGS=\"\"CGO_CXXFLAGS=\"-g -O2\"CGO_FFLAGS=\"-g -O2\"CGO_LDFLAGS=\"-g -O2\"PKG_CONFIG=\"pkg-config\" 在 Go 1.8 版本之前，GOPATH 环境变量默认是空的。从 Go 1.8 版本开始，Go 开发包在安装完成后，将 GOPATH 赋予了一个默认的目录，参见下表。 平 台 GOPATH 默认值 举 例 Windows 平台 %USERPROFILE%/go C:\\Users\\用户名\\go Unix 平台 $HOME/go /home/用户名/go 使用GOPATH的工程结构在GOPATH 指定的工作目录下，代码总是会保存在$GOPATH/src目录下。在工程经过go build、go install或者go get 等指令后，会将产生的二进制可执行文件放在$GOPATH/bin目录下，生成的中间缓存文件会被保存在 $GOPATH/pkg下。 如果需要将整个源码添加到版本管理工具中时，只需要添加 $GOPATH/src 目录的源码即可。bin 和 pkg 目录的内容都可以由 src 目录生成。 设置和使用GOPATH下面以Linux为例进行演示 设置当前目录为GOPATH1export GOPATH=`pwd` 建立GOPATH中的源码目录使用下面指令创建GOPATH中的src 目录，在src目录下还有一个hello目录，该目录用于保存源码 1mkdir -p src/hello 添加main.go源码文件使用 Linux 编辑器将下面的源码保存为 main.go 并保存到 $GOPATH/src/hello 目录下。 12345package mainimport \"fmt\"func main()&#123; fmt.Println(\"hello\")&#125; 编译源码并运行此时我们已经设定了 GOPATH，因此在 Go 语言中可以通过 GOPATH 找到工程的位置。 在命令行中执行如下指令编译源码： 1go install hello 编译完成的可执行文件会保存在 $GOPATH/bin 目录下。 在 bin 目录中执行 ./hello，命令行输出如下： 1hello world package(创建包)包 是多个Go 源码的集合，是一种高级的代码复用方案，Go 语言默认为我们提供了很多包，如fmt、os、io包等。 包要求在同一个目录下的所有文件的第一行添加如下代码，以标记该文件归属的包： 1package 包名 包的特性如下： 一个目录下的同级文件归属一个包。 包名可以与其目录不同名。 包名为 main 的包为应用程序的入口包，编译源码没有 main 包时，将无法编译输出可执行的文件。 包中的标识符如果想在一个包里引用另外一个包里的标识符（如类型、变量、常量等）时，必须首先将被引用的标识符导出，将要导出的标识符的首字母大写就可以让引用者可以访问这些标识符了。 import 导入包要引用其他包的标识符，可以使用 import 关键字，导入的包名使用双引号包围，包名是从 GOPATH 开始计算的路径，使用/进行路径分隔。 默认导入的写法导入有两种基本格式，即单行导入和多行导入，两种导入方法的导入代码效果是一致的 单行导入单行导入格式如下： 12import \"包1\"import \"包2\" 2) 多行导入当多行导入时，包名在 import 中的顺序不影响导入效果，格式如下： 12345import( \"包1\" \"包2\" …) 导入包后自定义引用的包名在默认导入包的基础上，在导入包路径前添加标识符即可形成自定义引用包，格式如下： 1customName \"path/to/package\" 其中，path/to/package 为要导入的包路径，customName 为自定义的包名。 12345678package mainimport ( renameLib \"chapter08/importadd/mylib\" \"fmt\")func main() &#123; fmt.Println(renameLib.Add(1, 2))&#125; 匿名导入包如果只希望导入包，而不使用任何包内的结构和类型，也不调用包内的任何函数时，可以使用匿名导入包，格式如下： 123import ( _ \"path/to/package\") 其中，path/to/package 表示要导入的包名，下画线_表示匿名导入包。 匿名导入的包与其他方式导入包一样会让导入包编译到可执行文件中，同时，导入包也会触发 init() 函数调用。 包在程序启动前的初始化入口：init在某些需求的设计上需要在程序启动时统一调用程序引用到的所有包的初始化函数，如果需要通过开发者手动调用这些初始化函数，那么这个过程可能会发生错误或者遗漏。我们希望在被引用的包内部，由包的编写者获得代码启动的通知，在程序启动时做一些自己包内代码的初始化工作。 init() 函数的特性如下： 每个源码可以使用 1 个 init() 函数。 init() 函数会在程序执行前（main() 函数执行前）被自动调用。 调用顺序为 main() 中引用的包，以深度优先顺序初始化。 例如，假设有这样的包引用关系：main→A→B→C，那么这些包的 init() 函数调用顺序为： 1C.init→B.init→A.init→main 说明： 同一个包中的多个 init() 函数的调用顺序不可预期。 init() 函数不能被其他函数调用。","categories":[{"name":"Go","slug":"Go","permalink":"https://blog.zhimma.com/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://blog.zhimma.com/tags/Go/"}]},{"title":"Go语言学习笔记6-接口","slug":"Go语言学习笔记6-接口","date":"2019-03-14T08:03:29.000Z","updated":"2019-03-20T08:06:43.078Z","comments":true,"path":"2019/03/14/Go语言学习笔记6-接口/","link":"","permalink":"https://blog.zhimma.com/2019/03/14/Go语言学习笔记6-接口/","excerpt":"","text":"[TOC] 接口本身是调用方和实现方均需要遵守的一种协议，大家按照统一的方法命名参数类型和数量来协调逻辑处理的过程。 Go 语言的接口设计是非侵入式的，接口编写者无须知道接口被哪些类型实现。而接口实现者只需知道实现的是什么样子的接口，但无须指明实现哪一个接口。编译器知道最终编译时使用哪个类型实现哪个接口，或者接口应该由谁来实现。 非侵入式设计是 Go 语言设计师经过多年的大项目经验总结出来的设计之道。只有让接口和实现者真正解耦，编译速度才能真正提高，项目之间的耦合度也会降低不少。 Go语言接口声明定义接口是双方约定的一种合作协议。接口实现者不需要关心接口会被怎样使用，调用者也不需要关心接口的实现细节。接口是一种类型，也是一种抽象结构，不会暴露所含数据的格式、类型及结构。 接口声明的格式每个接口类型由数个方法组成，接口的形式代码如下： 12345type 接口类型名 interface &#123; 方法名1(参数列表1) 返回值列表1 方法名2(参数列表2) 返回值列表2 ...&#125; 对各个部分的说明： 接口类型名： 使用 type 将接口定义为自定义的类型名。Go 语言在接口命名时，一并会在单词后面添加 er，如有写操作的接口叫 Writer，有字符串功能的接口叫 Stringer… 方法名： 当方法名首字母大写时，且这个接口类型名首字母也是大写时，这个方法可以被接口所在的包（package）之外的代码访问 参数列表、返回值列表： 参数列表和返回值列表中的参数变量名可以被忽略，如 123type write interface &#123; Write([]byte) error&#125; 开发中常见接口及写法Go 语言提供的很多包中都有接口，例如 io 包中提供的Writer 接口： 123type Writer interface &#123; Write(p []byte) (n int, err error)&#125; 这个接口可以调用 Write() 方法写入一个字节数组([]byte)，返回值告知写入字节数（n int）和可能发生的错误（err error） Go语言实现接口的条件接口定义后，需要实现接口，调用方才能正确编译通过并使用接口。接口的实现需要遵循两条规则才能让接口可用 条件一：接口的方法与实现接口的类型方式格式一致在类型中添加与接口签名一致的方法就可以实现该接口。签名包括方法中的名称、参数列表、返回参数列表。也就是说，只要实现接口类型中的方法的名称、参数列表、返回参数列表中的任意一项与接口要实现的方法不一致，那么接口的这个方法就不会被实现。 数据写入器的抽象： 123456789101112131415161718192021222324252627282930313233package mainimport \"fmt\"// 定义一个数据写入器type DataWriter interface &#123; // 定义 DataWriter 接口。这个接口只有一个方法，即 WriteData()，入参接收interface&#123;&#125; 类型的 data，返回一个 error 结构表示可能发生的错误 WriteData(data interface&#123;&#125;) error&#125;// 定文件结构，用于实现DataWritertype file struct &#123;&#125;// 实现DataWriter接口的WriteData方法// file 的 WriteData() 方法使用指针接收器。输入一个 interface&#123;&#125; 类型的 data，返回 error。func (f *file) WriteData(data interface&#123;&#125;) error &#123; fmt.Println(\"实现了DataWriter接口的WriteData方法\" , data) return nil&#125;func main() &#123; // 实例化结构file // 实例化 file 赋值给 f，f 的类型为 *file。 f := new(file) // 声明一个DataWriter类型的writer接口变量 var writer DataWriter // 将接口复赋值给f，也就是*file类型 // 将 *file 类型的 f 赋值给 DataWriter 接口的 writer，虽然两个变量类型不一致。但是 writer 是一个接口，且 f 已经完全实现了 DataWriter() 的所有方法，因此赋值是成功的。 writer = f // 使用DataWriter接口进行数据写入 writer.WriteData(\"data\")&#125; 当类型无法实现接口时，编译器会报错，下面列出常见的几种接口无法实现的错误。 1) 函数名不一致导致的报错2) 实现接口的方法签名不一致导致的报错条件二：接口中所有方法均被实现当一个接口中有多个方法时，只有这些方法都被实现了，接口才能被正确编译并使用。 在本节开头的代码中，为 DataWriter中 添加一个方法，代码如下： 123456// 定义一个数据写入器type DataWriter interface &#123; WriteData(data interface&#123;&#125;) error // 能否写入 CanWrite() bool&#125; 新增 CanWrite() 方法，返回 bool。此时再次编译代码，报错： 12cannot use f (type *file) as type DataWriter in assignment: *file does not implement DataWriter (missing CanWrite method) 需要在 file 中实现 CanWrite() 方法才能正常使用 DataWriter()。 Go 语言的接口实现是隐式的，无须让实现接口的类型写出实现了哪些接口。这个设计被称为非侵入式设计。 实现者在编写方法时，无法预测未来哪些方法会变为接口。一旦某个接口创建出来，要求旧的代码来实现这个接口时，就需要修改旧的代码的派生部分，这一般会造成雪崩式的重新编译。 Go语言类型与接口的关系类型和接口之间有一对多和多对一的关系，下面将列举出这些常见的概念，以方便读者理解接口与类型在复杂环境下的实现关系。 一个类型可以实现多个接口一个类型可以同时实现多个接口，而接口见彼此独立，不知道对方的实现 网络上的两个程序通过一个双向的通信连接实现数据的交换，连接的一端称为一个 Socket。Socket 能够同时读取和写入数据，这个特性与文件类似。因此，开发中把文件和 Socket 都具备的读写特性抽象为独立的读写器概念。 Socket 和文件一样，在使用完毕后，也需要对资源进行释放。 把 Socket 能够写入数据和需要关闭的特性使用接口来描述，请参考下面的代码： 12345678910type Socket struct &#123;&#125;// Socket 结构的Write方法实现了io.Writer接口func (s *Socket) Write(p []byte) (n int, err error) &#123; return 0, nil&#125;// 同时，Socket 结构也实现了 io.Close 接口：func (s *Socket) Close() error &#123; return nil&#125; 使用 Socket 实现的 Writer 接口的代码，无须了解 Writer 接口的实现者是否具备 Closer 接口的特性。同样，使用 Closer 接口的代码也并不知道 Socket 已经实现了 Writer 接口，如下图所示。 在代码中使用Socket结构实现的Writer接口和Closer接口代码如下： 12345678910111213141516171819202122type Socket struct &#123;&#125;func (s *Socket) Write(p []byte) (n int, err error) &#123; return 0, nil&#125;func (s *Socket) Close() error &#123; return nil&#125;func usingWriter(writer io.Writer) &#123; writer.Write(nil)&#125;func usingCloser(closer io.Closer) &#123; closer.Close()&#125;func main() &#123; s := new(Socket) usingWriter(s) usingCloser(s)&#125; 多个类型可以实现相同的接口一个接口的方法，不一定需要由一个类型完全实现，接口的方法可以通过在类型中嵌入其他类型或者结构体来实现。也就是说，使用者并不关心某个接口的方法是通过一个类型完全实现的，还是通过多个结构嵌入到一个结构体中拼凑起来共同实现的。 123456789101112131415161718// 一个服务需要满足能够开启和写日志的功能type Service interface &#123; Start() // 开启服务 Log(string) // 日志输出&#125;// 日志器 定义能输出日志的日志器结构。type Logger struct &#123;&#125;// 为 Logger 添加 Log() 方法，同时实现 Service 的 Log() 方法。func (g *Logger) Log(l string) &#123;&#125;// 定义 GameService 结构。type GameService struct &#123; Logger // 在 GameService 中嵌入 Logger 日志器，以实现日志功能。&#125;// 实现Service的Start()方法func (g *GameService) Start() &#123;&#125; 此时，实例化 GameService，并将实例赋给 Service，代码如下： 123var s Service = new(GameService)s.Start()s.Log(“hello”) s 就可以使用 Start() 方法和 Log() 方法，其中，Start() 由 GameService 实现，Log() 方法由 Logger 实现。 接口嵌套Go语言中不同结构体与结构体之间可以嵌套，接口与接口间也可以通过嵌套创造出新的接口 接口与接口嵌套组合而成了新接口，只要接口的所有方法被实现，则这个接口中的所有嵌套接口的方法均可以被调用 系统包中的接口嵌套组合Go 语言的io 包中定义写入器（Writer）、关闭器（Closer）和写入关闭器（WriteClose）3个接口，代码如下： 12345678910type Writer interface &#123; Write(p []byte) (n int, err error)&#125;type Closer interface &#123; Close() error&#125;type WriterCloser interface &#123; Writer Closer&#125; 在代码中使用接口嵌套组合1234567891011121314151617181920type device struct &#123;&#125;func (d *device) Write(p []byte) (n int, err error) &#123; return 0, nil&#125;func (d *device) Close() error &#123; return nil&#125;func main() &#123; // 对 device 实例化，由于 device 实现了 io.WriteCloser 的所有嵌入接口，因此 device 指针就会被隐式转换为 io.WriteCloser 接口。 var wc io.WriteCloser = new(device) // 写入数据 wc.Write(nil) var writeOnly io.Writer = new(device) writeOnly.Write(nil)&#125; 1) io.WriteCloser的实现及调用过程如图 1 所示。 2) io.Writer 的实现调用过程如图 2 所示。 给 io.WriteCloser 或 io.Writer 更换不同的实现者，可以动态地切换实现代码。","categories":[{"name":"Go","slug":"Go","permalink":"https://blog.zhimma.com/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://blog.zhimma.com/tags/Go/"}]},{"title":"Go语言学习笔记5-结构体","slug":"Go语言学习笔记5-结构体","date":"2019-03-12T07:16:13.000Z","updated":"2019-03-14T08:00:27.289Z","comments":true,"path":"2019/03/12/Go语言学习笔记5-结构体/","link":"","permalink":"https://blog.zhimma.com/2019/03/12/Go语言学习笔记5-结构体/","excerpt":"","text":"[TOC] Go 语言通过用自定义的方式形成新的类型，结构体是类型中带有成员的复合类型Go 语言使用结构体和结构体成员来描述真实世界的实体和实体对应的各种属性 Go 语言中的类型可以被实例化，使用new 或&amp;构造的类型实例的类型是类型的指针 结构体成员是由一系列的成员变量构成，这些成员变量也被称为“字段”。字段有以下特性： 字段拥有自己的类型和值 字段名必须唯一 字段的类型也可以是结构体，甚至是字段所在结构体的类型 关于 Go 语言的类(class) Go 语言中没有“类”的概念，也不支持“类”的继承等面向对象的概念。 Go 语言的结构体与“类”都是复合结构体，但 Go 语言中结构体的内嵌配合接口比面向对象具有更高的扩展性和灵活性。 Go 语言不仅认为结构体能拥有方法，且每种自定义类型也可以拥有自己的方法 Go 语言结构体定义Go 语言的关键字type 可以将各种基本类型定义为自定义类型，基本类型包括整型、字符串、布尔等。结构体是一种复合的基本类型，通过 type 定义为自定义类型后，使用结构体更便于使用 结构体的定义格式如下： 1234567891011type 类型名 struct &#123; 字段1 字段1类型 字段2 字段2类型 。。。&#125;type point struct &#123; x int y int xx, yy int&#125; Go语言实例化结构体-结构体分配内存并初始化结构体的定义只是一种内存布局的描述，只有当结构体实例化时，才会真正地分配内存。因此必须在定义结构体并实例化后才能使用结构体的字段 实例化就是根据结构体定义的格式创建一份与格式一致的内存区域，结构体实例与实例间的内存是完全独立的 Go 语言可以通过多种方式实例化结构体，根据实际需要可以选用不同的写法。 基本的实例化形式结构体本身是一种类型，可以像整形、字符串等类型一样，以var 的方式声明结构体即可完成实例化 基本实例化格式如下： 1var ins T 其中，T 为结构体类型，ins 为结构体的实例 12345678910func main() &#123; type point struct &#123; x int y int &#125; var p point p.x = 10 p.y = 20 fmt.Println(p) // &#123;10 20&#125;&#125; 指针类型的结构体Go 语言中，还可以使用 new 关键字对类型（包括结构体、整形。浮点数。字符串等）进行实例化，结构体在实例化后会形成指针类型的结构体 使用new 的格式如下： 1ins := new(T) 其中： T 为类型，可以是结构体、整形、字符串等 ins ：T 类型被实例化后保存到ins 变量中，ins 的类型为 *T，属于指针 Go 语言让我们可以像访问普通结构体一样使用.访问结构体指针的成员。 123456789101112131415func main() &#123; structDemo()&#125;func structDemo() &#123; type Player struct &#123; Name string HealthPoint int MagicPoint int &#125; factory := new(Player) factory.Name = \"鲁班\" factory.HealthPoint = 300 fmt.Println(factory) // &amp;&#123;鲁班 300 0&#125;&#125; 经过 new 实例化的结构体实例在成员赋值上与基本实例化的写法一致。 在 Go 语言中，访问结构体指针的成员变量时可以继续使用.。这是因为 Go 语言为了方便开发者访问结构体指针的成员变量，使用了语法糖（Syntactic sugar）技术，将 ins.Name 形式转换为 (*ins).Name。 取结构体的地址实例化在Go语言中，对结构体进行 &amp; 取地址操作时，视为对该类型进行一次new 的实例化操作，取地址格式如下： 1ins := &amp;T&#123;&#125; 其中： T 表示结构体类型 ins 为结构体的实例，类型为 *T ，是指针类型 下面使用结构体定义一个命令行指令（Command），指令中包含名称、变量关联和注释等。对 Command 进行指针地址的实例化，并完成赋值过程，代码如下： 12345678910type Command struct &#123; Name string // 指令名称 Var *int // 指令绑定的变量 Comment string // 指令的注释&#125;var version int = 1cmd := &amp;Command&#123;&#125;cmd.Name = \"version\"cmd.Var = &amp;versioncmd.Comment = \"show version\" 取地址实例化是最广泛的一种结构体实例化方式。可以使用函数封装上面的初始化过程，代码如下： 123456789101112func newCommand(name string, varref *int, comment string) *Command &#123; return &amp;Command&#123; Name: name, Var: varref, Comment: comment, &#125;&#125;cmd = newCommand( \"version\", &amp;version, \"show version\",) Go语言初始化结构体的成员变量结构体在实例化时可以直接对成员变量进行初始化。初始化有两种形式： 字段“键值对”形式 多个值的列表形式 键值对形式的初始化适合选择性填充字段较多的结构体； 多个值的列表形式适合填充字段较少的结构体 使用键值对初始化结构体结构体可以使用键值对初始化字段，每个键对应结构体中的一个字段。键的值对应字段需要初始化的值,键值对的填充是可选的，不需要初始化的字段可以不填入初始化列表中 结构体实例化后字段的默认值是字段类型的默认值。例如：数值为 0，字符串为空字符串，布尔为 false，指针为 nil 等 键值对初始化结构体格式如下 12345ins := 结构体类型名&#123; 字段1: 字段1的值， 字段2：字段2的值， ...&#125; 下面是对各个部分的说明： 结构体类型：定义结构体时的类型名称。 字段1、字段2：结构体的成员字段名。结构体类型名的字段初始化列表中，字段名只能出现一次。 字段1的值、字段2的值：结构体成员字段的初始值。 键值之间以:分隔；键值对之间以,分隔。 键值对初始化结构体的例子1234567891011121314type People struct &#123; name string child *People&#125;relation := &amp;People&#123; name: \"爷爷\", child: &amp;People&#123; name: \"爸爸\", child: &amp;People&#123; name: \"孩子\", &#125;, &#125;,&#125; 结构体成员中只能包含结构体的指针类型，包含非指针类型会引起编译错误。 使用多个值得列表初始化结构体Go 语言可以在键值对初始化的基础上忽略键，也就是说，可以使用多个值的列表初始化结构体的字段 多个值列表初始化结构体的书写格式12345ins := 结构体名&#123; 字段1的值， 字段2的值， ...&#125; 使用这种格式初始化时，需要注意： 必须初始化结构体的所有字段 每一个初始值得填充顺序必须与结构体中的声明顺序一致 键值对与值列表的初始化形式不能混用 多个值列表初始化结构体的实例12345678910111213type Address struct &#123; Province string City string ZipCode int PhoneNumber string&#125;addr := Address&#123; \"四川\", \"成都\", 610000, \"0\",&#125;fmt.Println(addr) // &#123;四川 成都 610000 0&#125; 初始化匿名结构体匿名结构体没有类型名称，无需通过 type关键字定义就可以直接使用 匿名结构体定义格式和初始化写法匿名结构体的初始化写法由结构体定义和键值对初始化两部分组成结构体定义时没有结构体类型名，只有字段和字段类型的定义键值对初始化部分由可选的多个键值对组成，如下格式： 1234567891011ins := struct &#123; // 匿名结构体字段定义 字段1 字段1类型 字段2 字段2类型 ...&#125;&#123; // 字段值初始化 初始化字段1： 字段1的值, 初始化字段2： 字段2的值, ...&#125; 键值对初始化部分是可选的，不初始化成员时，匿名结构体的格式为： 123456ins := struct &#123; // 匿名结构体字段定义 字段1 字段1类型 字段2 字段2类型 ...&#125; 使用匿名结构体的例子在本例中，使用匿名结构体的方式定义和初始化一个消息结构，这个消息结构具有消息标示部分（ID）和数据部分（data）。打印消息内容的 printMsg() 函数在接收匿名结构体时需要在参数上重新定义匿名结构体，代码如下： 1234567891011121314151617181920212223package mainimport ( \"fmt\")// 打印消息类型, 传入匿名结构体func printMsgType(msg *struct &#123; id int data string&#125;) &#123; // 使用动词%T打印msg的类型 fmt.Printf(\"%T\\n\", msg)&#125;func main() &#123; // 实例化一个匿名结构体 msg := &amp;struct &#123; // 定义部分 id int data string &#125;&#123; // 值初始化部分 1024, \"hello\", &#125; printMsgType(msg) // *struct &#123; id int; data string &#125;&#125; 代码说明如下： 第 6 行，定义 printMsgType() 函数，参数为 msg，类型为 *struct{id int data string}。因为类型没有使用 type 定义，所以需要在用到的地方每次进行定义。 第 11 行，使用字符串格式化中的%T动词，将 msg 的类型名打印出来。 第 15 行，对匿名结构体进行实例化，同时初始化成员。 第 16 和 17 行，定义匿名结构体的字段。 第 19 和 20 行，给匿名结构体字段赋予初始值。 第 22 行，将 msg 传入 printMsgType() 函数中进行函数调用。 匿名结构体的类型名是结构体包含字段成员的详细描述。匿名结构体在使用时需要重新定义，造成大量重复的代码，因此开发中较少使用。 Go语言的构造函数Go 语言的类型或结构体没有构造函数的功能，结构体的初始化过程可以使用函数封装实现 TODO 没搞懂Go语言方法和接收器Go语言中的方法是一种作用于特定类型变量的函数。这种特定类型变量叫做接收器(Receiver) 如果将特定类型理解为结构体或类时，接收器的概念就类似于其他语言中的 this 或者 self 在Go语言中，接收器的类型可以使任何类型，不仅仅是结构体，任何类型都可以拥有方法 提示在面向对象的语言中，类拥有的方法一般被理解为类可以做的事情。在 Go 语言中“方法”的概念与其他语言一致，只是 Go 语言建立的“接收器”强调方法的作用对象是接收器，也就是类实例，而函数没有作用对象。 为结构体添加方法面向过程实现方法面向过程中没有方法 的概念，只能通过结构体和函数，有使用者使用函数参数和调用关系来形成接近方法的概念： 1234567891011type Bag struct &#123; items []int&#125;// 将一个物品放入背包的过程func Insert(b *Bag, itemid int) &#123; b.items = append(b.items, itemid)&#125;func main() &#123; bag := new(Bag) Insert(bag, 1001)&#125; Insert() 函数将 Bag 参数放在第一位，强调 Insert 会操作 Bag 结构体。但实际使用中，并不是每个人都会习惯将操作对象放在首位。一定程度上让代码失去一些范式和描述性。同时，Insert() 函数也与 Bag 没有任何归属概念。随着类似 Insert() 的函数越来越多，面向过程的代码描述对象方法概念会越来越麻烦和难以理解。 Go语言的结构体方法12345678910type Bag struct &#123; items []int&#125;func (b *Bag) Insert(itemid int) &#123; b.items = append(b.items, itemid)&#125;func main() &#123; b := new(Bag) b.Insert(1001)&#125; 每个方法只能有一个接收器，如下图所示。 接收器-方法作用的目标接收器的格式如下： 123func (接收器变量 接收器类型) 方法名(参数列表)(返回参数)&#123; 函数体&#125; 对各部分的说明： 接收器变量：接收器中的参数变量名在命名时，官方建议使用接收器类型名的第一个小写字母，而不是 self、this 之类的命名。例如，Socket 类型的接收器变量应该命名为 s，Connector 类型的接收器变量应该命名为 c 等。 接收器类型：接收器类型和参数类似，可以是指针类型和非指针类型。 方法名、参数列表、返回参数：格式与函数定义一致。 接收器根据接收器的类型可以分为指针接收器、非指针接收器。两种接收器在使用时会产生不同的效果。根据效果的不同，两种接收器会被用于不同性能和功能要求的代码中。 指针类型接收器指针类型的接收器由一个结构体的指针组成，更接近与面向对象中的this 或者 self由于指针的特性，调用方法时，修改接收器指针的任意成员变量，在方法结束后，修改都是有效的。 1234567891011121314151617181920212223package mainimport \"fmt\"// 定义属性结构type Property struct &#123; value int // 属性值&#125;// 设置属性值func (p *Property) SetValue(v int) &#123; // 修改p的成员变量 p.value = v&#125;// 取属性值func (p *Property) Value() int &#123; return p.value&#125;func main() &#123; // 实例化属性 p := new(Property) // 设置值 p.SetValue(100) // 打印值 fmt.Println(p.Value()) // 100&#125; 非指针类型接收器当方法作用于非指针接收器时，Go 语言会在代码运行时将接收器的值复制一份。在非指针接收器的方法中可以获取接收器的成员值，但修改后无效。 1234567891011121314151617181920212223package mainimport ( \"fmt\")// 定义点结构type Point struct &#123; X int Y int&#125;// 非指针接收器的加方法func (p Point) Add(other Point) Point &#123; // 成员值与参数相加后返回新的结构 return Point&#123;p.X + other.X, p.Y + other.Y&#125;&#125;func main() &#123; // 初始化点 p1 := Point&#123;1, 1&#125; p2 := Point&#123;2, 2&#125; // 与另外一个点相加 result := p1.Add(p2) // 输出结果 fmt.Println(result) // &#123;3 3&#125;&#125; 由于例子中使用了非指针接收器，Add() 方法变得类似于只读的方法，Add() 方法内部不会对成员进行任何修改 指针和非指针接收器的使用在计算机中，小对象由于值复制时的速度较快，所以适合使用非指针接收器。大对象因为复制性能较低，适合使用指针接收器，在接收器和参数间传递时不进行复制，只是传递指针。 Go语言为任意类型添加方法Go语言可以对任何类型添加方法。给一种类型添加方法就像给结构体添加方法一样，因为结构体也是一种类型 为基本类型添加方法在Go语言中，使用 type 关键字可以定义出新的自定义类型。之后就可以为自定义类型添加各种方法。我们习惯于使用面向过程的方法判断一个值是否为0，例如： 123if v == 0 &#123; // v = 0&#125; 如果将 v 比作整型对象，那么判断 v 值就可以增加一个IsZero() 方法，通过这个方法就可以判断v 值是否为0，例如： 123if v.IsZero()&#123; // v = 0&#125; 为基本类型添加方法的详细流程如下： 1234567891011121314151617181920package mainimport ( \"fmt\")// 将int定义为MyInt类型type MyInt int// 为MyInt添加IsZero()方法func (m MyInt) IsZero() bool &#123; return m == 0&#125;// 为MyInt添加Add()方法func (m MyInt) Add(other int) int &#123; return other + int(m)&#125;func main() &#123; var b MyInt = 2 fmt.Println(b.IsZero()) // false b = 1 fmt.Println(b.Add(2)) //3&#125; Go语言类型内嵌和结构体内嵌结构体允许其成员字段在声明时没有字段名而只有类型，这种形式的字段被称为类型内嵌或匿名字段类型内嵌，写法如下： 12345678910type Data struct &#123; int float32 bool&#125;ins := &amp;Data &#123; int: 10， float32: 3.14, bool: true&#125; 类型内嵌其实仍然拥有自己的字段名，只是字段名就是其类型本身而已，结构体要求字段名称必须唯一，因此一个结构体中同种类型的匿名字段只能有一个 结构体实例化后，如果匿名的字段类型为结构体，那么可以直接访问匿名结构体里的所有成员，这种方式被称为结构体内嵌 声明结构体内嵌结构体类型内嵌比普通类型内嵌的概念复杂一些，下面通过一个实例来理解。 计算机图形学中的颜色有两种类型，一种是包含红、绿、蓝三原色的基础颜色；另一种是在基础颜色之外增加透明度的颜色。透明度在颜色中叫 Alpha，范围为 0～1 之间。0 表示完全透明，1 表示不透明。使用传统的结构体字段的方法定义基础颜色和带有透明度颜色的过程代码如下： 123456789101112131415161718192021222324252627package mainimport ( \"fmt\")// 基础颜色type BasicColor struct &#123; // 红、绿、蓝三种颜色分量 R, G, B float32&#125;// 完整颜色定义type Color struct &#123; // 将基本颜色作为成员 Basic BasicColor // 透明度 Alpha float32&#125;func main() &#123; var c Color // 设置基本颜色分量 c.Basic.R = 1 c.Basic.G = 1 c.Basic.B = 0 // 设置透明度 c.Alpha = 1 // 显示整个结构体内容 fmt.Printf(\"%+v\", c) // &#123;Basic:&#123;R:1 G:1 B:0&#125; Alpha:1&#125;&#125; 这种写法虽然合理但是写法很复杂。使用 Go 语言的结构体内嵌写法重新调整代码如下： 12345678910111213141516171819package mainimport ( \"fmt\")type BasicColor struct &#123; R, G, B float32&#125;type Color struct &#123; BasicColor Alpha float32&#125;func main() &#123; var c Color c.R = 1 c.G = 1 c.B = 0 c.Alpha = 1 fmt.Printf(\"%+v\", c)&#125; 第14-16 行，可以直接对 Color 的 R、G、B 成员进行设置，编译器通过 Color 的定义知道 R、G、B 成员来自 BasicColor 内嵌的结构体。 结构内嵌特性Go 语言的结构体内嵌有如下特性： 1. 内嵌的结构体可以直接访问其成员变量 嵌入结构体的成员，可以通过外部结构体的实例直接访问。如果结构体有多层嵌入结构体，结构体实例访问任意一级的嵌入结构体成员时都只用给出字段名，而无须像传统结构体字段一样，通过一层层的结构体字段访问到最终的字段。例如，ins.a.b.c的访问可以简化为ins.c。 2. 内嵌结构体的字段名是它的类型名 内嵌结构体字段仍然可以使用详细的字段进行一层层访问，内嵌结构体的字段名就是它的类型名，代码如下： 1234var c Colorc.BasicColor.R = 1c.BasicColor.G = 1c.BasicColor.B = 0 一个结构体只能嵌入一个同类型的成员，无须担心结构体重名和错误赋值的情况，编译器在发现可能的赋值歧义时会报错。 Go语言结构体内嵌模拟类的继承在面向对象思想中，实现对象关系需要使用“继承”特性。例如，人类不能飞行，鸟类可以飞行。人类和鸟类都可以继承自可行走类，但只有鸟类继承自飞行类。 Go 语言 的结构体内嵌特性就是一种组合特性，使用组合特性可以快速构建对象的不同特性。 下面的代码使用 Go 语言的结构体内嵌实现对象特性组合，请参考下面的代码。 人和鸟的特性： 1234567891011121314151617181920212223242526272829303132package mainimport \"fmt\"// 可飞行的type Flying struct&#123;&#125;func (f *Flying) Fly() &#123; fmt.Println(\"can fly\")&#125;// 可行走的type Walkable struct&#123;&#125;func (f *Walkable) Walk() &#123; fmt.Println(\"can calk\")&#125;// 人类type Human struct &#123; Walkable // 人类能行走&#125;// 鸟类type Bird struct &#123; Walkable // 鸟类能行走 Flying // 鸟类能飞行&#125;func main() &#123; // 实例化鸟类 b := new(Bird) fmt.Println(\"Bird: \") b.Fly() b.Walk() // 实例化人类 h := new(Human) fmt.Println(\"Human: \") h.Walk()&#125; 运行代码，输出如下： 12345Bird:can flycan calkHuman:can calk 使用 Go 语言的内嵌结构体实现对象特性，可以自由地在对象中增、删、改各种特性。Go 语言会在编译时检查能否使用这些特性。 Go语言初始化内嵌结构体结构体内嵌初始化时，将结构体内嵌的类型作为字段名像普通结构体一样进行初始化，详细实现过程请参考下面的代码。 车辆结构的组装和初始化： 123456789101112131415161718192021222324252627282930package mainimport \"fmt\"// 车轮type Wheel struct &#123; Size int&#125;// 引擎type Engine struct &#123; Power int // 功率 Type string // 类型&#125;// 车type Car struct &#123; Wheel Engine&#125;func main() &#123; c := Car&#123; // 初始化轮子 Wheel: Wheel&#123; Size: 18, &#125;, // 初始化引擎 Engine: Engine&#123; Type: \"1.4T\", Power: 143, &#125;, &#125; fmt.Printf(\"%+v\\n\", c)&#125; 初始化内嵌匿名结构体 在前面描述车辆和引擎的例子中，有时考虑编写代码的便利性，会将结构体直接定义在嵌入的结构体中。也就是说，结构体的定义不会被外部引用到。在初始化这个被嵌入的结构体时，就需要再次声明结构才能赋予数据。具体请参考下面的代码。 1234567891011121314151617181920212223242526272829303132package mainimport &quot;fmt&quot;// 车轮type Wheel struct &#123; Size int&#125;// 车type Car struct &#123; Wheel // 引擎 Engine struct &#123; Power int // 功率 Type string // 类型 &#125;&#125;func main() &#123; c := Car&#123; // 初始化轮子 Wheel: Wheel&#123; Size: 18, &#125;, // 初始化引擎 Engine: struct &#123; Power int Type string &#125;&#123; Type: &quot;1.4T&quot;, Power: 143, &#125;, &#125; fmt.Printf(&quot;%+v\\n&quot;, c)&#125;","categories":[{"name":"Go","slug":"Go","permalink":"https://blog.zhimma.com/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://blog.zhimma.com/tags/Go/"}]},{"title":"Go语言学习笔记4-函数","slug":"Go语言学习笔记4-函数","date":"2019-03-07T08:59:45.000Z","updated":"2019-03-12T07:15:24.688Z","comments":true,"path":"2019/03/07/Go语言学习笔记4-函数/","link":"","permalink":"https://blog.zhimma.com/2019/03/07/Go语言学习笔记4-函数/","excerpt":"","text":"[TOC] 函数是组织好的、可重复使用的、用来实现单一或相关联功能的代码段，其可以提高应用的模块性和代码的重复利用率。 Go 语言支持普通函数、匿名函数和闭包，从设计上对函数进行了优化和改进，让函数使用起来更加方便。 Go 语言的函数属于“一等公民”（first-class），也就是说： 函数本身可以作为值进行传递。 支持匿名函数和闭包（closure）。 函数可以满足接口。 Go语言函数声明(函数定义)普通函数需要先声明才能调用，一个函数的声明包含参数和函数名等，编译器通过声明才能了解函数应该怎样在调用代码和函数体之间传递参数和返回值 普通函数的声明形式Go语言的函数声明以func标识，后面紧接着函数名、参数列表、返回参数列表及函数体，具体形式如下： 123func 函数名(参数列表)(返回参数列表)&#123; 函数体&#125; 下面对各个部分进行说明： 函数名：由字母、数字、下划线组成。其中，函数名的第一个字母不能为数字，在同一个包内，函数名称不能重复 包(package) 是Go源码的一种组织方式，一个包可以认为是一个文件夹 参数列表：一个参数由参数变量和参数类型组成，例如 1func foo(a int, b string) 其中，参数列表中的变量作为函数的局部变量而存在。 返回参数列表：可以是返回值类型列表，也可以是类似参数列表中变量名和类型名的组合。函数在声明有返回值时，必须在函数体中使用return 语句提供返回值列表 函数体：能够被重复调用的代码片段 参数类型的缩写在参数列表中，如果有多个参数变量，则以逗号,分隔；如果相邻变量时同类型，则可以将类型省略，例如： 123func add(a ,b int) int &#123; return a + b&#125; 以上代码中，a 和 b 的参数类型均是 int 类型，则可以省略 a 的类型，在 b 后面有类型说明，这个类型也是 a 的类型 函数的返回值Go语言支持多返回值，多返回值能方便地取得函数执行后的多个返回参数，Go语言经常使用多返回值中的最后一个返回参数返回函数中可能发生的错误。示例如下： 1conn, err := connectToMysql() 在这段代码中，connectToNetwork 返回两个参数，conn 表示连接对象，err 返回错误。 Go 语言既支持安全指针，也支持多返回值，因此在使用函数进行逻辑编写时更为方便。 同一类型返回值如果返回值是同一类型，则用括号将多个返回值类型括起来，用逗号分割每个返回值的类型 使用 return 语句返回时，值列表的顺序需要与函数声明的返回值类型一致，示例如下： 1234func foo()(int, int) &#123; return 1, 2&#125;a, b := foo() 带有变量名的返回值Go 语言支持对返回值进行命名，这样返回值就可以和参数一样拥有参数变量名和类型 命名的返回值变量的默认值为类型的默认值，即数值为 0 ，字符串为空字符串，布尔值为false，指针为 nil 等 123456func foo()(a, b int) &#123; a = 1 b = 2 return&#125;a, b := foo() 调用函数函数在定义后，可以通过调用的方式，让当前代码跳转到被调用的函数中进行执行。调用前的函数局部变量都会被保存起来不会丢失；被调用的函数结束后，恢复到被调用函数的下一行继续执行代码，之前的局部变量也能继续访问。 函数内的局部变量只能在函数体中使用，函数调用结束后，这些局部变量都会被释放并且失效。 Go语言的函数调用格式如下： 1返回值变量列表 = 函数名(参数列表) 下面是对各个部分的说明： 函数名：需要调用的函数名。 参数列表：参数变量以逗号分隔，尾部无须以分号结尾。 返回值变量列表：多个返回值使用逗号分隔。 函数示例-将秒转为具体时间在本例中，使用一个数值表示时间中的“秒”值，然后使用resolveTime()函数将传入的秒数转换为天、小时和分钟等时间单位。 12345678910111213141516171819202122232425package mainimport \"fmt\"var SecondsPerMinute = 60var SecondsPerHour = 60 * SecondsPerMinutevar SecondsPerDay = 24 * SecondsPerHourfunc main() &#123; fmt.Println(resolveTime(1000)) _, hour, minute := resolveTime(1000) fmt.Println(hour, minute) day, _, _ := resolveTime(1000) fmt.Println(day)&#125;func resolveTime(seconds int) (day, hour, minutes int) &#123; day = seconds / SecondsPerDay hour = seconds / SecondsPerHour minutes = seconds / SecondsPerMinute return&#125; Go语言中传入参数和返回参数 在调用和返回时都使用值传递，这里需要注意的是指针、切片和map等引用型对象指向的内容在参数传递中不会发生复制，而是将指针进行复制，类似于创建一次引用 函数变量-把函数作为值保存到变量中在Go语言中，函数也是一种类型，可以和其他类型一样被保存在变量中。 123456789func main() &#123; var f func() f = foo f() &#125;func foo()&#123; fmt.Println(\"foo func\")&#125; Go语言匿名函数Go 语言支持匿名函数，即在需要使用函数时在定义函数，匿名函数没有函数名，只有函数体，函数可以被作为一种类型被赋值给函数类型的变量，匿名函数也往往以变量方式被传递 匿名函数经常被用于实现回调函数、闭包等 定义一个匿名函数匿名函数的定义格式如下： 123func(参数列表)(返回参数列表)&#123; 函数体&#125; 匿名函数的定义就是没有名字的普通函数定义 在定义时调用匿名函数匿名函数可以在声明后调用： 123func(data int) &#123; fmt.Prinln(data) &#125;(100) 将匿名函数赋值给变量匿名函数体可以被赋值： 1234f := func(data int) &#123; fmt.Println(data)&#125;f(100) 匿名函数的用途非常广泛，匿名函数本身是一种值，可以方便地保存在各种容器中实现回调函数和操作封装。 匿名函数用作回调函数下面的代码实现对切片的遍历操作，遍历中访问每个元素的操作使用匿名函数来实现。用户传入不同的匿名函数体可以实现对元素不同的遍历操作，代码如下： 123456789101112131415161718192021package mainimport ( &quot;fmt&quot;)// 遍历切片的每个元素, 通过给定函数进行元素访问func visit(list []int, f func(int)) &#123; for _, v := range list &#123; f(v) &#125;&#125;func main() &#123; // 使用匿名函数打印切片内容 visit([]int&#123;1, 2, 3, 4&#125;, func(v int) &#123; fmt.Println(v) &#125;)&#125; 匿名函数作为回调函数的设计在 Go 语言的系统包中也比较常见，strings 包中就有如下代码： 123func TrimFunc(s string, f func(rune) bool) string &#123; return TrimRightFunc(TrimLeftFunc(s, f), f)&#125; Go语言函数类型实现接口函数和其他类型一样都属于“一等公民”，其他类型能够实现接口，函数也可以，本节将分别对比结构体与函数实现接口的过程。 TODOGo语言闭包闭包是引用了自由变量的函数，被引用的自由变量和函数一同存在，即使已经离开了自由变量的环境也不会被释放或者删除，在闭包中可以继续使用这个自由变量，因此简单的说： 1函数 + 引用环境 = 闭包 一个函数类型就像结构体一样，可以被实例化。函数本身不存储任何信息，只有与引用环境结合后形成的闭包才具有“记忆性”。函数是编译期静态的概念，而闭包是运行期动态的概念。 在闭包内部修改引用的变量闭包对它作用域上的变量的引用可以进行修改，修改引用的变量就会对变量进行实际的修改。例如 12345678910str := \"hello\"// 创建一个匿名函数foo := func() &#123; // 匿名函数中访问str str = \"world\"&#125;foo()fmt.Println(str); // world 闭包的记忆效应被捕获到闭包中的变量让闭包本身拥有了记忆效应，闭包中的逻辑可以修改闭包捕获的变量，变量会跟随闭包生命期一直存在，闭包本身就如同变量一样拥有了记忆效应。 12345678910111213141516171819202122package mainimport \"fmt\"func addValue(value int) func() int &#123; return func() int &#123; value++ return value &#125;&#125;func main() &#123; // 创建一个累加器，初始值1 value := addValue(1) fmt.Println(value()) // 2 fmt.Println(value()) // 3 // 打印累加器的函数地址 fmt.Printf(\"%p\\n\" , value) // 创建一个累加器，初始值为1 otherValue := addValue(1) fmt.Println(otherValue()) // 2&#125; value 与 otherValue的函数地址不同，因此它们是两个不同的闭包实例 闭包的记忆效应进程被用于实现类似于设计模式中工厂模式的生成器。 Go语言可变参数(变参函数)所谓可变参数，是指参数数量不固定的函数形式。Go语言支持可变参数特性，函数声明和调用时没有固定数量的参数，同时也提供了一套方法进行可变参数的多级传参 Go语音的可变参数格式如下： 123func 函数名(固定参数列表，v ... T)(返回参数列表)&#123; 函数体&#125; 可变参数一般被放置在函数列表的末尾，前面是固定参数列表，当没有固定参数时，所有变量将是可变参数 v 为可变参数变量，类型为[]T,也就是拥有多个T 元素的 T 类型的切片v 和 T 之前由...组成 T 为可变参数的类型，当T 为interface{}时，传入的可以使任意类型 fmt包中的例子可变参数有两种类型：所有参数都是可变参数的形式，如fmt.Println,以及部分是可变参数的形式，如 fmt.Printf，可变参数只能出现在参数的后半部分，因此不可变的参数只能放在参数的前半部分。 所有参数都是可变参数：fmt.Println123func Println(a ...interface&#123;&#125;) (n int, err error)&#123; return Fprintln(os.Stdout, a...)&#125; fmt.Println 在使用时，传入的值类型不收限制，例如： 1fmt.Println(1, 2, 3, \"string\", true); 部分参数是可变参数： fmt.Printffmt.Printf 的第一个参数为参数列表， 后面的参数是可变参数，fmt.Printf函数格式如下： 123func Printf(format string, a ...interface&#123;&#125;) (n int, err error)&#123; return Fprintf(os.Stdout, format, a...)&#125; fmt.Printf() 函数在调用时，第一个函数始终必须传入字符串，对应参数是 format，后面的参数数量可以变化 Go语音defer(延迟执行语句)Go语音的defer 语句会将其后面跟随的语句进行延迟处理。在defer 归属的函数即将返回时，将延迟处理的语句按defer 的逆序进行执行，也就是说，先被defer 的语句最后执行，最后defer 的语句，最后被执行 多个延迟执行语句的处理顺序12345678fmt.Println(\"第一行\")defer fmt.Println(\"第二行\")defer fmt.Println(\"第三行\")fmt.Println(\"最后一行\")// 第一行// 最后一行// 第三行// 第二行 代码的延迟顺序与最终执行顺序是反向的 延迟调用是在 defer 所在函数结束时执行，函数结束可以是正常返回，也可以是出错时返回 使用延迟执行语句在函数退出时释放资源处理业务或逻辑中涉及成对的操作是一件比较繁琐的事情，比如打开和关闭文件、和解锁接受请求和回复请求、加锁等。在这些操作中，最容易忽略的就是在每个函数退出处正确地释放和关闭资源。 defer 语句正好是在函数退出时执行的语句，所以使用 defer 能非常方便地处理释放资源的问题 使用 defer 延迟释放资源12345678910111213141516func fileSize(filename string) int64 &#123; f, err := os.Open(filename) if err != nil&#123; return 0 &#125; // 延迟调用close ，此时close 不会被调用 defer f.Close() info , err := f.Stat() if err != nil&#123; return 0 &#125; size := info.Size() // defer 机制触发，调用close关闭文件 return size&#125; defer 后的语句（f.Close()）将会在函数返回前被调用，自动释放资源 Go语言处理运行时错误Go语言的错误处理思想及设计包含以下特征： 一个可能造成错误的函数，需要返回值中返回一个错误接口(error) 。如果调用是成功的，错误接口将返回nil , 否则返回错误 在函数调用后需要检查错误，如果发生错误，需要进行必要的错误处理 Go 语言没有类似 Java或 .NET 中的异常处理机制，虽然可以使用 defer、panic、recover 模拟，但官方并不主张这样做。Go 语言的设计者认为其他语言的异常机制已被过度使用，上层逻辑需要为函数发生的异常付出太多的资源。同时，如果函数使用者觉得错误处理很麻烦而忽略错误，那么程序将在不可预知的时刻崩溃。 Go 语言希望开发者将错误处理视为正常开发必须实现的环节，正确地处理每一个可能发生错误的函数。同时，Go 语言使用返回值返回错误的机制，也能大幅降低编译器、运行时处理错误的复杂度，让开发者真正地掌握错误的处理。 Go语言宕机(panic)，程序终止执行手动触发宕机Go 语言中可以在程序中手动触发宕机，让程序崩溃，这样使开发者可以及时的发现错误，同时减少可能的损失 Go 语言宕机时，会将堆栈和goroutine 信息输出到控制台，所以宕机也可以方便的知晓发生错误的位置 123456789101112package mainfunc main() &#123; panic(\"crash\") /** panic: crash goroutine 1 [running]: main.main() /Users/zhimma/go/src/awesomeProject/0312.go:4 +0x39 */&#125; 以上代码中只用了一个内建的函数 panic() 就可以造成崩溃，panic() 的声明如下： 1func panic(v interface&#123;&#125;) panic() 的参数可以是任意类型，后文将提到的 recover 参数会接收从 panic() 中发出的内容。 在宕机时触发延迟执行语句当panic() 触发的宕机发生时， panic() 后面的代码将不会被运行，但是在 panic() 函数前面已经运行的defer语句依然会在宕机时发生作用，例如下面的实例： 1234567891011121314151617package mainimport \"fmt\"func main() &#123; defer fmt.Println(\"宕机后执行的事情1\") panic(\"crash\") /** 宕机后执行的事情1 panic: crash goroutine 1 [running]: main.main() /Users/zhimma/go/src/awesomeProject/0312.go:8 +0xf1 &#125; */&#125; 宕机时，defer 语句会优先被执行 测试了下，结果好像不一定，有时候先panic,有时候先defer，以后了解了再来补充吧 Go语言恢复(recover)宕机,防止程序崩溃无论是代码运行错误，还是由Runtime层抛出的 panic 奔溃，还是主动出发的 panic 奔溃，都可以配合defer 和 recover 实现错误捕捉和恢复，让代码在发生奔溃后允许继续运行 在其他语言中，宕机往往以异常的形式存在。底层抛出异常，上层逻辑通过 try/catch 机制捕获异常，没有被捕获的严重异常会导致宕机，不活的异常可以被忽略，让代码继续运行 Go 语言没有异常系统，其使用 panic 触发宕机类似其他语言的抛出异常，那么 recover 的宕机恢复机制就是对应的 try/catch 机制 让程序崩溃时继续执行12345678910111213141516171819202122232425262728293031323334353637383940414243package mainimport ( \"fmt\" \"runtime\")type panicContext struct &#123; function string&#125;func main() &#123; fmt.Println(\"开始运行\") recoverDemo(func() &#123; fmt.Println(\"手动宕机前-----\") panic((&amp;panicContext&#123; \"手动触发panic\", &#125;)) &#125;) fmt.Println(\"手动宕机后\") recoverDemo(func() &#123; fmt.Println(\"赋值宕机前\") var a *int *a = 1 fmt.Println(\"赋值宕机后\") &#125;) fmt.Println(\"结束宕机\")&#125;func recoverDemo(entry func()) &#123; defer func() &#123; // 发生宕机时，获取panic传递的上下文并打印 err := recover() switch err.(type) &#123; case runtime.Error: // 运行时错误 fmt.Println(\"runtime error\", err) default: fmt.Println(\"error\", err) &#125; &#125;() entry()&#125; 1234567开始运行手动宕机前-----error &amp;&#123;手动触发panic&#125;手动宕机后赋值宕机前runtime error runtime error: invalid memory address or nil pointer dereference结束宕机 panic和recover的关系panic 和 defer 的组合有如下特性： 有 panic 没 recover，程序宕机。 有 panic 也有 recover 捕获，程序不会宕机。执行完对应的 defer 后，从宕机点退出当前函数后继续执行。 提示虽然 panic/recover 能模拟其他语言的异常机制，但并不建议代表编写普通函数也经常性使用这种特性。 在 panic 触发的 defer 函数内，可以继续调用 panic，进一步将错误外抛直到程序整体崩溃。 如果想在捕获错误时设置当前函数的返回值，可以对返回值使用命名返回值方式直接进行设置。","categories":[{"name":"Go","slug":"Go","permalink":"https://blog.zhimma.com/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://blog.zhimma.com/tags/Go/"}]},{"title":"Go语言学习笔记3-流程控制","slug":"Go语言学习笔记3-流程控制","date":"2019-03-07T06:54:49.000Z","updated":"2019-03-07T08:58:00.197Z","comments":true,"path":"2019/03/07/Go语言学习笔记3-流程控制/","link":"","permalink":"https://blog.zhimma.com/2019/03/07/Go语言学习笔记3-流程控制/","excerpt":"","text":"[TOC] Go 语言的常用流程控制有 if 和 for，而 switch 和 goto 主要是为了简化代码、降低重复代码而生的结构，属于扩展类的流程控制。 Go 语言中的基本流程控制语句，包括分支语句（if 和 switch）、循环（for）和跳转（goto）语句。还有循环控制语句（break 和 continue），前者的功能是中断循环或者跳出 switch 判断，后者的功能是继续 for 的下一个循环。 if else (分支结构)在Go语言中可以通过 if 关键字进行条件判断，格式如下： 1234567if 表达式1 &#123; 分支1&#125; else if 表达式2 &#123; 分支2&#125; else&#123; 分支3&#125; Go 语言规定与 if 匹配的左括号{必须与 if 和表达式放在同一行，如果尝试将{放在其他位置，将会触发编译错误。 特殊写法Go 语言规定与 if 匹配的左括号{必须与 if 和表达式放在同一行，如果尝试将{放在其他位置，将会触发编译错误。 1234if err := Connect(); err != nil &#123; fmt.Println(err) return&#125; Connect 是一个带有返回值的函数，err:=Connect() 是一个语句，执行 Connect 后，将错误保存到 err 变量中。 err！=nil才是 if 的判断表达式，当 err 不为空时，打印错误并返回。 循环语句forGo语言中所有的循环类型均可以使用for 关键字来完成 基于语句和表达式的基本for 循环格式如下： 123for 初始语句;条件表达式;结束表达式&#123; 循环体代码&#125; 循环体不停地进行循环，直到条件表达式返回 false 时自动退出循环，执行 for 的}之后的语句 for 循环可以通过break、goto、return、panic 语句强制退出循环。for 的初始语句、条件表达式、结束语句的详细介绍如下。 for 中的初始语句——开始循环时执行的语句初始语句是在第一次循环前执行的语句，一般使用初始语句执行变量初始化，如果变量在此处被声明，其作用域将被局限在这个for 的范畴内 注意：初始语句可以被忽略，但是初始语句之后的分号必须填写，代码如下： 1234stop := 2for ; step &gt; 0; step-- &#123; fmt.Println(step)&#125; 这段代码将 step 放在 for 的前面进行初始化，for 中没有初始语句，此时 step 的作用域就比在初始语句中声明 step 要大。 for 中的条件表达式——控制是否循环的开关对每次循环开始前计算的表达式，如果表达式为true ,则循环继续，否则结束循环，条件表达式可以被忽略，被忽律条件的条件表达式默认形成无限循环 结束循环时带可执行语句的无限循环123456var i intfor ; ; i++ &#123; if i &gt; 10 &#123; break &#125;&#125; 无线循环1234567var i intfor &#123; if i &gt; 10 &#123; break &#125; i++&#125; 只有一个循环条件的循环1234var i intfor i &lt;= 10 &#123; i++&#125; for 中的结束语句——每次循环结束时执行的语句在结束每次循环前执行的语句，如果循环被 break、goto、return、panic 等语句强制退出，结束语句不会被执行。 Demo 九九乘法表123456for x := 1; x &lt;= 9; x++ &#123; for y := 1; y &lt;= x; y++ &#123; fmt.Printf(\"%d*%d=%d \", x, y, x*y) &#125; fmt.Println();&#125; 1234567891*1=1 2*1=2 2*2=4 3*1=3 3*2=6 3*3=9 4*1=4 4*2=8 4*3=12 4*4=16 5*1=5 5*2=10 5*3=15 5*4=20 5*5=25 6*1=6 6*2=12 6*3=18 6*4=24 6*5=30 6*6=36 7*1=7 7*2=14 7*3=21 7*4=28 7*5=35 7*6=42 7*7=49 8*1=8 8*2=16 8*3=24 8*4=32 8*5=40 8*6=48 8*7=56 8*8=64 9*1=9 9*2=18 9*3=27 9*4=36 9*5=45 9*6=54 9*7=63 9*8=72 9*9=81 for range (键值循环)Go 语言可以使用for range遍历数组、切片、字符串、map 及通道（channel）。通过 for range遍历的返回值有一定的规律： 数组、切片、字符串返回索引和值。 map 返回键和值。 通道（channel）只返回通道内的值。 遍历数组、切片——获得索引和元素在遍历代码中，key 和 value 分别代表切片的下标及下标对应的值。下面的代码展示如何遍历切片，数组也是类似的遍历方法： 1234567for key, value := range []int&#123;1, 2, 3, 4&#125; &#123; fmt.Println(key, \"=&gt;\", value) // 0 =&gt; 1 // 1 =&gt; 2 // 2 =&gt; 3 // 3 =&gt; 4 &#125; 遍历字符串——获得索引和元素Go 语言和其他语言类似，可以通过 for range 的组合，对字符串进行遍历，遍历时，key 和 value 分别代表字符串的索引（base0）和字符串中的每一个字符。 123456789101112131415str := \"你好，zhimma\" for key, value := range str &#123; fmt.Printf(\"key:%d value:0x%x\\n\", key, value) /** key:0 value:0x4f60 type:int32 key:3 value:0x597d type:int32 key:6 value:0xff0c type:int32 key:9 value:0x7a type:int32 key:10 value:0x68 type:int32 key:11 value:0x69 type:int32 key:12 value:0x6d type:int32 key:13 value:0x6d type:int32 key:14 value:0x61 type:int32 */ &#125; 代码中的 value 变量，实际类型是 rune，实际上就是 int32，以十六进制打印出来就是字符的编码。 遍历map——获得map的键和值对于 map 类型来说，for range 遍历时，key 和 value 分别代表 map 的索引键 key 和索引对应的值，一般被称为 map 的键值对，因为它们总是一对一对的出现。下面的代码演示了如何遍历 map: 12345678910111213family := map[string]string&#123; \"dad\": \"zhimma dad\", \"mom\": \"zhimma mom\", \"daughter\": \"zhimma\", &#125;for key, value := range family &#123; fmt.Println(\"hello\", key, value) /** hello dad zhimma dad hello mom zhimma mom hello daughter zhimma */&#125; 对 map 遍历时，遍历输出的键值是无序的，如果需要有序的键值对输出，需要对结果进行排序。 遍历通道（channel）——接收通道数据for range可以遍历通道（channel），但是通道在遍历时，只输出一个值，即管道内的类型对应的数据 12345678910c := make(chan int)go func() &#123; c &lt;- 1 c &lt;- 2 c &lt;- 3 close(c)&#125;()for v := range c &#123; fmt.Println(v)&#125; switch case 语句分支选择可以理解为一种批量的if语句，使用 switch 语句可方便地对大量的值进行判断。 在 Go 语言中的 switch，不仅可以基于常量进行判断，还可以基于表达式进行判断。 基本写法Go 语言改进了 switch 的语法设计，避免人为造成失误。Go 语言的 switch 中的每一个 case 与 case 间是独立的代码块，不需要通过 break 语句跳出当前 case 代码块以避免执行到下一行。示例代码如下： 12345678910a := \"hello\"switch a &#123; case \"hello\": fmt.Println(\"hello\") case \"other\": fmt.Println(\"other\") default: fmt.Println(\"default\")&#125; 上面例子中，每个case 均是字符串格式，且使用了default 分支，Go语言规定每个 switch 只能有一个default 分支 一分支多值当出现多个 case 要放在一起的时候，可以像下面代码这样写： 12345var a = \"mum\"switch a &#123; case \"mum\" , \"dad\": fmt.Println(\"family\");&#125; 不通的 case 表达式使用逗号分隔 分支表达式case 后不仅仅只是常量，还可以和 if 一样添加表达式，代码如下： 123456num := 10switch &#123; case num &lt; 20 || num &gt; 20: fmt.Println(\"num value is \", num) // num value is 10&#125; 这种情况的 switch 后面不再跟判断变量，连判断的目标都没有了。 goto 语句——跳转到指定的标签goto 语句通过标签进行代码间的无条件跳转。goto 语句可以在快速跳出循环、避免重复退出上有一定帮助，使用 goto 语句能简化一些代码的实现过程。 使用 goto 退出多层循环多层循环中，传统方式退出 1234567891011121314151617181920212223package mainimport \"fmt\"func main() &#123; var breakAgain bool // 外循环 for x := 0; x &lt; 10; x++ &#123; // 内循环 for y := 0; y &lt; 10; y++ &#123; // 满足某个条件时, 退出循环 if y == 2 &#123; // 设置退出标记 breakAgain = true // 退出本次循环 break &#125; &#125; // 根据标记, 还需要退出一次循环 if breakAgain &#123; break &#125; &#125; fmt.Println(\"done\")&#125; 使用 goto方式优化 1234567891011121314151617package mainimport \"fmt\"func main() &#123; for x := 0; x &lt; 10; x++ &#123; for y := 0; y &lt; 10; y++ &#123; if y == 2 &#123; // 跳转到标签 goto breakHere &#125; &#125; &#125; // 手动返回, 避免执行进入标签 return // 标签breakHere: fmt.Println(\"done\")&#125; 第13行 ：标签只能被 goto 使用，但不影响代码执行流程，此处如果不手动返回，在不满足条件时，也会执行第 16 行代码。 使用 goto 语句后，无须额外的变量就可以快速退出所有的循环。 统一错误处理多处错误处理存在代码重复时是非常棘手的，例如： 12345678910111213err := firstCheckError()if err != nil &#123; fmt.Println(err) exitProcess() return&#125;err = secondCheckError()if err != nil &#123; fmt.Println(err) exitProcess() return&#125;fmt.Println(\"done\") 使用 goto 语句实现上面同样的逻辑： 12345678910111213err := firstCheckError() if err != nil &#123; goto onExit &#125; err = secondCheckError() if err != nil &#123; goto onExit &#125; fmt.Println(\"done\") returnonExit: fmt.Println(err) exitProcess() break (跳出循环)break 语句可以结束for、switch和select代码块。break 语句还可以在语句后面添加标签，表示退出摸个标签对应的代码块，标签要求必须定义在对应的for、switch和select的代码块上 下面看下跳出指定循环 123456789101112131415OuterLoop: for i := 0; i &lt; 2; i++ &#123; for j := 0; j &lt; 5; j++ &#123; fmt.Println(i , j) switch j &#123; case 2: fmt.Println(i, j) break OuterLoop case 3: fmt.Println(i, j) break OuterLoop &#125; &#125; &#125;&#125; 代码输出 : 10 2 代码说明如下： 第 1 行，外层循环的标签。 第 2 行和第 3 行，双层循环。 第 5 行，使用 switch 进行数值分支判断。 第 8 和第 11 行，退出 OuterLoop 对应的循环之外，也就是跳转到第 1 行。 continue(中断本次循环，继续下一次循环)continue 语句可以结束当前循环，开始下一次的循环迭代过程，仅限在 for 循环内使用 在 continue 语句后添加标签时，表示开始标签对应的循环 12345678910OuterLoops: for i := 0; i &lt; 2; i++ &#123; for j := 0; j &lt; 5; j++ &#123; switch j &#123; case 2: fmt.Println(i, j) continue OuterLoops &#125; &#125; &#125; 代码输出 : 120 21 2 第 7 行将结束当前循环，开启下一次的外层循环，而不是第 3 行的循环。","categories":[{"name":"Go","slug":"Go","permalink":"https://blog.zhimma.com/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://blog.zhimma.com/tags/Go/"}]},{"title":"Go语言学习笔记2-容器","slug":"Go语言学习笔记2-容器","date":"2019-03-06T06:13:39.000Z","updated":"2019-03-07T06:53:43.851Z","comments":true,"path":"2019/03/06/Go语言学习笔记2-容器/","link":"","permalink":"https://blog.zhimma.com/2019/03/06/Go语言学习笔记2-容器/","excerpt":"","text":"[TOC] 变量在一定程度上能满足函数及代码要求。如果编写一些复杂算法、结构和逻辑，就需要更复杂的类型来实现。这类复杂类型一般情况下具有各种形式的存储和处理数据的功能，将它们称为“容器（container）”。 数组数组(Array) 是一段固定长度的连续内存区域在Go语言中，数组从声明时就确定，使用时可以修改数组成员，但是数组大小不可变化。Go 的数组和切片都是从C语言延续过来的设计。 声明数组1var 数组变量名 [元素数量]T 其中： 数组变量名：数组声明及使用时的变量名 元素数量：数组的元素数量。可以是一个表达式，但最终通过编译期计算的结果必须是整形数值 T 可以是任意基本类型，包括 T 为数组本身。但类型为数组本身时，可以实现多维数组 12345var team [3] string team[0] = \"JD\" team[1] = \"TaoBao\" team[2] = \"Wechat\"fmt.Println(team) // [JD TaoBao Wechat] 数组初始化数组可以在声明时使用初始化列表进行元素设置，参考下面的代码： 1var team = [3]string&#123;\"JD\", \"TaoBao\", \"Wechat\"&#125; 这种方式编写时，需要保证大括号后面的元素数量与数组的大小一致 但一般情况下，这个过程可以交给编译器，让编译器在编译时，根据元素个数确定数组大小 1var team = [...]string&#123;\"JD\" , \"TaoBao\" , \"Wechat\"&#125; ...表示让编译器确定数组大小。上面例子中，编译器会自动为这个数组设置元素个数为 3 遍历数组遍历数组也和遍历切片类似 123456789var team = [...]string&#123;\"JD\", \"TaoBao\", \"Wechat\"&#125; for k, v := range team &#123; fmt.Println(k, v) /** 0 JD 1 TaoBao 2 Wechat */&#125; 切片切片是一个拥有相同类型元素的可变长度的序列，Go语言切片的内部包含地址、大小、容量，切片一般用于快速地操作一块数据集合。如果将数据集合比作切糕的话，切片就是你要的“那一块”。切的过程包含从哪里开始(这个就是切片的地址)及切多大(这个就是切片的大小)。容量可以理解为装切片的口袋大小 生成切片(从数组或切片生成新的切片)切片默认指向一段连续内存区域，可以是数组，也可以是切片本身 从连续内存区域生成切片是常见的操作 1slice[开始位置:结束位置] slice表示目标切片对象 开始位置对应目标切片对象的索引 结束位置对应目标切片的结束索引 从数组生成切片：12var arr = [3]int&#123;1,2,3&#125;fmt.Println(arr , arr[1:2]) // [1 2 3] [2] [2]就是arr[1:2]切片操作的结果 从数组或切片生成新的切片拥有如下特性： 取出的元素数量为：结束位置-开始位置。 取出元素不包含结束位置对应的索引，切片最后一个元素使用 slice[len(slice)] 获取。 当缺省开始位置时，表示从连续区域开头到结束位置。 当缺省结束位置时，表示从开始位置到整个连续区域末尾。 两者同时缺省时，与数组本身等效。 两者同时为0时，等效于空切片，一般用于切片复位。 根据索引位置取切片 slice 元素值时，取值范围是（0～len(slice)-1），超界会报运行时错误 生成切片时，结束位置可以填写 len(slice) 但不会报错。 具体，下面使用一些实例演示： 指定范围生成切片123456789101112131415161718package mainimport \"fmt\"func main() &#123; // 声明一个array build := [30]int&#123;&#125; // 赋值 for i:=0;i&lt;30;i++ &#123; build[i] = i +1 &#125; // 区间取值 fmt.Println(build[10:15]) // [11 12 13 14 15] // 中间部分到结尾所有元素 fmt.Println(build[20:]) // [21 22 23 24 25 26 27 28 29 30] // 开始到中间所有元素 fmt.Println(build[:8]) // [1 2 3 4 5 6 7 8]&#125; 切片在指针的基础上增加了大小，约束了切片对应的内存区域，切片使用中无法对切片内部的地址和大小进行手动调整，因此切片比指针更安全、强大。 表示原有的切片生成切片的格式中，当开始和结束都范围都被忽略，则生成的切片将表示和原切片一致的切片，并且生成的切片与原切片在数据内容上是一致的 12345678package mainimport \"fmt\"func main() &#123; originSlice := []int&#123;1, 2, 3&#125; fmt.Println(originSlice[:]) // [1 2 3]&#125; 重置切片，清空所有元素12345678package mainimport \"fmt\"func main() &#123; originSlice := []int&#123;1, 2, 3&#125; fmt.Println(originSlice[0:0]) // []&#125; 声明新的切片除了可以从原有的数组或者切片中生成切片，你也可以声明一个新的切片每一种类型都可以拥有其切片类型，表示多个类型元素的连续集合。因此切片类型也可以被声明。切片类型声明格式如下： 1var name []T Name 表示切片的变量名 T 表示切片对应的元素类型 12345678910111213141516171819202122package mainimport \"fmt\"func main() &#123; // 声明字符串切片 var strSlice []string // 声明整形切片 var intSlice []int // 声明一个空切片 var emptySlice = []int&#123;&#125; fmt.Println(strSlice, intSlice, emptySlice) // [] [] [] // 输出3个切片大小 fmt.Println(len(strSlice), len(intSlice), len(emptySlice)) // 0 0 0 // 切片判定空的结果 fmt.Println(strSlice == nil) fmt.Println(intSlice == nil) fmt.Println(emptySlice == nil) // true true false&#125; 切片是动态结构，只能与nil判定相等，不能互相判等时。 声明新的切片后，可以使用append()函数来添加元素。 使用make()函数构造切片如果需要动态的创建一个切片，可以使用make()内建函数 1make([]T,size,cap) T : 切片的元素类型 size : 就是为这个类型分配多少个元素 cap : 预分配元素数量，这个值设定不影响size,只是能提前分配空间，降低多次分配空间造成的性能问题。 示例如下： 123456789101112package mainimport \"fmt\"func main() &#123; a := make([]int, 2) b := make([]int, 2, 10) fmt.Println(a, b) // [0 0] [0 0] fmt.Println(len(a), len(b)) // 2 2 fmt.Println(cap(a), cap(b)) // 2 10&#125; a 和 b 均是预分配 2 个元素的切片，只是 b 的内部存储空间已经分配了 10 个，但实际使用了 2 个元素。容量不会影响当前的元素个数，因此 a 和 b 取 len 都是 2。 使用 make() 函数生成的切片一定发生了内存分配操作。但给定开始与结束位置（包括切片复位）的切片只是将新的切片结构指向已经分配好的内存区域，设定开始与结束位置，不会发生内存分配操作。 切片不一定必须经过 make() 函数才能使用。生成切片、声明后使用 append() 函数均可以正常使用切片 使用append()为切片添加元素 Go语言的内建函数 append() 可以为切片动态添加元素 每个切片会指向一片内存空间，这片空间能容纳一定数量的元素 当空间不能容纳足够多的元素时，切片就会进行“扩容”。“扩容”操作往往发生在 append() 函数调用时 切片在扩容时，容量的扩展规律按容量的 2 倍数扩充，例如 1、2、4、8、16…… 123456789101112131415161718192021package mainimport \"fmt\"func main() &#123; var numbers []int for i := 0; i &lt; 10; i++ &#123; numbers = append(numbers, i) fmt.Printf(\"len: %d,cap: %d,pointer: %p\\n\", len(numbers), cap(numbers), numbers) // len : 1,cap:1,pointer:0xc000014080 // len : 2,cap:2,pointer:0xc0000140b0 // len : 3,cap:4,pointer:0xc0000160c0 // len : 4,cap:4,pointer:0xc0000160c0 // len : 5,cap:8,pointer:0xc00001a080 // len : 6,cap:8,pointer:0xc00001a080 // len : 7,cap:8,pointer:0xc00001a080 // len : 8,cap:8,pointer:0xc00001a080 // len : 9,cap:16,pointer:0xc00008a000 // len : 10,cap:16,pointer:0xc00008a000 &#125;&#125; append() 函数除了添加一个元素外，也可以一次性添加很多元素。 12345678910111213141516package mainimport \"fmt\"func main() &#123; var company []string // 添加一个元素 company = append(company , \"JD\") // 添加多个元素 company = append(company , \"taobao\" , \"t.tt\") // 添加切片 newCompany := []string&#123;\"mi\" , \"坚果\"&#125; company = append(company , newCompany...) fmt.Println(company) // [JD taobao t.tt mi 坚果]&#125; 第 13 行，在newCompany后面加上了...，表示将 newCompany 整个添加到 car 的后面。 切片复制使用内建的 copy() 函数，可以迅速地将一个切片的数据复制到另外一个切片空间中，copy() 函数的使用格式如下： 1copy(originSlice,srcSlice []T)int originSlice 为数据来源切片 srcSlice为复制的目标。目标切片必须分配过空间且足够承载复制的元素个数，来源和目标的类型一致，copy 的返回值表示实际发生复制的元素个数。 删除切片元素Go语言并没有对删除切片元素提供专用的语法或者接口，需要使用切片本身的特性来删除元素 12345678seq := []string&#123;\"a\", \"b\", \"c\", \"d\", \"e\"&#125;// 指定删除位置 index := 2// 查看删除位置之前的元素和之后的元素fmt.Println(seq[:index], seq[index+1:])// [a b] [d e]// 将删除点前后的元素连接起来 seq = append(seq[:index], seq[index+1:]...)fmt.Println(seq) // [a b d e] 代码的删除过程可以使用下图来描述。 Go 语言中切片删除元素的本质是：以被删除元素为分界点，将前后两个部分的内存重新连接起来。 连续容器的元素删除无论是在任何语言中，都要将删除点前后的元素移动到新的位置。随着元素的增加，这个过程将会变得极为耗时。因此，当业务需要大量、频繁地从一个切片中删除元素时，如果对性能要求较高，就需要反思是否需要更换其他的容器（如双链表等能快速从删除点删除元素）。 map(Go语言映射)在业务和算法中需要使用任意类型的关联关系时，就需要使用到映射，如学号和学生的对应、名字与档案的对应等。 Go语言提供的映射关系容器为 map，map使用散列表（hash）实现。 添加数据到mapGo语言中 map 的定义是这样的： 1map[keyType]vluesType keyTyp 表示键类型 valueType 表示键对应值类型 一个map 里，符合keyType和valueType 的映射总是成对出现 123456789101112package mainimport \"fmt\"func main() &#123; scene := make(map[string]int) // 这里的map是内部实现的类型 scene[\"route\"] = 666 fmt.Println(scene[\"route\"]) // 666 v := scene[\"route2\"] fmt.Println(v) // 0&#125; 尝试查找一个不存在的键，那么返回的将是 ValueType 的默认值 某些情况下，需要明确知道查询中某个键是否在 map 中存在，可以使用一种特殊的写法来实现 1v, ok := scene[\"route\"] 在默认获取键值的基础上，多取了一个变量 ok，可以判断键 route 是否存在于 map 中。 map 还可以在声明时填充内容，例如： 1234567m := map[string]string&#123; \"W\" : \"forward\", \"A\": \"left\", \"D\": \"right\", \"S\": \"backward\", &#125;fmt.Println(m) // map[W:forward A:left D:right S:backward] 遍历map遍历key 和 valuemap 的遍历过程使用 for range循环完成，代码如下： 12345678910111213family := map[string]string&#123; \"dad\": \"zhimma dad\", \"mom\": \"zhimma mom\", \"daughter\": \"zhimma\", &#125;for key,value := range family&#123; fmt.Println(\"hello\" , key ,value) /** hello dad zhimma dad hello mom zhimma mom hello daughter zhimma */&#125; 只遍历value遍历时，可以同时获得键和值。如只遍历值，可以使用下面的形式： 1for _,value := range family&#123; 将不需要的键改为匿名变量形式。 只遍历key只遍历键时，使用下面的形式： 1for key := range family&#123; 无须将值改为匿名变量形式，忽略值即可 map 元素删除和清空元素删除使用delete()内建函数从map中删除一组键值对 ，delete()函数的格式如下 1delete(map , 键) map 为要删除的map实例 键为要删除的 map 键值对中的键 12345678910111213numMap := make(map[string]int) numMap[&quot;aroute&quot;] = 66 numMap[&quot;brazil&quot;] = 4 numMap[&quot;china&quot;] = 960 delete(numMap , &quot;brazil&quot;) for k, v := range numMap &#123; fmt.Println(k , v) /** aroute 66 china 960 */ &#125; 清空map有意思的是,Go语言中并没有为 map 提供任何清空所有元素的函数、方法。清空 map 的唯一办法就是重新 make 一个新的 map。不用担心垃圾回收的效率，Go 语言中的并行垃圾回收效率比写一个清空函数高效多了。 sync.Map(在并发环境中使用的map)Go 语言中的 map 在并发情况下，只读是线程安全的，同时读写线程不安全。 12345678910111213141516171819// 创建一个int到int的映射 n := make(map[int]int) // 开启一段并发代码 go func() &#123; for &#123; // 不停地对map进行写入 n[1] = 1 &#125; &#125;() // 开启一段并发代码 go func() &#123; for &#123; _ = n[1] &#125; &#125;() for &#123; &#125; 运行代码会报错，输出如下：fatal error: concurrent map read and map write 运行时输出提示：并发的 map 读写。也就是说使用了两个并发函数不断地对 map 进行读和写而发生了竞态问题。map 内部会对这种并发操作进行检查并提前发现 需要并发读写时，一般的做法是加锁，但这样性能并不高。Go 语言在 1.9 版本中提供了一种效率较高的并发安全的 sync.Map。sync.Map 和 map 不同，不是以语言原生形态提供，而是在 sync 包下的特殊结构。 sync.Map有以下特性： 无须初始化，直接声明即可。 sync.Map 不能使用 map 的方式进行取值和设置等操作，而是使用 sync.Map 的方法进行调用。Store 表示存储，Load 表示获取，Delete 表示删除。 使用 Range 配合一个回调函数进行遍历操作，通过回调函数返回内部遍历出来的值。Range 参数中的回调函数的返回值功能是：需要继续迭代遍历时，返回 true；终止迭代遍历时，返回 false。 并发安全的 sync.Map 演示代码如下： 12345678910111213141516var scenes sync.Map // 将键值对保存到sync.Map scenes.Store(\"greece\", 97) scenes.Store(\"london\", 100) scenes.Store(\"egypt\", 200) // 从sync.Map中根据键取值 fmt.Println(scenes.Load(\"london\")) // 根据键删除对应的键值对 scenes.Delete(\"london\") // 遍历所有sync.Map中的键值对 scenes.Range(func(k, v interface&#123;&#125;) bool &#123; fmt.Println(\"iterate:\", k, v) // iterate: greece 97 //i terate: egypt 200 return true &#125;) sync.Map 没有提供获取 map 数量的方法，替代方法是获取时遍历自行计算数量。sync.Map 为了保证并发安全有一些性能损失，因此在非并发情况下，使用 map 相比使用 sync.Map 会有更好的性能。 list(列表)列表是一种非连续存储的容器，由多个节点组成，节点通过一些变量记录彼此之前的关系。列表有多种实现方法，如单链表、双链表等。 列表的原理可以这样理解：假设 A、B、C 三个人都有电话号码，如果 A 把号码告诉给 B，B 把号码告诉给 C，这个过程就建立了一个单链表结构，如下图所示： 如果在这个基础上，再从 C 开始将自己的号码给自己知道号码的人，这样就形成了双链表结构，如下图所示。 那么如果需要获得所有人的号码，只需要从 A 或者 C 开始，要求他们将自己的号码发出来，然后再通知下一个人如此循环。这个过程就是列表遍历。如果 B 换号码了，他需要通知 A 和 C，将自己的号码移除。这个过程就是列表元素的删除操作，如下图所示 在Go语言中，将列表使用 container/list 包来实现，内部的实现原理是双链表。列表能够高效地进行任意位置的元素插入和删除操作。 初始化列表list 的初始化有两种方法：New 和声明。两种方法的初始化效果都是一致的。 通过container/list 包的 New 方法初始化 list1变量名 := list.New() 通过声明初始化list1var 变量名 list.List 列表与切片和 map 不同的是，列表并没有具体元素类型的限制。因此，列表的元素可以是任意类型。这既带来便利，也会引来一些问题。给一个列表放入了非期望类型的值，在取出值后，将 interface{} 转换为期望类型时将会发生宕机。 在列表中插入元素双链表支持从队列前方或后方插入元素，分别对应的方法是 PushFront和 PushBack。 这两个方法都会返回一个 *list.Element 结构。如果在以后的使用中需要删除插入的元素，则只能通过 *list.Element 配合Remove()方法进行删除，这种方法可以让删除更加效率化，也是双链表特性之一 下面代码展示给list添加元素： 1234l := list.New()l.PushBack(\"age\")l.PushFront(\"18+\") 列表插入元素的方法如下表所示。 方 法 功 能 InsertAfter(v interface {}, mark Element) Element 在 mark 点之后插入元素，mark 点由其他插入函数提供 InsertBefore(v interface {}, mark Element) Element 在 mark 点之前插入元素，mark 点由其他插入函数提供 PushBackList(other *List) 添加 other 列表元素到尾部 PushFrontList(other *List) 添加 other 列表元素到头部 从列表中删除元素列表的插入函数的返回值会提供一个 *list.Element 结构，这个结构记录着列表元素的值及和其他节点之间的关系等信息。从列表中删除元素时，需要用到这个结构进行快速删除。 123456789101112131415l := list.New() // 尾部插入age l.PushBack(\"age\") // 头部插入18+ l.PushFront(\"18+\") // 尾部添加后保存元素句柄 element := l.PushBack(\"fist\") // 在fist之后添加high l.InsertAfter(\"high\", element) // 在fist之前添加noon l.InsertBefore(\"noon\", element) // 使用 l.Remove(element) 下表中展示了每次操作后列表的实际元素情况。 操作内容 列表元素 l.PushBack(“age”) age l.PushFront(“18+”) 18+, age element := l.PushBack(“fist”) 18+, age, fist l.InsertAfter(“high”, element) 18+, age, fist, high l.InsertBefore(“noon”, element) 18+, age, noon, fist, high l.Remove(element) 18+, age, noon, high 遍历列表遍历双链表需要配合 Front()函数获取头元素，遍历时只要元素不为空就可以继续进行。每一次遍历调用元素的 Next，如代码中第 6 行所示 12345678910l := list.New()// 尾部添加l.PushBack(\"canon\")// 头部添加l.PushFront(67)for i := l.Front(); i != nil; i = i.Next() &#123; fmt.Println(i.Value) // 67 // canon&#125; 使用 for 语句进行遍历，其中 i:=l.Front() 表示初始赋值，只会在一开始执行一次；每次循环会进行一次 i!=nil 语句判断，如果返回 false，表示退出循环，反之则会执行 i=i.Next()。","categories":[{"name":"Go","slug":"Go","permalink":"https://blog.zhimma.com/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://blog.zhimma.com/tags/Go/"}]},{"title":"Go语言学习笔记1-基本语法","slug":"Go语言学习笔记1-基本语法","date":"2019-03-05T08:48:28.000Z","updated":"2019-03-07T07:55:34.994Z","comments":true,"path":"2019/03/05/Go语言学习笔记1-基本语法/","link":"","permalink":"https://blog.zhimma.com/2019/03/05/Go语言学习笔记1-基本语法/","excerpt":"","text":"[TOC] 变量声明(使用var关键字) 变量（Variable）的功能是存储用户的数据,不同的逻辑有不同的对象类型，也就有不同的变量类型 Go语言使用var关键字进行变量的声明 1234567var a intvar b stringvar c []float32var d func() boolvar e struct&#123; x int&#125; 第一行，声明一个整形类型的变量，用来保存整数数值 第二行，声明一个字符串类型的变量 第三行，声明一个32位浮点切片类型的变量，浮点切片表示由多个浮点类型组成的数据结构 第四行，声明一个返回值为bool类型的函数变量，这种形式一般用于回调函数，即将函数以变量的形式保存下来，在需要的时候重新调用这个函数 声明一个结构体类型的变量，这个结构体拥有一个整形的x字段 标准格式声明1var 变量名 变量类型 变量的声明是以var关键字开头，要声明的变量名放中间，将其类型放在后面，行尾无需分号 批量格式声明123456789var ( a int b string c []float32 d func() bool e struct &#123; x int &#125;) 使用var和括号，可以将一组变量定义放在一起 变量初始化Go语言在声明变量时，自动对变量对应的内存区域进行初始化操作。每个变量会初始化其类型的默认值，例如： 整型和浮点型变量的默认值为 0 字符串变量的默认值为空字符串 布尔型变量默认为 bool 切片、函数、指针变量的默认为 nil 在声明变量的时候也可以进行赋初始值 变量初始化标准格式12var 变量名 类型 = 表达式var name string = \"zhimma\" 编译器推导类型在标准格式的基础上，可以省略部分变量类型，编译器会尝试根据等号右边的表达式推导变量的类型 1var name = \"zhimma\" 等号右边的部分在编译原理里被称做右值（rvalue） 短变量声明并初始化var 的变量声明还有一种精简写法 1name := \"zhimma\" 左值变量必须是没有定义过的变量 在多个短变量声明和赋值中，至少有一个新声明的变量出现在左值中，即便其他变量名可能是重复声明的，编译器也不会报错，代码如下： 12conn, err := net.Dial(\"tcp\", \"127.0.0.1:8080\")conn2, err := net.Dial(\"tcp\", \"127.0.0.1:8080\") 多个变量同时赋值1234var a int = 100var b int = 200b, a = a, b 多重赋值时，变量的左值和右值按从左到右的顺序赋值多重赋值在 Go 语言的错误处理和函数返回值中会大量地使用。 匿名变量在使用多重赋值时，如果不需要在左值中接收变量，可以使用匿名变量（anonymous variable） 匿名变量的表现是一个下划线_,使用匿名变量时，只需要在变量声明的地方使用下画线替换即可 123456func GetData() (int, int) &#123; return 100, 200&#125;a, _ := GetData()_, b := GetData()fmt.Println(a, b)// 100 200 匿名变量不占用命名空间，不会分配内存。匿名变量与匿名变量之间也不会因为多次声明而无法使用。 匿名变量： 可以理解为一种占位符。 本身这种变量不会进行空间分配，也不会占用一个变量的名字。 在for range 可以对 key 使用匿名变量，也可以对 value 使用匿名变量。 Go语言类型Go语言 中有丰富的数据类型，除了基本的整型、浮点型、布尔型、字符串外，还有切片、结构体、函数、map、通道（channel）等。Go 语言的基本类型和其他语言大同小异，切片类型有着指针的便利性，但比指针更为安全，很多高级语言都配有切片进行安全和高效率的内存操作。 整数类型整型分为以下两个大类： 按长度分为：int8、int16、int32、int64 还有对应的无符号整型：uint8、uint16、uint32、uint64 其中，uint8 就是我们熟知的 byte 型，int16 对应C语言中的 short 型，int64 对应C语言中的 long 型。 浮点类型（小数类型）Go语言 支持两种浮点型数：float32 和 float64。这两种浮点型数据格式遵循 IEEE 754 标准： float32 的浮点数的最大范围约为 3.4e38，可以使用常量定义：math.MaxFloat32。 float64 的浮点数的最大范围约为 1.8e308，可以使用一个常量定义：math.MaxFloat64。 打印浮点数时，可以使用 fmt 包配合动词%f，代码如下： 1234567891011package mainimport ( \"fmt\" \"math\")func main() &#123; fmt.Printf(\"%f\\n\", math.Pi) //按默认宽度和精度输出整型 fmt.Printf(\"%.2f\\n\", math.Pi) //按默认宽度，2 位精度输出（小数点后的位数）。&#125;// 3.141593// 3.14 bool类型布尔型数据只有 true（真）和 false（假）两个值，布尔型无法参与数值运算，也无法与其他类型进行转换 字符串字符串在Go语言中以原生数据类型出现，使用字符串就像使用其他原生数据类型（int、bool、float32、float64 等）一样。 字符串转义符Go 语言的字符串常见转义符包含回车、换行、单双引号、制表符等，如下表所示。 转移符 含 义 \\r 回车符（返回行首） \\n 换行符（直接跳到下一行的同列位置） \\t 制表符 \\’ 单引号 \\” 双引号 \\ 反斜杠 定义多行字符串在源码中，将字符串的值以双引号书写的方式是字符串的常见表达方式，被称为字符串字面量（string literal）这种双引号字面量不能跨行。如果需要在源码中嵌入一个多行字符串时，就必须使用`字符，代码如下： 1234const str = `第一行第二行第三行` 在`间的所有代码均不会被编译器识别，而只是作为字符串的一部分 字符类型(byte和rune)字符串中的每一个元素叫做“字符”，在遍历或者单个获取字符串元素时可以获得字符 Go语言的字符有以下两种： uint8类型，也叫byte型，代表了ASCLL码中的一个字符 rune类型，代表一个UTF-8字符。当需要处理中文、日文或者其他复合字符时，则需要用到 rune 类型。rune 类型实际是一个 int32。 使用fmt.Printf中的%T动词可以输出、变量的实际类型，使用这个方法可以查看 byte 和 rune 的本来类型 12345var a byte = 'a'fmt.Printf(\"%d %T\\n\", a, a) // 97 uint8var b rune = '你'fmt.Printf(\"%d %T\\n\", b, b) // 20320 int32 可以发现，byte 类型的 a 变量，实际类型是 uint8，其值为 ‘a’，对应的 ASCII 编码为 97rune 类型的 b 变量的实际类型是 int32，对应的 Unicode 码就是 20320Go 使用了特殊的 rune 类型来处理 Unicode，让基于 Unicode 的文本处理更为方便，也可以使用 byte 型进行默认字符串处理，性能和扩展性都有照顾。 Go数据类型转换Go语言使用类型前置加括号的方式进行数据类型转换，一般格式如下： 1T(表达式) T代表要转换的类型。表达式包括变量、复杂算子和函数返回值等 类型转换时，需要考虑两种类型的关系和范围，是否会发生数值截断等 常量相对于变量，常量是恒定不变的值，例如圆周率。可以在编译时，对常量表达式进行计算求值，并在运行期使用该计算结果，计算结果无法被修改。常量表示起来非常简单，如下面的代码： 1const pi = 4.14159 常量的声明和变量声明非常类似，只是把 var 换成了 const。 多个变量可以一起声明，类似的，常量也是可以多个一起声明的，如下面的代码： 1234const ( pi = 3.141592 e = 2.718281) 常量因为在编译期确定，所以可以用于数组声明，如下面的代码： 12const size = 4var arr [size]int 模拟枚举(const和iota模拟枚举)Go语言现阶段没有枚举，可以使用 const 常量配合 iota 模拟枚举，请看下面的代码： 12345678910111213type Weapon intconst ( Arrow Weapon = iota // 开始生成枚举值, 默认为0 Shuriken SniperRifle Rifle Blower)// 输出所有枚举值fmt.Println(Arrow, Shuriken, SniperRifle, Rifle, Blower) // 1 2 3 4// 使用枚举类型并赋初值var weapon Weapon = Blowerfmt.Println(weapon) // 4 枚举类型其实本质是一个 int 一样。当然，某些情况下，如果需要 int32 和 int64 的枚举，也是可以的。 Go语言type关键字(类型别名)类型定义1type byte uint8 类型别名类型别名的写法为： 1type TypeAlias = Type 类型别名规定：TypeAlias 只是 Type 的别名，本质上 TypeAlias 与 Type 是同一个类型。就像一个孩子小时候有小名、乳名，上学后用学名，英语老师又会给他起英文名，但这些名字都指的是他本人。 1234567891011121314151617181920package mainimport &quot;fmt&quot;// 将NewInt定义为int类型type NewInt int// 将int取一个别名叫IntAliastype IntAlias = intfunc main() &#123; var a NewInt fmt.Printf(&quot;a type : %T\\n&quot;, a) // a type : main.NewInt var b IntAlias fmt.Printf(&quot;b type %T\\n&quot;, b) // b type int&#125; 代码说明如下： 第 8 行，将 NewInt 定义为 int 类型，这是常见定义类型的方法，通过 type 关键字的定义，NewInt 会形成一种新的类型。NewInt 本身依然具备int的特性。 第 11 行，将 IntAlias 设置为 int 的一个别名，使用 IntAlias 与 int 等效。 第 15 行，将 a 声明为 NewInt 类型，此时若打印，则 a 的值为 0。 第 16 行，使用%T格式化参数，显示 a 变量本身的类型。 第 18 行，将 b 声明为 IntAlias 类型，此时打印 b 的值为 0。 第 19 行，显示 b 变量的类型。 结果显示a的类型是 main.NewInt，表示 main 包下定义的 NewInt 类型。b 类型是 int。IntAlias 类型只会在代码中存在，编译完成时，不会有 IntAlias 类型。 Go语言指针指针（pointer）概念在Go语言中被拆分为两个核心概念： 类型指针，运行对这个指针类型的数据进行修改，传递数据使用指针，而无需拷贝数据。类型指针不能进行偏移和运算 切片，由指向起始元素的原始指针、元素数量和容量组成 受益于这样的约束和拆分，Go 语言的指针类型变量拥有指针的高效访问，但又不会发生指针偏移，从而避免非法修改关键性数据问题。同时，垃圾回收也比较容易对不会发生偏移的指针进行检索和回收。 切片比原始指针具备更强大的特性，更为安全。切片发生越界时，运行时会报出宕机，并打出堆栈，而原始指针只会崩溃。 要明白指针，需要知道几个概念：指针地址、指针类型和指针取值，下面将展开细说。 指针地址和指针类型每个变量在运行时都拥有一个地址，这个地址代表变量在内存中的位置 Go 语言中使用&amp;作符放在变量前面对变量进行“取地址”操作。 1ptr := &amp;v // v的类型为T 其中v代表被取地址的变量，被取地址的v使用ptr变量进行接收，ptr的类型就为*T,称做 T 的指针类型，*代表指针。 1234567891011package mainimport ( \"fmt\")func main() &#123; var cat int = 1 var str string = \"banana\" fmt.Printf(\"%p %p\", &amp;cat, &amp;str) // 0xc042052088 0xc0420461b0&#125; 代码说明： 第 8 行，声明整型 cat 变量。 第 9 行，声明字符串 str 变量。 第 10 行，使用 fmt.Printf 的动词%p输出 cat 和 str 变量取地址后的指针值，指针值带有0x的十六进制前缀。 输出值在每次运行是不同的，代表 cat 和 str 两个变量在运行时的地址。 提示：变量、指针和地址三者的关系是：每个变量都拥有地址，指针的值就是地址 指针取值在对普通变量使用&amp;操作符获取地址获得这个变量的指针后，可以对指针使用*操作，也就是指针取值 12345678910111213141516171819202122package mainimport \"fmt\"func main() &#123; // 声明一个字符串类型变量 var house = \"陕西西安\" // 对字符串取地址，ptr类型为*string ptr := &amp;house // 输出ptr类型 fmt.Printf(\"ptr type: %T\\n\" , ptr) // ptr type: *string // 输出ptr指针地址 fmt.Printf(\"address: %p\\n\" , ptr) // address: 0xc00000e1e0 // 对指针进行取值操作 value := *ptr // 取值后类型 fmt.Printf(\"value type: %T\\n\" , value) // value type: string // 指针去之后就是指向变量的值 fmt.Printf(\"value: %s\\n\" , value) //value: 陕西西安&#125; 取地址操作符&amp;和取值操作符*是一对互补操作符，&amp;取出地址，*根据地址取出地址指向的值。 变量、指针地址、指针变量、取地址、取值的相互关系和特性如下： 对变量进行取地址（&amp;）操作，可以获得这个变量的指针变量。 指针变量的值是指针地址。 对指针变量进行取值（*）操作，可以获得指针变量指向的原变量的值。 使用指针修改值通过指针不仅可以取值，也可以修改值。 前面已经使用多重赋值的方法进行数值交换，使用指针同样可以进行数值交换，代码如下： 123456789101112131415161718192021package mainimport \"fmt\"func main() &#123; // 准备两个变量, 赋值1和2 x, y := 1, 2 // 交换变量值 swap(&amp;x, &amp;y) // 输出变量值 fmt.Println(x, y) // 2 1&#125;func swap(a, b *int) &#123; // 取a指针的值, 赋给临时变量t t := *a // 取b指针的值, 赋给a指针指向的变量 *a = *b // 将a指针的值赋给b指针指向的变量 *b = t&#125; *操作符作为右值时，意义是取指针的值，作为左值时，也就是放在赋值操作符的左边时，表示 a 指向的变量 *操作符的根本意义就是操作指针指向的变量，当操作在右值时，就是取指向变量的值，当操作在左值时，就是将值设置给指向的变量。 创建指针的另一种方法——new() 函数Go 语言还提供了另外一种方法来创建指针变量，格式如下： 1new(T) // T代表类型 一般这样写： 1234str := new(string)*str = \"zhimma\"fmt.Println(*str) // zhimma new() 函数可以创建一个对应类型的指针，创建过程会分配内存。被创建的指针指向的值为默认值。","categories":[{"name":"Go","slug":"Go","permalink":"https://blog.zhimma.com/categories/Go/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://blog.zhimma.com/tags/Go/"}]},{"title":"Nginx单域名配置多Vue工程和多PHP接口","slug":"Nginx单域名配置多Vue工程和多PHP接口","date":"2019-03-01T09:42:33.000Z","updated":"2019-03-01T10:03:18.633Z","comments":true,"path":"2019/03/01/Nginx单域名配置多Vue工程和多PHP接口/","link":"","permalink":"https://blog.zhimma.com/2019/03/01/Nginx单域名配置多Vue工程和多PHP接口/","excerpt":"","text":"先简单说下需求吧： 前后端分离项目，一个域名可以访问所有的客户端，例如，我们的项目中前端有单独的访问域名，后端有单独的接口域名，我们的项目存在3个客户端，即小程序端，boss后台管理端，console客户端，按照最简单的业务来解析域名，则需要6个域名。 由于种种原因吧，只能提供一个域名，所以就要借助Nginx的重定向或者rewrite功能； 大致流程和这里基本保持一致，点击查看Nginx单域名配置多Vue工程和PHP接口 先贴一下代码吧： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114server &#123; listen 80; server_name btg.ma; index index.html index.htm index.php; ##----boss api-----## ################################################################ location ^~ /boss/b &#123; // 目录重定向 alias /Users/zhimma/Data/www/Btg_Base_Crs/boss-api/src/public/; try_files $uri $uri/ @bossBackend; location ~ \\.php$ &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index /index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param SCRIPT_FILENAME $request_filename; include fastcgi_params; &#125; &#125; location @bossBackend &#123; if (!-e $request_filename) &#123; // 兼容路由模式-个人猜想 rewrite ^/boss/b/(.*)$ /boss/b/index.php?s=$1 last; rewrite /boss/b/(.*)$ /boss/b/index.php$is_args$args last; break; &#125; &#125; ##----boss web-----## ################################################################ location ^~ /boss/f &#123; alias /Users/zhimma/Data/www/Btg_Base_Crs/web/boss/; if (!-e $request_filename) &#123; rewrite ^/boss/f/(.*) /boss/f/index.html last; break; &#125; try_files $uri $uri/ @router; &#125; ##----console api-----## ################################################################ location ^~ /console/b &#123; alias /Users/zhimma/Data/www/Btg_Base_Crs/console-api/src/public/; try_files $uri $uri/ @consoleBackend; location ~ \\.php$ &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index /index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param SCRIPT_FILENAME $request_filename; include fastcgi_params; &#125; &#125; location @consoleBackend &#123; if (!-e $request_filename) &#123; rewrite ^/console/b/(.*)$ /console/b/index.php?s=$1 last; rewrite /console/b/(.*)$ /console/b/index.php$is_args$args last; break; &#125; &#125; ##----console web-----## ################################################################ location ^~ /console/f &#123; alias /Users/zhimma/Data/www/Btg_Base_Crs/web/console/; if (!-e $request_filename) &#123; rewrite ^/console/f/(.*) /console/f/index.html last; break; &#125; try_files $uri $uri/ @router; &#125; ##----crs api-----## ################################################################ location ^~ /crs/b &#123; alias /Users/zhimma/Data/www/Btg_Base_Crs/crs-api/src/public/; try_files $uri $uri/ @crsBackend; location ~ \\.php$ &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index /index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param SCRIPT_FILENAME $request_filename; include fastcgi_params; &#125; &#125; location @crsBackend &#123; if (!-e $request_filename) &#123; rewrite ^/crs/b/(.*)$ /crs/b/index.php?s=$1 last; rewrite /crs/b/(.*)$ /crs/b/index.php$is_args$args last; break; &#125; &#125; ##----crs-----## ################################################################ location ^~ /crs/f &#123; alias /Users/zhimma/Data/www/Btg_Base_Crs/web/crs/; if (!-e $request_filename) &#123; rewrite ^/crs/f/(.*) /crs/f/index.html last; break; &#125; try_files $uri $uri/ @router; &#125; location @router &#123; rewrite ~.*$ /index.html last; &#125; #location /favicon.ico &#123; # root /data/wwwroot/mk.vchangyi.com/web_test/backend; #&#125; access_log /var/log/nginx/btg.ma.access.log main; error_log /var/log/nginx/btg.ma.error.log error;&#125; 最终效果就成为这样子： domain/origin/type 域名+管理端+前后端，f代表frontend,b代表backend boss: 前台页面：btg.ma/boss/f 后台接口地址：btg.ma/boss/b console: 前台页面：btg.ma/console/f 后台接口地址：btg.ma/console/b crs: 前台页面：btg.ma/crs/f 后台接口地址：btg.ma/crs/b done;","categories":[{"name":"Nginx","slug":"Nginx","permalink":"https://blog.zhimma.com/categories/Nginx/"},{"name":"Vue","slug":"Vue","permalink":"https://blog.zhimma.com/categories/Vue/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://blog.zhimma.com/tags/Nginx/"},{"name":"Vue","slug":"Vue","permalink":"https://blog.zhimma.com/tags/Vue/"}]},{"title":"Nginx目录路径重定向","slug":"Nginx目录路径重定向","date":"2019-03-01T09:41:48.000Z","updated":"2019-03-01T10:07:55.645Z","comments":true,"path":"2019/03/01/Nginx目录路径重定向/","link":"","permalink":"https://blog.zhimma.com/2019/03/01/Nginx目录路径重定向/","excerpt":"","text":"如果希望域名后边跟随的路径指向本地磁盘的其他目录,而不是默认的web目录时,需要设置nginx目录访问重定向 应用场景:domain.com/image自动跳转到domain.com/folderName/image目录。 Nginx目录路径重定向的几种实现方式:Nginx修改root映射修改root映射实现nginx目录访问重定向是最简单的方式, 推荐采用这一种. 123location /image &#123; root /folderName;&#125; 通过Nginx rewrite内部跳转实现访问重定向123location /image &#123; rewrite ^/image/(.*)$ /folderName/image/$1 last;&#125; Nginx设置别名alias映射实现123location /image &#123; alias /folderName/image; #这里写绝对路径&#125; 通过nginx的permanent 301绝对跳转实现123location /image &#123; rewrite ^/image/(.*)$ http://dashidan.com/folderName/image/$1;&#125; 通过判断uri实现页面跳转123if ( $request_uri ~* ^(/image))&#123; rewrite ^/image/(.*)$ /folderName/image/$1 last;&#125; 以上转自https://dashidan.com/article/webserver/nginx/4.html 具体实例，请参考这里","categories":[{"name":"Nginx","slug":"Nginx","permalink":"https://blog.zhimma.com/categories/Nginx/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://blog.zhimma.com/tags/Nginx/"}]},{"title":"Jenkins-基础方法Basic Steps","slug":"Jenkins-基础方法Basic-Steps","date":"2019-02-20T06:38:25.000Z","updated":"2019-02-21T12:01:37.486Z","comments":true,"path":"2019/02/20/Jenkins-基础方法Basic-Steps/","link":"","permalink":"https://blog.zhimma.com/2019/02/20/Jenkins-基础方法Basic-Steps/","excerpt":"","text":"[TOC] 这篇开始来学习一个最基础的pipeline组件，这个也是一个独立的插件，在安装pipeline的时候默认会自动安装，插件的名称是Pipeline: Basic Steps,你可以去你自己jenkins环境，插件管理下的installed下面找到这个插件 下面具体介绍下该插件包含的各个方法 deleteDir()方法默认递归删除WORKSPACE下的文件和文件夹,这个方法是没有参数，也不需要参数 123456789101112131415pipeline &#123; agent any stages &#123; stage ('input-test') &#123; steps &#123; script&#123; sh(\"ls -al $&#123;env.WORKSPACE&#125;\") deleteDir() // clean up current work directory sh(\"ls -al $&#123;env.WORKSPACE&#125;\") &#125; &#125; &#125; &#125; &#125; dir()方法如果使用了dir语句块，这个方法就是改变当前的工作目录 在dir语句块里执行的其他路径或者相对路径，都是和dir里面设置的文件路径相关，这个和WORKSPACE相对文件路径已经没有关系了。 123456789101112131415pipeline&#123; agent any stages&#123; stage(\"dir\") &#123; steps&#123; println env.WORKSPACE dir(\"$&#123;env.WORKSPACE&#125;/testdata\")&#123; sh \"pwd\" &#125; &#125; &#125; &#125;&#125; echo() 和 error()方法echo就是和groovy中的println没有任何区别,如果看打印的效果。一般来说使用echo就是打印info debug级别的日志输出用 如果遇到错误，就可以使用error(‘error message’) ，如果出现执行到error方法，jenkins job会退出并显示失败效果。 123456789101112pipeline&#123; agent any stages&#123; stage(\"dir\") &#123; steps&#123; echo(\"this is echo info\") error(\"this is error info\") &#125; &#125; &#125;&#125; fileExists()方法这是判断一个文件是否存在，返回值是布尔类型，true就表示文件存在，false表示文件不存在 12345678910111213141516171819pipeline&#123; agent any stages&#123; stage(\"Demo\") &#123; steps&#123; script &#123; json_file = \"$&#123;env.WORKSPACE&#125;/testdata/test_json.json\" if(fileExists(json_file) == true) &#123; echo(\"json file is exists\") &#125;else &#123; error(\"here haven't find json file\") &#125; &#125; &#125; &#125; &#125;&#125; pwd()方法其实这个方法和linux下的shell命令pwd是一样的。由于jenkins支持windows和linux，但是linux是pwd，windows上是dir,所以这个插件就干脆支持一个方法，统称为pwd() 123456789101112131415pipeline&#123; agent any stages&#123; stage(\"Demo\") &#123; steps&#123; script &#123; sh(\"pwd\") println \"===========\" println pwd() &#125; &#125; &#125; &#125;&#125; isUnix()方法1234567891011121314151617pipeline&#123; agent any stages&#123; stage(&quot;Demo&quot;) &#123; steps&#123; script &#123; if(isUnix() == true) &#123; echo(&quot;this jenkins job running on a linux-like system&quot;) &#125;else &#123; error(&quot;the jenkins job running on a windows system&quot;) &#125; &#125; &#125; &#125; &#125;&#125; mail指令Jenkins服务器上配置smtp服务subject 必填，邮件标题主题，Type: String body 必填，邮件正文，Type: String 1234567891011121314151617pipeline&#123; agent any stages&#123; stage(\"Demo\") &#123; steps&#123; script &#123; script &#123; mail to: 'admin@163.com', subject: \"Running Pipeline: $&#123;currentBuild.fullDisplayName&#125;\", body: \"Something is wrong with $&#123;env.BUILD_URL&#125;\" &#125; &#125; &#125; &#125; &#125;&#125;","categories":[{"name":"容器化服务","slug":"容器化服务","permalink":"https://blog.zhimma.com/categories/容器化服务/"},{"name":"Jenkins","slug":"Jenkins","permalink":"https://blog.zhimma.com/categories/Jenkins/"}],"tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"https://blog.zhimma.com/tags/Jenkins/"}]},{"title":"Jenkins-Pipeline语法入门","slug":"Jenkins-Pipeline语法入门","date":"2019-02-18T14:28:14.000Z","updated":"2019-02-19T10:21:32.770Z","comments":true,"path":"2019/02/18/Jenkins-Pipeline语法入门/","link":"","permalink":"https://blog.zhimma.com/2019/02/18/Jenkins-Pipeline语法入门/","excerpt":"","text":"[TOC] 流水线最基础的部分是 “step”。基本上, step告诉 Jenkins 要做什么，以及作为声明式(Declarative)和脚本化(Scripted)流水线语法的基本构建块。 Declarative Pipeline-声明管道有效的Declarative Pipeline必须包含在一个pipeline块内，例如 123pipeline &#123; /* insert Declarative Pipeline here */&#125; Declarative Pipeline中有效的基本语句和表达式遵循与Groovy语法相同的规则 ，但有以下例外： Pipeline的顶层必须是块(block)，其实就是pipeline { } 没有分号作为语句分隔符。每个声明必须在自己的一行 块只能包含章节， 指令，步骤Sections, Directives, Steps或赋值语句 属性引用语句被视为无参数方法调用。所以例如，input被视为input（） 第一点： 就是声明指定的代码块 第二点：分号写了也是多余的。Groovy代码还可以写分号，Jenkins Pipeline代码就不需要，每行只写一个声明语句块或者调用方法语句 第三点：只能包含Sections, Directives, Steps或者赋值语句 第四点：没懂，懂了再回来补充 Sections-章节/节段Declarative Pipeline 代码中的Sections指的是必须包含一个或者多个指令或者步骤的代码区域块。Sections不是一个关键字或者指令，只是一个逻辑概念。 agent指令-代理agent部分指定整个Pipeline或特 定阶段将在Jenkins环境中执行的位置，具体取决于该agent 部分的放置位置。该部分必须在pipeline块内的顶层定义 ，但阶段级使用是可选的 简单来说，agent部分主要作用就是告诉Jenkins，选择那台节点机器去执行Pipeline代码；这个指令是必须要有的，也就在你顶层pipeline {…}的下一层，必须要有一个agent{…} agent这个指令对应的多个可选参数。 这里注意一点，在具体某一个stage {…}里面也可以使用agent指令。这种用法不多，一般我们在顶层使用agent，这样，接下来的全部stage都在一个agent机器下执行代码。 为了支持Pipeline作者可能拥有的各种用例，该agent部分支持几种不同类型的参数。这些参数可以应用于pipeline块的顶层，也可以应用在每个stage指令内。 参数1：any作用：在任何可用的代理上执行Pipeline或stage 123pipeline &#123; agent any&#125; 上面这种是最简单的，如果你Jenkins平台环境只有一个master，那么这种写法就最省事情 参数2：none作用：当在pipeline块的顶层应用时，将不会为整个Pipeline运行分配全局代理，并且每个stage部分将需要包含其自己的agent部分,就像上面说的在具体某一个stage {…}里面也可以使用agent指令 12345678910pipeline &#123; agent none stages &#123; stage('Build')&#123; agent &#123; label '具体的节点名称' &#125; &#125; &#125;&#125; 参数3：label作用：使用提供的标签在Jenkins环境中可用的代理机器上执行Pipeline或stage内执行 1234pipeline &#123; agent &#123; label '具体一个节点label名称' &#125; 参数4：node作用：和上面label功能类似，但是node运行其他选项，例如customWorkspace 12345678pipeline &#123; agent &#123; node &#123; label 'xxx-agent-机器' customWorkspace \"$&#123;env.JOB_NAME&#125;/$&#123;env.BUILD_NUMBER&#125;\" &#125; &#125;&#125; post指令post部分定义将在Pipeline运行或阶段结束时运行的操作 post部分定义一个或多个steps,这些阶段根Pipeline或stage的完成情况而运行,post 支持以下 post-condition 块中的其中之一: always, changed, failure, success, unstable, 和 aborted 简单来说，post可以放在顶层，也就是和agent{…}同级，也可以放在stage里面。一般放顶层的比较多。而且pipeline代码中post代码块不是必须的，使用post的场景基本上执行完一个构建，进行发送消息通知，例如构建失败会发邮件通知 简单示例： 12345678910pipeline &#123; agent any stages &#123; stage ('Test') &#123; &#125; &#125; Post &#123; //写相关post部分代码 &#125;&#125; post条件的基本用法在post代码块区域，支持多种条件指令，这些指令有always，changed，failure，success，unstable，和aborted。下面分别来介绍这些条件的基本用法。 条件1：always作用：无论Pipeline运行的完成状态如何都会执行这段代码 1234567891011121314151617181920pipeline &#123; agent &#123; node &#123; label 'xxx-agent-机器' customWorkspace \"$&#123;env.JOB_NAME&#125;/$&#123;env.BUILD_NUMBER&#125;\" &#125; &#125; stages &#123; stage ('Build') &#123; sh \"pwd\" //这个是Linux的执行 &#125; &#125; Post &#123; always &#123; script &#123; //写相关清除/恢复环境等操作代码 &#125; &#125; &#125;&#125; 这个always场景，很容易想到的场景就是，事后清理环境。例如测试完了，对数据库进行恢复操作，恢复到测试之前的环境。 条件2：changed作用：只有当前Pipeline运行的状态与先前完成的Pipeline的状态不同时，才能触发运行。 1234567891011121314151617181920pipeline &#123; agent &#123; node &#123; label 'xxx-agent-机器' customWorkspace \"$&#123;env.JOB_NAME&#125;/$&#123;env.BUILD_NUMBER&#125;\" &#125; &#125; stages &#123; stage ('Build') &#123; sh \"pwd\" //这个是Linux的执行 &#125; &#125; Post &#123; changed &#123; script &#123; // 例如发邮件代码 &#125; &#125; &#125;&#125; 这个场景，大部分是写发邮件状态。例如，你最近几次构建都是成功，突然变成不是成功状态，里面就触发发邮件通知。当然，使用changed这个指令没success和failure要频率高 条件3：failure作用：只有当前Pipeline运行的状态与先前完成的Pipeline的状态不同时，才能触发运行。 12345678910111213141516171819202122pipeline &#123; agent &#123; node &#123; label 'xxx-agent-机器' customWorkspace \"$&#123;env.JOB_NAME&#125;/$&#123;env.BUILD_NUMBER&#125;\" &#125; &#125; stages &#123; stage ('Build') &#123; steps&#123; sh \"pwd\" //这个是Linux的执行 &#125; &#125; &#125; Post &#123; failure &#123; script &#123; // 例如发邮件代码 &#125; &#125; &#125;&#125; 这个failure条件一般来说，百分百会写到Pipeline代码中，内容无非就是发邮件通知，或者发微信群，钉钉机器人，还有国外的slack聊天群组等。 stages和steps指令 stages被外层的pipeline { }包裹，内部包含多个stage 每个stage代码块内包含多个steps { }，一个stage下至少有一个steps { }，一般也就是一个steps { } 我们可以在一个steps下写调用一个或者几个方法，也就是两三行代码。具体的代码实现，可以放在别的包里面 stages下可以包含多个stage, 在一个Declarative Pipeline脚本中，只允许出现一次stages 以后我们大部分的pipeline代码都在每一个stage里面的steps下,如下示例 1234567891011121314151617181920pipeline &#123; agent any stages &#123; stage(\"Build\") &#123; steps &#123; println \"Build\" &#125; &#125; stage(\"Test\") &#123; steps &#123; println \"Test\" &#125; &#125; stage(\"Deploy\") &#123; steps &#123; println \"Deploy\" &#125; &#125; &#125;&#125; 上面println是Groovy的语法，就是一个打印语句。不管以后pipeline代码有多么复杂，都是以这个为基础骨架，例如添加一些try catch语句还有其他的指令。 stage指令该stage指令在该stages部分中，应包含步骤部分，可选agent部分或其他特定于阶段的指令。实际上，Pipeline完成的所有实际工作都将包含在一个或多个stage指令中。 stage一定是在stages{…}里面，一个pipeline{…}中至少有一个stages{…}和一个stage{…}.这里多说一句，一个stage{…}中至少有一个steps{…}。stage{…}还有一个特点就是，里面有一个强制的字符串参数，例如下面的”Example”，这个字符串参数就是描述这个stage是干嘛的，这个字符串参数是不支持变量的，只能你自己取名一个描述字段。 12345678910pipeline &#123; agent any stages &#123; stage('Example') &#123; steps &#123; echo 'Hello World' &#125; &#125; &#125;&#125; environment指令environment指令定义一个键-值，该键-值对将被定义为所有步骤的环境变量，或者是特定于阶段的步骤， 这取决于 environment 指令在流水线内的位置。 解释一下什么意思，environment{…}, 大括号里面写一些键值对，也就是定义一些变量并赋值，这些变量就是环境变量。环境变量的作用范围，取决你environment{…}所写的位置，你可以写在顶层环境变量，让所有的stage下的step共享这些变量，也可以单独定义在某一个stage下，只能供这个stage去调用变量，其他的stage不能共享这些变量。 一般来说，我们基本上上定义全局环境变量，如果是局部环境变量，我们直接用def关键字声明就可以，没必要放environment{…}里面 12345678910111213Pipeline &#123; agent any environment &#123; boolStatus = true &#125; stages &#123; stage('Demo') &#123; if(boolStatus == true)&#123; // Todo &#125; &#125; &#125;&#125; options指令该options指令允许在Pipeline本身内配置Pipeline专用选项，Pipeline提供了许多这些选项，例如buildDiscarder，但它们也可能由插件提供，例如 timestamps。 一个pipeline{…}内只运行出现一次options{…}, 下面看一个下这个retry的使用。 12345678910111213pipeline &#123; agent any option &#123; retry(3) &#125; stages &#123; stage(\"Demo\")&#123; steps &#123; //Tode &#125; &#125; &#125;&#125; 上面的整个pipeline{…}, 如果在jenkins上job执行失败，会继续执行，如果再遇到失败，继续执行一次，总共执行三次 把options{…}放在顶层里，也可以放在具体的某一个stage下，意味这这个stage下所有代码，如果遇到失败，最多执行三次。 parameters指令parameters是参数的意思，parameters指令提供用户在触发Pipeline时应提供的参数列表，这些用户指定的参数的值通过该params对象可用于Pipeline步骤。 我们很多人听过参数化构建(Build with Parameters)，也可能知道如何在一个jenkins job上，通过UI创建不同的参数，例如有字符串参数，布尔选择参数，下拉多选参数等。这些参数即可以通过UI点击创建，也可以通过pipeline代码去写出来。我们先来看看了解有那些具体参数类型，然后挑选几个，分别用UI和代码方式去实现创建这些参数。 字符串参数就是定义一个字符串参数，用户可以在Jenkins UI上输入字符串，常见使用这个参数的场景有，用户名，收件人邮箱，文件网络路径，主机名称的或者url等 123456pipeline &#123; agent any parameters &#123; string(name: 'DEPLOY_ENV', defaultValue: 'staging', description: '') &#125;&#125; 布尔值参数就是定义一个布尔类型参数，用户可以在Jenkins UI上选择是还是否，选择是表示代码会执行这部分，如果选择否，会跳过这部分。一般需要使用布尔值的场景有，执行一些特定集成的脚本或则工作，或者事后清除环境，例如清楚Jenkins的workspace这样的动作。 123456pipeline &#123; agent any parameters &#123; booleanParam(name: 'DEBUG_BUILD', defaultValue: true, description: '') &#125;&#125; 文本参数文本（text）的参数就是支持写很多行的字符串，这个变量我好像没有使用过，例如想给发送一段欢迎的消息，你可以采用text的参数。 1234567pipeline &#123; agent any parameters &#123; text(name: 'Welcome_text', defaultValue: 'One\\nTwo\\nThree\\n', description: '') &#125; &#125; 上面的\\n表示换行，上面写了三行的text 选择参数选择（choice）的参数就是支持用户从多个选择项中，选择一个值用来表示这个变量的值。工作中常用的场景，有选择服务器类型，选择版本号等。 1234567pipeline &#123; agent any parameters &#123; choice(name: 'ENV_TYPE', choices: ['test', 'dev', 'product'], description: 'test means test env,….') &#125;&#125; 文件参数文件（file）参数就是在Jenkins 参数化构建UI上提供一个文件路径的输入框，Jenkins会自动去你提供的网络路径去查找并下载。一般伴随着还有你需要在Pipleline代码中写解析文件。也有这样场景，这个构建job就是把一个war包部署到服务器上特定位置，你可以使用这个文件参数。 123456pipeline &#123; agent any parameters &#123; name: 'FILE', description: 'Some file to upload') &#125;&#125; 密码参数密码（password）参数就是在Jenkins 参数化构建UI提供一个密文密码输入框，例如，我需要在一些linux机器上做自动化操作，需要提供机器的用户名和密码，由于密码涉及安全问题，一般都采用暗文显示，这个时候你就不能用string类型参数，就需要使用password参数类型 1234567pipeline &#123; agent any parameters &#123; password(name: 'PASSWORD', defaultValue: 'SECRET', description: 'A secret password') &#125;&#125; web ui方式 12345678910111213pipeline &#123; agent any stages &#123; stage ('Test') &#123; steps&#123; println \"stringValue = $&#123;stringValue&#125;\" println \"passwordValue = $&#123;passwordValue&#125;\" println \"boolValue = $&#123;boolValue&#125;\" println \"choseValue = $&#123;choseValue&#125;\" &#125; &#125; &#125;&#125; triggers指令 该triggers指令定义了Pipeline应重新触发的自动化方式。对于与源代码集成的Pipeline，如GitHub或BitBucket，triggers可能不需要基于webhook的集成可能已经存在 目前有三个可用的触发器是cron和pollSCM 和 upstream 在一个pipeline{…}代码中，只运行出现一次triggers{…},而且这个指令不是必须存在的。 triggers是触发器的意思，所以这块是设置什么条件下触发pipeline代码执行，以及触发的频率 cron接受一个cron风格的字符串来定义Pipeline应重新触发的常规间隔，例如： triggers { cron(‘H 4/* 0 0 1-5’) } ####pollSCM 接受一个cron风格的字符串来定义Jenkins应该检查新的源更改的常规间隔。如果存在新的更改，则Pipeline将被重新触发。例如：triggers { pollSCM(‘H 4/* 0 0 1-5’) } upstream接受逗号分隔的作业字符串和阈值。 当字符串中的任何作业以最小阈值结束时，将重新触发pipeline。例如：triggers { upstream(upstreamProjects: ‘job1,job2’, threshold: hudson.model.Result.SUCCESS) } 举例一个可能利用scm的场景，如果一个公司做到了很好的代码覆盖测试，一般都会，如果监控到有人提交代码，就会自动化触发启动相关的单元测试。这个场景就是适合在pipeline代码里使用triggers指令，下面代码举例一个pollSCM的基本使用。 12345678910111213pipeline &#123; agent any triggers &#123; pollSCM (‘H H(9-16)/2 * * 1-5)’) &#125; stages &#123; stage('Example') &#123; steps &#123; echo 'Hello World' &#125; &#125; &#125;&#125; 解释下“H H(9-16)/2 1-5)”的含义，这个你可以在上面截图这个页面点击右侧这个问号，出来具体含义。第一步，先根据空格，把字符串切割成5段 所以，H H(9-16)/2 1-5) 的含义就是： 第一部分“H” 表示hash，记住不是表示hour，是一个散列值，含义就是在一个小时之内，会执行一次，但是这次是一个散列值，而且不会并发执行。 第二部分“H(9-16)/2”，表示白天在早上9点到下午5点，每间隔2小时执行一次。 第三部分“*“，每天执行 第四部分“*“表示每月执行 第五部分“1-5“ 表示周一到周五执行 所以上面这个表达式“H H(9-16)/2 1-5) “的含义就是，在每个月的周一到周五的白天，从早上9点到下午5点，每间隔两个小时去触发一次自动化构建。 这个就比较适合，我们每天上班，间隔两个小时去跑一次单元自动化测试。间隔时间长短，取决服务器压力和业务具体场景 input指令该input指令允许在一个stage{…}显示提示输入等待。在inpt{…}写一些条件，然后用户触发构建这个job，但是这个时候没有接收到有效的input, job会一直在等待中； 下面解释input{…}里面支持写那些option。 message必选，这个message会在用户提交构建的页面显示，提示用户提交相关的input条件 id可选，可以作为这个input的标记符，默认的标记符是这个stage的名称 ok可选， 主要是在ok按钮上显示一些文本，在input表单里 submitter可选，里面可以写多个用户名称或者组名称，用逗号隔开。意思就是，只有这写名称的对应用户登陆jenkins，才能提交这个input动作，如果不写，默认是任何人都可以提交input。 parameters可选，我们前面学的parameters没有区别，就是定义一些参数的地方 代码示例： 1234567891011121314151617181920pipeline &#123; agent any stages &#123; stage ('input-test') &#123; input &#123; message \"是否继续执发布操作?\" ok \"是的,继续执行\" // 用户,好像设置了没什么用 submitter \"admin\" parameters &#123; string(name: 'NAME', defaultValue: 'zhimma') &#125; &#125; steps &#123; echo \"Hello, $&#123;NAME&#125;, nice to meet you.\" &#125; &#125; &#125; &#125; when指令when指令允许流水线根据给定的条件决定是否应该执行阶段 when指令必须包含至少一个条件。 when 指令包含多个条件, 所有的子条件必须返回True，阶段才能执行 下面详细解释下when可以使用的内置条件 branch当正在构建的分支与模式给定的分支匹配时，执行这个阶段;例如：when { branch &#39;master&#39; }。请注意，这仅适用于多分支Pipeline。 environment当指定的环境变量是给定的值时，执行这个步骤, 例如: when { environment name: &#39;DEPLOY_TO&#39;, value: &#39;production&#39; } expression当指定的Groovy表达式评估为true时，执行这个阶段, 例如: when { expression { return params.DEBUG_BUILD } } not当嵌套条件是错误时，执行这个阶段,必须包含一个条件，例如: when { not { branch &#39;master&#39; } } allOf当所有的嵌套条件都正确时，执行这个阶段,必须包含至少一个条件，例如: when { allOf { branch &#39;master&#39;; environment name: &#39;DEPLOY_TO&#39;, value: &#39;production&#39; } } anyOf当至少有一个嵌套条件为真时，执行这个阶段,必须包含至少一个条件，例如: when { anyOf { branch &#39;master&#39;; branch &#39;staging&#39; } } 在进入 stage的 agent前测试执行when默认情况下, 如果定义了某个阶段的agent，在进入该stage的agent后该 stage的when 条件将会被执行。但是, 可以通过在 when块中指定beforeAgent 选项来更改此选项。如果beforeAgent 被设置为 true, 那么就会首先对when条件进行评估 , 并且只有在when条件验证为真时才会进入agent 12345678910111213141516171819202122232425pipeline &#123; agent any environment &#123; quick_test = false &#125; stages &#123; stage('Example Build') &#123; steps &#123; script &#123; echo 'Hello World' &#125; &#125; &#125; stage('Example Deploy') &#123; when &#123; expression &#123; return (quick_test == \"true\") &#125; &#125; steps &#123; echo 'Deploying' &#125; &#125; &#125;&#125; https://testerhome.com/topics/9977 https://testerhome.com/topics/17251","categories":[{"name":"容器化服务","slug":"容器化服务","permalink":"https://blog.zhimma.com/categories/容器化服务/"},{"name":"Jenkins","slug":"Jenkins","permalink":"https://blog.zhimma.com/categories/Jenkins/"}],"tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"https://blog.zhimma.com/tags/Jenkins/"}]},{"title":"使用Jenkins的Pipeline发布代码至远程服务器","slug":"使用Jenkins的Pipeline发布代码至远程服务器","date":"2019-02-16T14:07:31.000Z","updated":"2019-02-19T01:39:44.080Z","comments":true,"path":"2019/02/16/使用Jenkins的Pipeline发布代码至远程服务器/","link":"","permalink":"https://blog.zhimma.com/2019/02/16/使用Jenkins的Pipeline发布代码至远程服务器/","excerpt":"","text":"通常我们软件开发流程大概是下面所示： 本地开发-&gt;本地自测-&gt;提交代码-&gt;编译发布-&gt;测试人员测试-&gt;提交至生产 在发布阶段， 有的项目是采用版本控制工具在Linux服务器上拉取对应分支的最新代码 有的是在本地对比Git版本的差异，生成差异的文件，打包上传至服务器，进行覆盖原来的代码文件完成发布 上面2种方法是我工作中遇到的，下面我们来学习一种新的发布方式：Jenkins Jenkins可以帮你在写完代码后，一键完成开发过程中的一系列工作 特别是在开发阶段，配合WebHook可以非常省心的完成代码发布工作，开发者只需要提交代码，就会触发Jenkins发布任务的执行，从而将最新代码部署到服务器上 什么是Jenkins？Jenkins是一个Java开放的开源程序，所以，需要提前安装Java JDK环境，能支持安装到windows,mac,linux平台，主要是一个管理工具 为什么要使用Jenkins?我们用它，主要是项目上的持续集成和持续交付。持续集成对应英文（Continuous Integration），有时候简称CI，持续交付对应英文（Continuous Delivery），简称CD，以后，听到了CI和CD，就明白了什么意思。下面这张图，是Jenkins在实际项目运用上的一个经典的流程图 ##安装Jenkins 安装的方式很多，我这里学习Jenkins采用的是Docker创建容器的方式运行 下载Jenkinsdocker pull jenkins 下载完成，查看下 123☁ ~ docker image lsREPOSITORY TAG IMAGE ID jenkins/jenkins latest 9b74eda1c268 创建映射目录这个目录根据个人需求，可以进行重新指定，我指定的是：/Users/zhimma/jenkins mkdir /Users/zhimma/jenkins 创建容器docker run -d -p 49001:8080 -v $PWD/jenkins:/var/jenkins_home -t jenkins/jenkins 我映射了容器的端口8080到主机上的端口49001， 第一个数字代表主机上的端口，而最后一个代表容器的端口 运行后，Docker会帮我们创建一个Jenkins的运行环境的容器，使用docker ps 查看容器启动情况 123☁ ~ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES782c2fb5ef74 jenkins/jenkins &quot;/sbin/tini -- /usr/…&quot; 3 days ago Up 3 days 50000/tcp, 0.0.0.0:49001-&gt;8080/tcp trusting_burnell 到这，我们的Jenkins环境搭建完成，下面我们做一些初始化工作 初始化Jenkins解锁Jenkins在浏览器中输入localhost:49001，进入web页面，第一次需要先解锁Jenkins 进入/Users/zhimma/jenkins/secrets目录，制initialAdminPassword文件的内容就是首次解锁的密码 安装推荐插件 设置管理员安装Jenkins插件我们目前使用GitLab管理代码，所以我们先安装下面几个插件 GitLab Plugin Gitlab Hook Plugin AnsiColor（可选）这个插件可以让Jenkins的控制台输出的log带有颜色（就和linux控制台那样） ###配置SSH 本机生成SSH：ssh-keygen -t rsa -C &quot;Your email&quot;，最终生成id_rsa和id_rsa.pub(公钥) Gitlab上添加公钥：复制id_rsa.pub里面的公钥添加到Gitlab Jenkins上配置密钥到SSH：复制id_rsa里面的公钥添加到Jenkins（private key选项） ##开始Pipeline 在Jenkins中，把每一段管道比作是不同的Job，我们提到Jenkins的工作流程，build-deploy-test-release，每个流程之间我们都可以用Pipeline来连接，大致如下效果图。 SCM:软件配置管理工具 ###创建Pipeline风格的项目 ###实现Pipeline-Web UI方式 如下图所示： ###实现Pipeline-Jenkinsfile方式 上面通过Web UI方式只适用于非常简单的任务，而大型复杂的任务最好采用Jenkinsfile方式并纳入SCM管理。 这次我选择从SCM中的Jenkinsfile来定义管道。 我们需要在自己的项目根目录创建Jenkinsfile文件，在里面编写具体的发布流程代码。 使用Jenkinsfile接下来详细介绍一下怎样编写Jenkinsfile来完成各种复杂的任务。 Pipeline支持两种形式，一种是Declarative管道，一个是Scripted管道。 一个Jenkinsfile就是一个文本文件，里面定义了Jenkins Pipeline。 将这个文本文件放到项目的根目录下面，纳入版本系统。 ####Declarative风格类型 123456789101112131415161718192021pipeline &#123; agent any stages &#123; stage('Build') &#123; steps &#123; echo 'Building..' &#125; &#125; stage('Test') &#123; steps &#123; echo 'Testing..' &#125; &#125; stage('Deploy') &#123; steps &#123; echo 'Deploying..' &#125; &#125; &#125;&#125; 上面是一个Declarative类型的Pipeline，目前实际开发基本采用这种方式； 第一行是小写的pipeline，然后一对大括{}，大括号里面就是代码块，用来和别的代码块隔离出来，pipeline是一个语法标识符,也叫关键字，如果是Declarative类型，一定是pipeline {}这样起头的；如果是脚本文件，pipeline不要求一定是第一行代码。也就是说pipeline前面可以有其他代码，例如导入语句，和其他功能代码。pipeline是一个执行pipeline代码的入口，jenkins可以根据这个入门开始执行里面不同stage 第二行agent any，agent是一个语法关键字，any是一个option类型，agent是代理的意思，这个和选择用jenkins平台上那一台机器去执行任务构建有关 第三行stages{}, stages是多个stage的意思，也就是说一个stages可以包含多个stage，从上面代码结果你也可以看出来。上面写了三个stage，根据你任务需要，你可以写十多个都可以 第四行stage(‘Build’) {}, 这个就是具体定义一个stage,一般一个stage就是指完成一个业务场景。Build是认为给这个任务取一个名字。 第五行steps{},字面意思就是很多个步骤的意思。这里提一下，看到了steps，当然还有step这个指令。一般来说，一个steps{}里面就写几行代码，或者一个try catch语句。 postpost section 定义了管道执行结束后要进行的操作。支持在里面定义很多Conditions块： always, changed, failure, success 和 unstable。 这些条件块会根据不同的返回结果来执行不同的逻辑。 always：不管返回什么状态都会执行 changed：如果当前管道返回值和上一次已经完成的管道返回值不同时候执行 failure：当前管道返回状态值为”failed”时候执行，在Web UI界面上面是红色的标志 success：当前管道返回状态值为”success”时候执行，在Web UI界面上面是绿色的标志 unstable：当前管道返回状态值为”unstable”时候执行，通常因为测试失败，代码不合法引起的。在Web UI界面上面是黄色的标志 12345678910111213141516// Declarative //pipeline &#123; agent any stages &#123; stage('Example') &#123; steps &#123; echo 'Hello World' &#125; &#125; &#125; post &#123; ① always &#123; ② echo 'I will always say Hello again!' &#125; &#125;&#125; stages由一个或多个stage指令组成，stages块也是核心逻辑的部分。 我们建议对于每个独立的交付部分（比如Build,Test,Deploy）都应该至少定义一个stage指令。比如： 1234567891011// Declarative //pipeline &#123; agent any stages &#123; ① stage('Example') &#123; steps &#123; echo 'Hello World' &#125; &#125; &#125;&#125; steps在stage中定义一系列的step来执行命令。 1234567891011// Declarative //pipeline &#123; agent any stages &#123; stage('Example') &#123; steps &#123; ① echo 'Hello World' &#125; &#125; &#125;&#125; agentagent指令指定整个管道或某个特定的stage的执行环境。它的参数可用使用： any - 任意一个可用的agent none - 如果放在pipeline顶层，那么每一个stage都需要定义自己的agent指令 label - 在jenkins环境中指定标签的agent上面执行，比如agent { label &#39;my-defined-label&#39; } node - agent { node { label &#39;labelName&#39; } } 和 label一样，但是可用定义更多可选项 docker - 指定在docker容器中运行 dockerfile - 使用源码根目录下面的Dockerfile构建容器来运行 environmentenvironment定义键值对的环境变量 1234567891011121314151617// Declarative //pipeline &#123; agent any environment &#123; ① CC = 'clang' &#125; stages &#123; stage('Example') &#123; environment &#123; ② AN_ACCESS_KEY = credentials('my-prefined-secret-text') ③ &#125; steps &#123; sh 'printenv' &#125; &#125; &#125;&#125; options还能定义一些管道特定的选项，介绍几个常用的： skipDefaultCheckout - 在agent指令中忽略源码checkout这一步骤。 timeout - 超时设置options { timeout(time: 1, unit: &#39;HOURS&#39;) } retry - 直到成功的重试次数options { retry(3) } timestamps - 控制台输出前面加时间戳options { timestamps() } parameters参数指令，触发这个管道需要用户指定的参数，然后在step中通过params对象访问这些参数。 1234567891011121314// Declarative //pipeline &#123; agent any parameters &#123; string(name: 'PERSON', defaultValue: 'Mr Jenkins', description: 'Who should I say hello to?') &#125; stages &#123; stage('Example') &#123; steps &#123; echo \"Hello $&#123;params.PERSON&#125;\" &#125; &#125; &#125;&#125; triggers触发器指令定义了这个管道何时该执行，一般我们会将管道和GitHub、GitLab、BitBucket关联， 然后使用它们的webhooks来触发，就不需要这个指令了。如果不适用webhooks，就可以定义两种cron和pollSCM cron - linux的cron格式triggers { cron(&#39;H 4/* 0 0 1-5&#39;) } pollSCM - jenkins的poll scm语法，比如triggers { pollSCM(&#39;H 4/* 0 0 1-5&#39;) } 1234567891011121314// Declarative //pipeline &#123; agent any triggers &#123; cron('H 4/* 0 0 1-5') &#125; stages &#123; stage('Example') &#123; steps &#123; echo 'Hello World' &#125; &#125; &#125;&#125; stagestage指令定义在stages块中，里面必须至少包含一个steps指令，一个可选的agent指令，以及其他stage相关指令。 1234567891011// Declarative //pipeline &#123; agent any stages &#123; stage('Example') &#123; steps &#123; echo 'Hello World' &#125; &#125; &#125;&#125; tools定义自动安装并自动放入PATH里面的工具集合 1234567891011121314// Declarative //pipeline &#123; agent any tools &#123; maven 'apache-maven-3.0.1' ① &#125; stages &#123; stage('Example') &#123; steps &#123; sh 'mvn --version' &#125; &#125; &#125;&#125; 注：① 工具名称必须预先在Jenkins中配置好了 → Global Tool Configuration. 内置条件 branch - 分支匹配才执行 when { branch &#39;master&#39; } environment - 环境变量匹配才执行 when { environment name: &#39;DEPLOY_TO&#39;, value: &#39;production&#39; } expression - groovy表达式为真才执行 expression { return params.DEBUG_BUILD } } 1234567891011121314151617// Declarative //pipeline &#123; agent any stages &#123; stage(&apos;Example Build&apos;) &#123; steps &#123; echo &apos;Hello World&apos; &#125; &#125; stage(&apos;Example Deploy&apos;) &#123; when &#123; branch &apos;production&apos; &#125; echo &apos;Deploying&apos; &#125; &#125;&#125; Steps这里就是实实在在的执行步骤了，每个步骤step都具体干些什么东西， 前面的Sections、Directives算控制逻辑和环境准备，这里的就是真实执行步骤。 这部分内容最多不可能全部讲完，官方Step指南 包含所有的东西。 Declared Pipeline和Scripted Pipeline都能使用这些step，除了下面这个特殊的script。 一个特殊的step就是script，它可以让你在声明管道中执行脚本，使用groovy语法，这个非常有用： 123456789101112131415161718192021222324// Declarative //pipeline &#123; agent any stages &#123; stage('Example') &#123; steps &#123; echo 'Hello World' script &#123; def browsers = ['chrome', 'firefox'] for (int i = 0; i &lt; browsers.size(); ++i) &#123; echo \"Testing the $&#123;browsers[i]&#125; browser\" &#125; &#125; script &#123; // 一个优雅的退出pipeline的方法，这里可执行任意逻辑 if( $VALUE1 == $VALUE2 ) &#123; currentBuild.result = 'SUCCESS' return &#125; &#125; &#125; &#125; &#125;&#125; Scripted风格类型1234567891011node &#123; stage('Build') &#123; // &#125; stage('Test') &#123; // &#125; stage('Deploy') &#123; // &#125;&#125; 这个代码，有两点和上面不同。 第一个是Scripted模式是node{}开头，并没有pipeline{}直观。 第二个要指出的是，scripted模式下没有stages这个关键字或者指令，只有stage。上面其实可以node(‘Node name’) {}来开头，Node name就是从节点或master节点的名称。 Scripted Pipeline没那么多东西，就是定义一个node， 里面多个stage，里面就是使用Groovy语法执行各个step了，非常简单和清晰，也非常灵活。 环境变量Jenkins定了很多内置的环境变量，可在文档localhost:49001/pipeline-syntax/globals#env找到， 通过env直接使用它们： 1234567891011121314151617181920212223pipeline &#123; agent any stages &#123; stage('Build') &#123; steps &#123; // 测试环境变量 echo \"Running $&#123;env.BUILD_ID&#125; on $&#123;env.JENKINS_URL&#125;\" echo 'Building..' &#125; &#125; stage('Test') &#123; steps &#123; echo 'Testing..' &#125; &#125; stage('Deploy') &#123; steps &#123; echo 'Deploying..' &#125; &#125; &#125;&#125; 在项目根目录修改Jenkinsfile后，提交到服务器，点击立即构建，查看Console Output输出系统的环境变量 参考引用https://blog.csdn.net/u011541946/article/details/83152494 https://blog.csdn.net/u011541946/article/category/8223796/2? https://www.xncoding.com/2017/03/22/fullstack/jenkins02.html","categories":[{"name":"容器化服务","slug":"容器化服务","permalink":"https://blog.zhimma.com/categories/容器化服务/"},{"name":"Jenkins","slug":"Jenkins","permalink":"https://blog.zhimma.com/categories/Jenkins/"}],"tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"https://blog.zhimma.com/tags/Jenkins/"}]},{"title":"Nginx单域名配置多Vue工程和PHP接口","slug":"Nginx单域名配置多Vue工程和PHP接口","date":"2019-02-01T07:16:26.000Z","updated":"2019-03-01T10:06:15.869Z","comments":true,"path":"2019/02/01/Nginx单域名配置多Vue工程和PHP接口/","link":"","permalink":"https://blog.zhimma.com/2019/02/01/Nginx单域名配置多Vue工程和PHP接口/","excerpt":"","text":"点击查看单域名配置多Vue工程和多PHP接口 项目中遇到了一个问题，需要一个域名完成整个项目的部署，之前是使用的多个域名进行解析的，例如： 后台页面：backend.domain.com,后台接口：backend-api.domain.com 前台页面：fontend.domain.com,前台接口：fontend-api.domain.com 现在的需求是：domain.com就可以访问到前后台所有页面的接口 解决思路：Nginx和Vue路由配置 前端配置vue-cli3.x在vue.config.js的文件中加入（此处为了打包后的JS,CSS等文件的路径引向） 123module.exports = &#123; baseUrl:&apos;/frontend/&apos; //根据www.xxx.com/后面的路径写入（比如www.domain.com/frontend）&#125; baseUrl官方文档 Nginx配置1234567891011121314# 匹配含有backend的location /backend &#123; # 重新定义root目录 root /Users/zhimma/Data/www/MK_Project/public/web; # 或者 # alias /Users/zhimma/Data/www/MK_Project/public/web/backend; # 解决刷新页面404错误 if (!-e $request_filename) &#123; rewrite ^/(.*) /backend/index.html last; break; &#125; # 尝试t尝试列出的文件并设置内部文件指向。 try_files $uri $uri/ @router;&#125; PHP 配置1234567891011121314try_files $uri $uri/ @rewrite; location @rewrite &#123; rewrite ^/(.*)$ /index.php?_url=/$1; &#125; location ~ \\.php &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index /index.php; fastcgi_split_path_info ^(.+\\.php)(/.+)$; fastcgi_param PATH_INFO $fastcgi_path_info; fastcgi_param PATH_TRANSLATED $document_root$fastcgi_path_info; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params;&#125; 我的配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950server &#123; listen 80; server_name mk.ma; index index.php index.html index default; root /Users/zhimma/Data/www/MK_Project/public; #error_page 404 /404.html; try_files $uri $uri/ @rewrite; location @rewrite &#123; rewrite ^/(.*)$ /index.php?_url=/$1; &#125; location ~ \\.php &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index /index.php; fastcgi_split_path_info ^(.+\\.php)(/.+)$; fastcgi_param PATH_INFO $fastcgi_path_info; fastcgi_param PATH_TRANSLATED $document_root$fastcgi_path_info; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125; # 匹配含有backend的 location /backend &#123; # 重新定义root目录 root /Users/zhimma/Data/www/MK_Project/public/web; # 或者 # alias /Users/zhimma/Data/www/MK_Project/public/web/backend; # 解决刷新页面404错误 if (!-e $request_filename) &#123; rewrite ^/(.*) /backend/index.html last; break; &#125; # 尝试t尝试列出的文件并设置内部文件指向。 try_files $uri $uri/ @router; &#125; location /frontend &#123; alias /Users/zhimma/Data/www/MK_Project/public/web/frontend; if (!-e $request_filename) &#123; rewrite ^/(.*) /frontend/index.html last; break; &#125; try_files $uri $uri/ @router; &#125; location @router &#123; rewrite ~.*$ /index.html last; &#125; location /favicon.ico &#123; root /Users/zhimma/Data/www/MK_Project/public/web/backend; &#125;&#125; 最后解析域名，访问地址就只需要一个 后台：domain.com/backend 前台：domain.com/frontend 接口地址统一调用：domain.com 总结一下： Nginx功能很强大，下一步需要彻底弄懂Nginx的一些配置项，方便项目的一些需求","categories":[{"name":"Nginx","slug":"Nginx","permalink":"https://blog.zhimma.com/categories/Nginx/"},{"name":"Vue","slug":"Vue","permalink":"https://blog.zhimma.com/categories/Vue/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://blog.zhimma.com/tags/Nginx/"},{"name":"Vue","slug":"Vue","permalink":"https://blog.zhimma.com/tags/Vue/"}]},{"title":"Docker相关知识再整理(1)","slug":"Docker相关知识再整理(1)","date":"2019-01-28T07:05:03.000Z","updated":"2019-04-10T03:15:24.604Z","comments":true,"path":"2019/01/28/Docker相关知识再整理(1)/","link":"","permalink":"https://blog.zhimma.com/2019/01/28/Docker相关知识再整理(1)/","excerpt":"","text":"[TOC] 镜像Docker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。镜像不包含任何动态数据，其内容在构建之后也不会被改变。 个人理解为是创建容器的基础，类似于安装系统是所需的ISO文件，镜像就是生成容器所需的ISO。 镜像获取镜像1docker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签] 列出镜像列出镜像-不显示中间层镜像123☁ ~ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEnginx latest 42b4762643dc 5 days ago 109MB 列表包含了 仓库名、标签、镜像 ID、创建时间 以及 所占用的空间 为了加速镜像构建、重复利用资源，Docker 会利用 中间层镜像 所有镜像：1☁ ~ docker image ls -a 特定格式显示镜像列表12☁ ~ docker image ls -q42b4762643dc 12docker image ls --format \"&#123;&#123;.ID&#125;&#125;: &#123;&#123;.Repository&#125;&#125;\"5f515359c7f8: redis 查看镜像、容器、数据卷所占用的空间123456☁ ~ docker system dfTYPE TOTAL ACTIVE SIZE RECLAIMABLEImages 14 13 1.347GB 211.8MB (15%)Containers 23 22 4.962kB 0B (0%)Local Volumes 0 0 0B 0BBuild Cache 0 0 0B 0B 悬浮镜像清理无标签镜像也被称为 虚悬镜像(dangling image) ，可以用下面的命令专门显示这类镜像： 123$ docker image ls -f dangling=trueREPOSITORY TAG IMAGE ID CREATED SIZE&lt;none&gt; &lt;none&gt; 00285df0df87 5 days ago 342 MB 1234☁ ~ docker image pruneWARNING! This will remove all dangling images.Are you sure you want to continue? [y/N] yTotal reclaimed space: 0B 删除镜像1docker image rm [选项] &lt;镜像1&gt; [&lt;镜像2&gt; ...] 其中，&lt;镜像&gt; 可以是 镜像短 ID、镜像长 ID、镜像名 或者 镜像摘要 12# 删除所有镜像docker image rm $(docker image ls -q) 保存容器为镜像运行一个容器的时候（如果不使用卷的话），我们做的任何文件修改都会被记录于容器存储层里 1docker commit [选项] &lt;容器ID或容器名&gt; [&lt;仓库名&gt;[:&lt;标签&gt;]] Demo: 123456docker commit \\ --author \"Nanme &lt;Email&gt;\" \\ --message \"Description\" \\ webserver \\ nginx:v2sha256:07e33465974800ce65751acc279adc6ed2dc5ed4e0838f8b86f0c87aa1795214 其中 --author 是指定修改的作者，而 --message 则是记录本次修改的内容、 使用 docker commit 意味着所有对镜像的操作都是黑箱操作，生成的镜像也被称为黑箱镜像，换句话说，就是除了制作镜像的人知道执行过什么命令、怎么生成的镜像，别人根本无从得知 容器容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的 命名空间。因此容器可以拥有自己的 root 文件系统、自己的网络配置、自己的进程空间，甚至自己的用户 ID 空间。容器内的进程是运行在一个隔离的环境里，使用起来，就好像是在一个独立于宿主的系统下操作一样 镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。 仓库一个集中的存储、分发镜像的服务，Docker Registry 就是这样的服务。 一个 Docker Registry 中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像 通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本。我们可以通过 &lt;仓库名&gt;:&lt;标签&gt; 的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 latest 作为默认标签。 DockerfileDockerfile 是一个文本文件，其内包含了一条条的指令(Instruction)，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。 构建镜像1docker build [选项] &lt;上下文路径/URL/-&gt; 在 Dockerfile 文件所在目录执行： 1docker build -t nginx:v3 . ###FROM FROM 就是指定基础镜像，因此一个 Dockerfile 中 FROM 是必备的指令，并且必须是第一条指令。 RUN 执行命令RUN 指令是用来执行命令行命令的。由于命令行的强大能力，RUN 指令在定制镜像时是最常用的指令之一， 其格式有两种： shell 格式：RUN &lt;命令&gt;，就像直接在命令行中输入的命令一样。刚才写的 Dockerfile 中的 RUN 指令就是这种格式。 1RUN echo '&lt;h1&gt;Hello, Docker!&lt;/h1&gt;' &gt; /usr/share/nginx/html/index.html exec 格式：RUN [&quot;可执行文件&quot;, &quot;参数1&quot;, &quot;参数2&quot;]，这更像是函数调用中的格式。 COPY 复制文件格式： COPY [--chown=&lt;user&gt;:&lt;group&gt;] &lt;源路径&gt;... &lt;目标路径&gt; COPY [--chown=&lt;user&gt;:&lt;group&gt;] [&quot;&lt;源路径1&gt;&quot;,... &quot;&lt;目标路径&gt;&quot;] 和 RUN 指令一样，也有两种格式，一种类似于命令行，一种类似于函数调用。 COPY 指令将从构建上下文目录中 &lt;源路径&gt; 的文件/目录复制到新的一层的镜像内的 &lt;目标路径&gt; 位置。 ​ &lt;源路径&gt; 可以是多个，甚至可以是通配符，其通配符规则要满足 Go 的 filepath.Match 规则，如： 12COPY hom* /mydir/COPY hom?.txt /mydir/ 使用 COPY 指令，源文件的各种元数据都会保留。比如读、写、执行权限、文件变更时间等 ADD 更高级的复制文件ADD 指令和 COPY 的格式和性质基本一致。但是在 COPY 基础上增加了一些功能。 如果 &lt;源路径&gt; 为一个 tar 压缩文件的话，压缩格式为 gzip, bzip2 以及 xz 的情况下，ADD 指令将会自动解压缩这个压缩文件到 &lt;目标路径&gt; 去。 在 Docker 官方的 Dockerfile 最佳实践文档 中要求，尽可能的使用 COPY，因为 COPY 的语义很明确，就是复制文件而已，而 ADD 则包含了更复杂的功能，其行为也不一定很清晰。最适合使用 ADD 的场合，就是所提及的需要自动解压缩的场合。 在使用该指令的时候还可以加上 --chown=&lt;user&gt;:&lt;group&gt; 选项来改变文件的所属用户及所属组。 1ADD --chown=55:mygroup files* /mydir/ CMD 容器启动命令CMD 指令的格式和 RUN 相似，也是两种格式： shell 格式：CMD &lt;命令&gt; exec 格式：CMD [&quot;可执行文件&quot;, &quot;参数1&quot;, &quot;参数2&quot;...] 参数列表格式：CMD [&quot;参数1&quot;, &quot;参数2&quot;...]。在指定了 ENTRYPOINT 指令后，用 CMD 指定具体的参数。 之前介绍容器的时候曾经说过，Docker 不是虚拟机，容器就是进程。既然是进程，那么在启动容器的时候，需要指定所运行的程序及参数。CMD 指令就是用于指定默认的容器主进程的启动命令的。 VOLUME 定义匿名卷格式为： VOLUME [&quot;&lt;路径1&gt;&quot;, &quot;&lt;路径2&gt;&quot;...] VOLUME &lt;路径&gt; EXPOSE 声明端口格式为 EXPOSE &lt;端口1&gt; [&lt;端口2&gt;...]。 要将 EXPOSE 和在运行时使用 -p &lt;宿主端口&gt;:&lt;容器端口&gt; 区分开来。-p，是映射宿主端口和容器端口，换句话说，就是将容器的对应端口服务公开给外界访问，而 EXPOSE 仅仅是声明容器打算使用什么端口而已，并不会自动在宿主进行端口映射。 ENV 设置环境变量格式有两种： ENV &lt;key&gt; &lt;value&gt; ENV &lt;key1&gt;=&lt;value1&gt; &lt;key2&gt;=&lt;value2&gt;... 这个指令很简单，就是设置环境变量而已，用法如下面的格式$KEY ARG 构建参数格式：ARG &lt;参数名&gt;[=&lt;默认值&gt;] 构建参数和 ENV 的效果一样，都是设置环境变量，ARG 所设置的构建环境的环境变量，在将来容器运行时是不会存在这些环境变量的。不要因此就使用 ARG 保存密码之类的信息，因为 docker history 还是可以看到所有值的。 Dockerfile 中的 ARG 指令是定义参数名称，以及定义其默认值。该默认值可以在构建命令 docker build 中用 --build-arg &lt;参数名&gt;=&lt;值&gt; 来覆盖。","categories":[{"name":"容器化服务","slug":"容器化服务","permalink":"https://blog.zhimma.com/categories/容器化服务/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://blog.zhimma.com/tags/Docker/"}]},{"title":"1-Mac-Docker-Kubernetes-Rancher-环境初始化","slug":"1-Mac-Docker-Kubernetes-Rancher-环境初始化","date":"2019-01-25T04:01:43.000Z","updated":"2019-02-19T01:39:31.625Z","comments":true,"path":"2019/01/25/1-Mac-Docker-Kubernetes-Rancher-环境初始化/","link":"","permalink":"https://blog.zhimma.com/2019/01/25/1-Mac-Docker-Kubernetes-Rancher-环境初始化/","excerpt":"","text":"composer install --ignore-platform-reqs","categories":[{"name":"容器化服务","slug":"容器化服务","permalink":"https://blog.zhimma.com/categories/容器化服务/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"https://blog.zhimma.com/tags/Mac/"},{"name":"k8s","slug":"k8s","permalink":"https://blog.zhimma.com/tags/k8s/"},{"name":"Rancher","slug":"Rancher","permalink":"https://blog.zhimma.com/tags/Rancher/"}]},{"title":"CentOS7☞lnmp环境搭建","slug":"CentOS7☞lnmp环境搭建","date":"2018-11-29T16:00:00.000Z","updated":"2019-02-12T06:46:06.957Z","comments":true,"path":"2018/11/30/CentOS7☞lnmp环境搭建/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/CentOS7☞lnmp环境搭建/","excerpt":"本文转自这里 简介 LNMP是Linux、Nginx、MySQL(MariaDB)和PHP的缩写，这个组合是最常见的WEB服务器的运行环境之一。 本文将带领大家在CentOS 7操作系统上搭建一套LNMP环境。 本教程适用于CentOS 7.x版本。","text":"本文转自这里 简介 LNMP是Linux、Nginx、MySQL(MariaDB)和PHP的缩写，这个组合是最常见的WEB服务器的运行环境之一。 本文将带领大家在CentOS 7操作系统上搭建一套LNMP环境。 本教程适用于CentOS 7.x版本。 安装Nginx yum install nginx 如果报没有可用的软件包nginx错误，解决方法这这里 按照提示，输入yes后开始安装。安装完毕后，Nginx的配置文件在/etc/nginx目录下。使用以下命令启动Nginx：systemctl start nginx 检查系统中firewalld防火墙服务是否开启，如果已开启，我们需要修改防火墙配置，开启Nginx外网端口访问。systemctl status firewalld 如果显示active (running)，则需要调整防火墙规则的配置。 修改/etc/firewalld/zones/public.xml文件，在zone一节中增加：1234&lt;zone&gt; ... &lt;service name=&quot;nginx&quot;/&gt;&lt;zone&gt; 保存后重新加载firewalld服务：systemctl reload firewalld 可以通过浏览器访问 http://&lt;外网IP地址&gt; 来确定Nginx是否已经启动。 最后将Nginx设置为开机启动：systemctl enable nginx.service 测试环境的话，为了方便也可以先禁用掉防火墙 安装MySQL(MariaDB)https://my.oschina.net/Laily/blog/713023 MariaDB是MySQL的一个分支，主要由开源社区进行维护和升级，而MySQL被Oracle收购以后，发展较慢。在CentOS 7的软件仓库中，将MySQL更替为了MariaDB。 我们可以使用yum直接安装MariaDB：yum install mariadb-server 安装完成之后，执行以下命令重启MariaDB服务： systemctl start mariadb MariaDB默认root密码为空，我们需要设置一下，执行脚本：/usr/bin/mysql_secure_installation 首先提示输入当前的root密码：Enter current password for root (enter for none): 初始root密码为空，我们直接敲回车进行下一步Set root password? [Y/n] 设置root密码，默认选项为Yes，我们直接回车，提示输入密码，在这里设置您的MariaDB的root账户密码Remove anonymous users? [Y/n] 是否移除匿名用户，默认选项为Yes，建议按默认设置，回车继续Disallow root login remotely? [Y/n] 是否禁止root用户远程登录？如果您只在本机内访问MariaDB，建议按默认设置，回车继续Remove test database and access to it? [Y/n] 是否删除测试用的数据库和权限？ 建议按照默认设置，回车继续Reload privilege tables now? [Y/n] 是否重新加载权限表？因为我们上面更新了root的密码，这里需要重新加载，回车。 完成后你会看到Success!的提示，MariaDB的安全设置已经完成。我们可以使用以下命令登录MariaDB：mysql -uroot -p 按提示输入root密码，就会进入MariaDB的交互界面，说明已经安装成功。 最后我们将MariaDB设置为开机启动systemctl enable mariadb 安装PHP https://www.yaosansi.com/post/install-php-yum-on-centos/我们可以直接使用yum安装PHP：yum install php-fpm php-mysql 安装完成后我们将php-fpm启动：systemctl start php-fpm 将php-fpm设置为开机启动: systemctl enable php-fpm php安装完成之后，需要设置一下php session的目录：12sudo mkdir /var/lib/php/session/sudo chown -R apache:apache /var/lib/php/session/ 这时php-fpm已经安装完毕，但是现在需要配置一下Nginx，在/etc/nginx/conf.d目录中新建一个名为php.conf的文件，其内容为：123456789101112server &#123; listen 80; # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # location ~ \\.php$ &#123; root /usr/share/php; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125;&#125; 然后执行以下命令使我们的配置生效：systemctl reload nginx 以上我们配置了Nginx的8000端口用来测试，如果您在美团云控制台创建机器时选择了绑定防火墙，需要检查该防火墙是否允许80端口，如果不允许的话，可以在防火墙设置中新增防火墙，并关联到该主机。 我们在/usr/share/php目录下新建一个名为phpinfo.php的文件用来展示phpinfo信息，文件内容为： &lt;?php echo phpinfo(); ?&gt; 我们从浏览器打开 http://&lt;外网IP地址&gt;:80/phpinfo.php，您就能看到phpinfo信息了，说明我们php环境已经部署成功; 升级PHP版本 yum 默认安装的版本是5.4，现在升级PHP版本至5.6 执行下面命令：123rpm -Uvh https://mirror.webtatic.com/yum/el7/epel-release.rpmrpm -Uvh https://mirror.webtatic.com/yum/el7/webtatic-release.rpm 执行下面的命令删除phpyum remove php-common然后像安装那样问你是否继续的，输入yes即可 安装php5.6 yum install -y php56w php56w-opcache php56w-xml php56w-fpm php56w-mcrypt php56w-gd php56w-devel php56w-mysql php56w-intl php56w-mbstring 查看php版本php-fpm --version 重启服务123systemctl restart nginxsystemctl restart mariadbsystemctl restart php-fpm","categories":[{"name":"Linux","slug":"Linux","permalink":"https://blog.zhimma.com/categories/Linux/"}],"tags":[{"name":"CentOS","slug":"CentOS","permalink":"https://blog.zhimma.com/tags/CentOS/"},{"name":"LNMP","slug":"LNMP","permalink":"https://blog.zhimma.com/tags/LNMP/"}]},{"title":"CentOS中PHP7的安装","slug":"CentOS中PHP7的安装","date":"2018-11-29T16:00:00.000Z","updated":"2019-01-24T09:10:06.454Z","comments":true,"path":"2018/11/30/CentOS中PHP7的安装/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/CentOS中PHP7的安装/","excerpt":"本文安装参考这里，并且全程墙外网; 查看Linux版本cat /etc/centos-release CentOS Linux release 7.3.1611 (Core)","text":"本文安装参考这里，并且全程墙外网; 查看Linux版本cat /etc/centos-release CentOS Linux release 7.3.1611 (Core) 删除之前的 PHP 版本yum remove php* php-common //如果存在其他版本删除原来的版本 rpm 安装 Php7 相应的 yum源rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm rpm -Uvh https://mirror.webtatic.com/yum/el7/webtatic-release.rpm yum安装PHP7yum install php70w-fpm php70w-opcache 查看PHP版本php -v / php-fpm -v 附录 Package Provides Package Provides php70w mod_php, php70w-zts php70w-bcmath php70w-cli php-cgi, php-pcntl, php-readline php70w-common php-api, php-bz2, php-calendar, php-ctype, php-curl, php-date, php-exif, php-fileinfo,php-filter,php-ftp,php-gettext,php-gmp, php-hash, php-iconv, php-json, php-libxml, php-openssl,php-pcre,php-spl,php-tokenizer, php-zend-abi, php-zip, php-zlib php70w-dba php70w-devel php70w-embedded php-embedded-devel php70w-enchant php70w-fpm php70w-gd php70w-imap php70w-interbase php_database, php-firebird php70w-intl php70w-ldap php70w-mbstring php70w-mcrypt php70w-mysql php-mysqli, php_database php70w-mysqlnd php-mysqli, php_database php70w-odbc php-pdo_odbc, php_database php70w-opcache php70w-pecl-zendopcache php70w-pdo php70w-pdo_sqlite, php70w-sqlite3 php70w-pdo_dblib php70w-mssql php70w-pear php70w-pecl-apcu php70w-pecl-imagick php70w-pecl-redis php70w-pecl-xdebug php70w-pgsql php-pdo_pgsql, php_database php70w-phpdbg php70w-process php-posix, php-sysvmsg, php-sysvsem, php-sysvshm php70w-pspell php70w-recode php70w-snmp php70w-soap php70w-tidy php70w-xml php-dom, php-domxml, php-wddx, php-xsl php70w-xmlrpc","categories":[{"name":"Linux","slug":"Linux","permalink":"https://blog.zhimma.com/categories/Linux/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://blog.zhimma.com/tags/PHP/"}]},{"title":"AJAX解决跨域问题（Access-Control-Allow-Origin）","slug":"AJAX解决跨域问题（Access-Control-Allow-Origin）","date":"2018-11-29T16:00:00.000Z","updated":"2019-02-01T07:34:14.703Z","comments":true,"path":"2018/11/30/AJAX解决跨域问题（Access-Control-Allow-Origin）/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/AJAX解决跨域问题（Access-Control-Allow-Origin）/","excerpt":"之前遇到过跨域的问题,一直觉得很神秘,也没有多关注,就过去了,今天又看到几篇文章说跨域,闲来无事于是将其整理记录下来； 一些概念先来阐述下几个概念: 跨域:是指浏览器对于JavaScript的同源策略限制,只要协议、域名、端口有任何一个不同,都被当作是不同的域,都不能执行或获取其他网站的资源； 姑且这么定义吧,举个简单例子,就是www.client.com网站上的程序不能从www.server.com网站上获取数据,如果强行获取,则会报出下面错误","text":"之前遇到过跨域的问题,一直觉得很神秘,也没有多关注,就过去了,今天又看到几篇文章说跨域,闲来无事于是将其整理记录下来； 一些概念先来阐述下几个概念: 跨域:是指浏览器对于JavaScript的同源策略限制,只要协议、域名、端口有任何一个不同,都被当作是不同的域,都不能执行或获取其他网站的资源； 姑且这么定义吧,举个简单例子,就是www.client.com网站上的程序不能从www.server.com网站上获取数据,如果强行获取,则会报出下面错误 有没有跨域,判断是不是属于跨域,可以参考下面: URL 说明 是否允许通信 http://www.a.com/a.js 调用 http://www.a.com/b.js 同一域名下 允许 http://www.a.com/lab/a.js 调用 http://www.a.com/script/b.js 同一域名下不同文件夹 允许 http://www.a.com:8000/a.js 调用 http://www.a.com/b.js 同一域名,不同端口 不允许 http://www.a.com/a.js 调用 https://www.a.com/b.js 同一域名,不同协议 不允许 http://www.a.com/a.js 调用 http://70.32.92.74/b.js 域名和域名对应ip 不允许 http://www.a.com/a.js 调用 http://script.a.com/b.js 主域相同,子域不同 不允许 http://www.a.com/a.js 调用 http://a.com/b.js 同一域名,不同二级域名（同上） 不允许（cookie这种情况下也不允许访问） http://www.cnblogs.com/a.js 调用 http://www.a.com/b.js 不同域名 不允许 CORS:CORS（Cross-Origin Resource Sharing）跨域资源共享,定义了必须在访问跨域资源时,浏览器与服务器应该如何沟通.CORS背后的基本思想就是使用自定义的HTTP头部让浏览器与服务器进行沟通,从而决定请求或响应是应该成功还是失败. 服务器端对于CORS的支持,主要就是通过设置Access-Control-Allow-Origin来进行的.如果浏览器检测到相应的设置,就可以允许Ajax进行跨域的访问. 解决方法Solution 1:服务端程序解决如果是双方预定沟通好请求允许数据,可以在服务端添加header头来解决 123header( &quot;Access-Control-Allow-Origin:*&quot; );header( &quot;Access-Control-Allow-Methods:POST,GET&quot; ); 看下面的例子: 客户端 www.client.com/cliend.html 123456789101112131415161718192021222324252627&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8&quot;&gt; &lt;title&gt; 跨域测试 &lt;/title&gt; &lt;script src=&quot;//code.jquery.com/jquery-1.11.3.min.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;button style=&quot;width:100px&quot;&gt;click client&lt;/button&gt;&lt;script type=&quot;text/javascript&quot;&gt; $(&quot;button&quot;).click(function () &#123; $.ajax(&#123; url: &quot;http://www.server.com/server.php&quot;, type: &quot;post&quot;, data: &#123;&apos;text&apos;: &apos;hello world&apos;&#125;, success: function (msg) &#123; $(&quot;button&quot;).html(msg); &#125; &#125;); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 服务器端 www.server.com/server.php 12345678910111213141516171819202122232425262728//允许所有域名获取数据&lt;?php$text = $_POST[&apos;text&apos;];//允许所有的域名header(&apos;content-type:application:json;charset=utf8&apos;);header(&apos;Access-Control-Allow-Origin:*&apos;); header(&apos;Access-Control-Allow-Methods:POST,GET&apos;); header(&apos;Access-Control-Allow-Headers:x-requested-with,content-type&apos;);echo json_encode($text);?&gt;//允许制定域名获取数据&lt;?php$text = $_POST[&apos;text&apos;];header(&apos;content-type:application:json;charset=utf8&apos;);$origin = isset($_SERVER[&apos;HTTP_ORIGIN&apos;]) ? $_SERVER[&apos;HTTP_ORIGIN&apos;] : &apos;&apos;;//允许指定域名$allow_origin = [ &apos;http://www.client.com&apos;, &apos;http://www.client2.com&apos;];if (in_array($origin, $allow_origin)) &#123; header(&apos;Access-Control-Allow-Origin:&apos; . $origin); header(&apos;Access-Control-Allow-Methods:POST,GET&apos;); header(&apos;Access-Control-Allow-Headers:x-requested-with,content-type&apos;);&#125;echo json_encode($text);?&gt; 这样,理论上就可以解决跨域问题: Solution 2:代理模式解决思路:例如 www.client.com/client.html 需要调用 www.server.com/server.php ,可以写一个接口 www.client.com/server.php ,由这个接口在后端去调用 www.server.com/server.php 并拿到返回值,然后再返回给index.html,这就是一个代理的模式.相当于绕过了浏览器端,自然就不存在跨域问题. Solution 3:使用JSONP使用之前,建议去看下我的另一篇文章Json和JsonP,然后再过来实践； 还是直接上代码: 客户端 www.client.com/client.html 12345678910111213141516171819202122232425262728293031&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8&quot;&gt; &lt;title&gt; 跨域测试 &lt;/title&gt; &lt;script src=&quot;//code.jquery.com/jquery-1.11.3.min.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;button id=&quot;clickMe&quot; style=&quot;width:100px&quot;&gt;click get jsonP&lt;/button&gt;&lt;script type=&quot;text/javascript&quot;&gt; $(&quot;#clickMe&quot;).click(function () &#123; $.ajax(&#123; url: &quot;http://www.server.com/jsonP.json&quot;, type: &quot;post&quot;, dataType: &quot;jsonP&quot;, data: &#123;&apos;text&apos;: &apos;hello world&apos;&#125;, jsonpCallback: &apos;returnData&apos;, //可自定义 函数名 success: function (msg) &#123; alert(msg.text); &#125;, error: function (XMLHttpRequest, textStatus, errorThrown) &#123; alert(XMLHttpRequest.status); alert(XMLHttpRequest.readyState); alert(textStatus); &#125; &#125;); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 服务器端 www.server.com/jsonP.json 1returnData(&#123;&quot;text&quot;:&quot;hello jsonP&quot;&#125;); 同样的也可以跨域获取数据 Solution 4:使用html5 API postMessage(转自这里)客户端 www.client.com/client.html 1234567891011121314151617181920&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Document&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;iframe style=&quot;display: none&quot; src=&quot;http://www.server.com/server.html&quot; name=&quot;postIframe&quot; onload=&quot;messageLoad()&quot;&gt;&lt;/iframe&gt;&lt;script&gt; function messageLoad() &#123; var url = &quot;http://www.server.com&quot;; window.postIframe.postMessage(&quot;给我tsort的信息&quot;, url); //发送数据 &#125; window.onmessage = function (e) &#123; e = e || event; console.log(e.data); //接收b返回的数据,在控制台有两次输出 &#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 服务器端 www.server.com/server.html 12345678910111213141516171819202122&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Document&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;script&gt; window.onmessage = function(e)&#123; e = e || event; alert(e.data); //立即弹出a发送过来的数据 e.source.postMessage(&quot;好的,请稍等三秒！&quot;,e.origin); //立即回复a var postData = &#123;name:&quot;tsrot&quot;,age:24&#125;; var strData = JSON.stringify(postData); //json对象转化为字符串 setTimeout(function()&#123; e.source.postMessage(strData,e.origin); &#125;,3000); //3秒后向a发送数据 &#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;","categories":[{"name":"JavaScript","slug":"JavaScript","permalink":"https://blog.zhimma.com/categories/JavaScript/"}],"tags":[{"name":"Cors","slug":"Cors","permalink":"https://blog.zhimma.com/tags/Cors/"}]},{"title":"Centos7中，PHP7下，扩展的安装","slug":"Centos7中，PHP7下，扩展的安装","date":"2018-11-29T16:00:00.000Z","updated":"2019-02-01T07:34:13.763Z","comments":true,"path":"2018/11/30/Centos7中，PHP7下，扩展的安装/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/Centos7中，PHP7下，扩展的安装/","excerpt":"CentOS 下，PHP有多种方式来安装拓展， 主要有 包管理式的 yum 安装、pecl 安装， 以及源码编译安装。包管理式的安装卸载尤为方便，而源码编译式的安装则方便参数调优。一般搭建本机开发环境推荐包管理式的安装，节约时间。而线上部署环境则推荐编译安装， 方便调优。 环境和准备 环境 ： windows10 PHP版本：7.0.13 Nginx: 1.10.3 Centos: docker 创建的CentOS容器[CentOS Linux release 7.3.1611 (Core) ] 全程墙外网方式1： yum 安装扩展（mongodb）","text":"CentOS 下，PHP有多种方式来安装拓展， 主要有 包管理式的 yum 安装、pecl 安装， 以及源码编译安装。包管理式的安装卸载尤为方便，而源码编译式的安装则方便参数调优。一般搭建本机开发环境推荐包管理式的安装，节约时间。而线上部署环境则推荐编译安装， 方便调优。 环境和准备 环境 ： windows10 PHP版本：7.0.13 Nginx: 1.10.3 Centos: docker 创建的CentOS容器[CentOS Linux release 7.3.1611 (Core) ] 全程墙外网方式1： yum 安装扩展（mongodb） yum方式安装能自动安装扩展的.so动态库，并配置好php.ini yum search mongodb|grep php //搜索到拓展名为 php70w-pecl-mongodb 等待自动安装完成后，查看phpinfo 到此 yum 安装扩展方法介绍完毕 pecl 安装（redis） pecl 安装需要准备2个文件：phpize ，php-config 查找phpize的位置12[root@9e2c60482bdc conf.d]# whereis phpizephpize: /usr/bin/phpize 查找php-config的位置12[root@9e2c60482bdc conf.d]# whereis php-configphp-config:[root@9e2c60482bdc conf.d]# 发现没有php-config，先测试运行下phpize 123php-config:[root@9e2c60482bdc conf.d]# phpizeCan t find PHP headers in /usr/include/phpThe php-devel package is required for use of this command. 直接报错，接下来安装php-devel解决上面报错yum install php70w-devel​再次运行phpize,没有报找不到的错误，出现了下面的报错：​ 123[root@9e2c60482bdc bin]# phpizeCannot find config.m4. Make sure that you run /usr/bin/phpize in the top level source directory of the module 先不管他，进行下一步的操作 更新 pear（非必须）我们需要先从pear官网下载 go-pear 工具,这个工具将帮我们同时安装 pecl包管理器(管理php的C拓展) pear包管理器(管理php类库)；​wget http://pear.php.net/go-pear.phar​下载完成后安装工具，运行下面命令：php go-pear.phar， 然后默认回车即可，暂时没搞懂这些参数的意思，估计是配置路经相关吧，先回车再说； 安装扩展（1）搜索扩展包：pecl search redis ​ 123456789101112131415161718192021222324252627[root@9e2c60482bdc test]# pecl search redisWarning: Invalid argument supplied for foreach() in Command.php on line 249Warning: Invalid argument supplied for foreach() in /usr/share/pear/PEAR/Command.php on line 249Warning: Invalid argument supplied for foreach() in Command.php on line 249Warning: Invalid argument supplied for foreach() in /usr/share/pear/PEAR/Command.php on line 249Warning: Invalid argument supplied for foreach() in Command.php on line 249Warning: Invalid argument supplied for foreach() in /usr/share/pear/PEAR/Command.php on line 249Warning: Invalid argument supplied for foreach() in Command.php on line 249Warning: Invalid argument supplied for foreach() in /usr/share/pear/PEAR/Command.php on line 249Warning: Invalid argument supplied for foreach() in Command.php on line 249...Warning: Invalid argument supplied for foreach() in PEAR/Command.php on line 249Warning: Invalid argument supplied for foreach() in /usr/share/pear/PEAR/Command.php on line 249XML Extension not found ​刷刷的报错了，查询错误后贴出解决方法： 解决方法 1vi /usr/bin/pecl //文件最后一行去掉 -n 参数 再次搜索， 123456[root@9e2c60482bdc test]# pecl search redisRetrieving data...0%Matched packages, channel pecl.php.net:=======================================Package Stable/(Latest) Localredis 3.1.2 (stable) PHP extension for interfacing with Redis 安装扩展包pecl install mongodb 123456789101112131415161718192021[root@9e2c60482bdc test]# pecl install redisdownloading redis-3.1.2.tgz ...Starting to download redis-3.1.2.tgz (199,041 bytes).................done: 199,041 bytes20 source files, buildingrunning: phpizeConfiguring for:PHP Api Version: 20151012Zend Module Api No: 20151012Zend Extension Api No: 320151012building in /tmp/pear/install/pear-build-rootLPns3m/redis-3.1.2running: /tmp/pear/install/redis/configure --with-php-config=/usr/bin/php-configchecking for grep that handles long lines and -e... /usr/bin/grepchecking for egrep... /usr/bin/grep -Echecking for a sed that does not truncate output... /usr/bin/sedchecking for cc... nochecking for gcc... noconfigure: error: in /tmp/pear/install/pear-build-rootLPns3m/redis-3.1.2:configure: error: no acceptable C compiler found in $PATHSee &apos;config.log&apos; for more detailsERROR: &apos;/tmp/pear/install/redis/configure --with-php-config=/usr/bin/php-config&apos; failed 刷刷的报错了，查询错误后贴出解决方法:yun install gcc gcc+ 安装扩展（2）执行：pecl install monodb 等待自动安装完成后，显示如下 12345Build process completed successfullyInstalling &apos;/usr/lib64/php/modules/redis.so&apos;install ok: channel://pecl.php.net/redis-3.1.2configuration option &quot;php_ini&quot; is not set to php.ini locationYou should add &quot;extension=redis.so&quot; to php.ini ​然后在php配置文件中稍作修改，将extension=redis.so添加到php.ini中,重启PHP,查看phpinfo 到此 pecl 安装扩展方法介绍完毕 源码编译安装（Seaslog） Seaslog文档 安装git先安装git，克隆Seaslog源码 123456[root@9e2c60482bdc test]# git clone https://github.com/Neeke/SeasLog.gitCloning into &apos;SeasLog&apos;...remote: Counting objects: 1094, done.remote: Total 1094 (delta 0), reused 0 (delta 0), pack-reused 1094Receiving objects: 100% (1094/1094), 1.04 MiB | 24.00 KiB/s, done.Resolving deltas: 100% (628/628), done. 安装Seaslog进入Seaslog目录，执行 123$ /path/to/phpize //更换自己对应的目录$ ./configure --with-php-config=/path/to/php-config$ make &amp;&amp; make install 完成之后，修改php配置文件，将extension = seaslog.so添加到php.ini中,重启PHP,查看phpinfo 总结至此Linux中,给PHP安装扩展的3种方式记录完毕,特此总结 参考1 参考2 peal官网","categories":[{"name":"Linux","slug":"Linux","permalink":"https://blog.zhimma.com/categories/Linux/"}],"tags":[{"name":"CentOS","slug":"CentOS","permalink":"https://blog.zhimma.com/tags/CentOS/"},{"name":"PHP","slug":"PHP","permalink":"https://blog.zhimma.com/tags/PHP/"}]},{"title":"Git","slug":"Git","date":"2018-11-29T16:00:00.000Z","updated":"2019-01-24T09:11:10.422Z","comments":true,"path":"2018/11/30/Git/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/Git/","excerpt":"","text":"集中式vs分布式集中式版本控制系统，版本库是集中存放在中央服务器的，而干活的时候，用的都是自己的电脑，所以要先从中央服务器取得最新的版本，然后开始干活，干完活了，再把自己的活推送给中央服务器。中央服务器就好比是一个图书馆，你要改一本书，必须先从图书馆借出来，然后回到家自己改，改完了，再放回图书馆。 集中式版本控制系统最大的毛病就是必须联网才能工作 分布式版本控制系统根本没有“中央服务器”，每个人的电脑上都是一个完整的版本库，这样，你工作的时候，就不需要联网了，因为版本库就在你自己的电脑上。既然每个人电脑上都有一个完整的版本库，那多个人如何协作呢？比方说你在自己电脑上改了文件A，你的同事也在他的电脑上改了文件A，这时，你们俩之间只需把各自的修改推送给对方，就可以互相看到对方的修改了。 安装后的配置12345678$ git config --global user.name &quot;Your Name&quot;$ git config --global user.email &quot;email@example.com&quot;$ git config --global color.ui true //Git会适当地显示不同的颜色$ git config --global alias.st status$ git config --global alias.co checkout$ git config --global alias.ci commit$ git config --global alias.br branch&amp; git config --global alias.last &apos;log -1&apos; Git是分布式版本控制系统，所以，每个机器都必须自报家门：你的名字和Email地址。git config命令的--global参数，用了这个参数，表示你这台机器上所有的Git仓库都会使用这个配置，当然也可以对某个仓库指定不同的用户名和Email地址。 文件的增删改查仓库管理运行git status命令看看仓库状态git diff查看文件修改记录 文件跟踪git log命令显示从最近到最远的提交日志 如果嫌输出信息太多，看得眼花缭乱的，可以试试加上--pretty=oneline参数Git提供了一个命令git reflog用来记录你的每一次命令 版本管理假如提交历史记录为下面所示1234$ git log --pretty=oneline3628164fb26d48395383f8f31179f24e0882e1e0 latest commitea34578d5496d7dd233c827ed32a8cd576c5ee85 second commitcb926e7ea50ad11b8f9e909c05226233bf755030 first commit 在Git中，用HEAD表示当前版本，也就是最新的提交3628164...882e1e0，上一个版本就是HEAD^，上上一个版本就是HEAD^^，当然往上100个版本写100个^比较容易数不过来，所以写成HEAD~100。Git允许我们在版本的历史之间穿梭，使用命令git reset --hard commit_id 我们要把当前版本“latest commit”回退到上一个版本“second commit”，就可以使用git reset命令：12$ git reset --hard HEAD^HEAD is now at ea34578 second commit 工作区和暂存区工作区（Working Directory）：就是你在电脑里能看到的目录版本库（Repository）：工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。 版本库又名仓库，英文名repository，你可以简单理解成一个目录，这个目录里面的所有文件都可以被Git管理起来，每个文件的修改、删除，Git都能跟踪，以便任何时刻都可以追踪历史，或者在将来某个时刻可以“还原”。初始化一个Git仓库，使用git init命令。添加文件到Git仓库，分两步： 第一步，使用命令git add &lt;file&gt;，注意，可反复多次使用，添加多个文件； 第二步，使用命令git commit，完成。 Git的版本库里存了很多东西，其中最重要的就是称为stage（或者叫index）的暂存区，还有Git为我们自动创建的第一个分支master，以及指向master的一个指针叫HEAD。 git add命令实际上就是把要提交的所有修改放到暂存区（Stage），然后，执行git commit就可以一次性把暂存区的所有修改提交到分支。一旦提交后，如果你又没有对工作区做任何修改，那么工作区就是“干净”的 撤销修改当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令git checkout -- file当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令git reset HEAD file，就回到了场景1，第二步按场景1操作。已经提交了不合适的修改到版本库时，想要撤销本次提交，使用命令git reset --hard commit_id 删除文件rm删除文件后，Git知道你删除了文件，因此，工作区和版本库就不一致了，git status命令会立刻告诉你哪些文件被删除了现在你有两个选择，一是确实要从版本库中删除该文件，那就用命令git rm删掉，并且git commit，现在，文件就从版本库中被删除了。另一种情况是删错了，因为版本库里还有呢，所以可以很轻松地把误删的文件恢复到最新版本，使用git checkout -- test.txt，git checkout其实是用版本库里的版本替换工作区的版本，无论工作区是修改还是删除，都可以“一键还原”。 命令git rm用于删除一个文件。如果一个文件已经被提交到版本库，那么你永远不用担心误删，但是要小心，你只能恢复文件到最新版本，你会丢失最近一次提交后你修改的内容。 关联远程仓库要关联一个远程库，使用命令git remote add origin git@server-name:path/repo-name.git；关联后，使用命令git push -u origin master第一次推送master分支的所有内容；此后，每次本地提交后，只要有必要，就可以使用命令git push origin master推送最新修改； 分支管理分支就是科幻电影里面的平行宇宙，当你正在电脑前努力学习Git的时候，另一个你正在另一个平行宇宙里努力学习SVN。如果两个平行宇宙互不干扰，那对现在的你也没啥影响。不过，在某个时间点，两个平行宇宙合并了，结果，你既学会了Git又学会了SVN！ 创建合并删除分支每次提交，Git都把它们串成一条时间线，这条时间线就是一个分支。截止到目前，只有一条时间线，在Git里，这个分支叫主分支，即master分支。HEAD严格来说不是指向提交，而是指向master，master才是指向提交的，所以，HEAD指向的就是当前分支。 每次提交，master分支都会向前移动一步，这样，随着你不断提交，master分支的线也越来越长 当我们创建新的分支，例如dev时，Git新建了一个指针叫dev，指向master相同的提交，再把HEAD指向dev，就表示当前分支在dev上 从现在开始，对工作区的修改和提交就是针对dev分支了，比如新提交一次后，dev指针往前移动一步，而master指针不变 假如我们在dev上的工作完成了，就可以把dev合并到master上。Git怎么合并呢？最简单的方法，就是直接把master指向dev的当前提交，就完成了合并： 合并完分支后，甚至可以删除dev分支。删除dev分支就是把dev指针给删掉，删掉后，我们就剩下了一条master分支： 开始实战：我们创建dev分支，然后切换到dev分支：12$ git checkout -b devSwitched to a new branch &apos;dev&apos; git checkout命令加上-b参数表示创建并切换，相当于以下两条命令： 123$ git branch dev$ git checkout devSwitched to branch &apos;dev&apos; 然后，用git branch命令查看当前分支：123$ git branch* dev master git branch命令会列出所有分支，当前分支前面会标一个*号 然后，我们就可以在dev分支上正常提交;如果dev分支的工作完成，我们就可以切换回master分支： 12$ git checkout masterSwitched to branch &apos;master&apos; 切换回master分支后，再查看一个readme.txt文件，刚才添加的内容不见了！因为那个提交是在dev分支上，而master分支此刻的提交点并没有变： 现在，我们把dev分支的工作成果合并到master分支上：12345$ git merge devUpdating d17efd8..fec145aFast-forward readme.txt | 1 + 1 file changed, 1 insertion(+) 合并完成后，就可以放心地删除dev分支了： 12$ git branch -d devDeleted branch dev (was fec145a). 丢弃一个没有被合并过的分支，可以通过git branch -D &lt;name&gt;强行删除。 删除后，查看branch，就只剩下master分支了： 12$ git branch* master 总结： Git鼓励大量使用分支： 查看分支：git branch 创建分支：git branch &lt;name&gt; 切换分支：git checkout &lt;name&gt; 创建+切换分支：git checkout -b &lt;name&gt; 合并某分支到当前分支：git merge &lt;name&gt; 删除分支：git branch -d &lt;name&gt; 分支策略在实际开发中，我们应该按照几个基本原则进行分支管理： 首先，master分支应该是非常稳定的，也就是仅用来发布新版本，平时不能在上面干活； 那在哪干活呢？干活都在dev分支上，也就是说，dev分支是不稳定的，到某个时候，比如1.0版本发布时，再把dev分支合并到master上，在master分支发布1.0版本； 你和你的小伙伴们每个人都在dev分支上干活，每个人都有自己的分支，时不时地往dev分支上合并就可以了。 所以，团队合作的分支看起来就像这样： Bug分支-暂存文件当你接到一个修复一个代号101的bug的任务时，很自然地，你想创建一个分支issue-101来修复它，但是，等等，当前正在dev上进行的工作还没有提交，并不是你不想提交，而是工作只进行到一半，还没法提交，预计完成还需1天时间。但是，必须在两个小时内修复该bug，怎么办？ 幸好，Git还提供了一个stash功能，可以把当前工作现场“储藏”起来，等以后恢复现场后继续工作 123$ git stashSaved working directory and index state WIP on dev: 6224937 add mergeHEAD is now at 6224937 add merge 现在，用git status查看工作区，就是干净的（除非有没有被Git管理的文件），因此可以放心地创建分支来修复bug。等到bug修复完毕，继续在dev分支开发时，工作区是干净的，刚才的工作现场存到哪去了？用git stash list命令看看：12$ git stash liststash@&#123;0&#125;: WIP on dev: 6224937 add merge 一是用git stash apply恢复，但是恢复后，stash内容并不删除，你需要用git stash drop来删除； 另一种方式是用git stash pop，恢复的同时把stash内容也删了 多人协作多人协作的工作模式通常是这样： 首先，可以试图用git push origin branch-name推送自己的修改； 如果推送失败，则因为远程分支比你的本地更新，需要先用git pull试图合并； 如果合并有冲突，则解决冲突，并在本地提交； 没有冲突或者解决掉冲突后，再用git push origin branch-name推送就能成功！ 如果git pull提示“no tracking information”，则说明本地分支和远程分支的链接关系没有创建，用命令git branch --set-upstream branch-name origin/branch-name。 小结： 查看远程库信息，使用git remote -v； 本地新建的分支如果不推送到远程，对其他人就是不可见的； 从本地推送分支，使用git push origin branch-name，如果推送失败，先用git pull抓取远程的新提交； 在本地创建和远程分支对应的分支，使用git checkout -b branch-name origin/branch-name，本地和远程分支的名称最好一致； 建立本地分支和远程分支的关联，使用git branch --set-upstream branch-name origin/branch-name； 从远程抓取分支，使用git pull，如果有冲突，要先处理冲突。 标签管理发布一个版本时，我们通常先在版本库中打一个标签（tag），这样，就唯一确定了打标签时刻的版本。将来无论什么时候，取某个标签的版本，就是把那个打标签的时刻的历史版本取出来。所以，标签也是版本库的一个快照。 tag就是一个让人容易记住的有意义的名字，它跟某个commit绑在一起 创建标签在Git中打标签非常简单，首先，切换到需要打标签的分支上，然后，敲命令git tag &lt;name&gt;就可以打一个新标签1$ git tag v1.0 可以用命令git tag查看所有标签 12$ git tagv1.0 给之前的提交打标签1$ git tag tag_name commit_id 操作标签如果标签打错了，也可以删除：12$ git tag -d v0.1Deleted tag &apos;v0.1&apos; (was e078af9) 因为创建的标签都只存储在本地，不会自动推送到远程。所以，打错的标签可以在本地安全删除。 如果要推送某个标签到远程，使用命令git push origin &lt;tagname&gt;：1234$ git push origin v1.0Total 0 (delta 0), reused 0 (delta 0)To git@github.com:michaelliao/learngit.git * [new tag] v1.0 -&gt; v1.0 或者，一次性推送全部尚未推送到远程的本地标签： 1234567$ git push origin --tagsCounting objects: 1, done.Writing objects: 100% (1/1), 554 bytes, done.Total 1 (delta 0), reused 0 (delta 0)To git@github.com:michaelliao/learngit.git * [new tag] v0.2 -&gt; v0.2 * [new tag] v0.9 -&gt; v0.9 如果标签已经推送到远程，要删除远程标签就麻烦一点，先从本地删除 12$ git tag -d v0.9Deleted tag &apos;v0.9&apos; (was 6224937) 然后，从远程删除。删除命令也是push，但是格式如下： 123$ git push origin :refs/tags/v0.9To git@github.com:michaelliao/learngit.git - [deleted] v0.9 小结： 命令git push origin &lt;tagname&gt;可以推送一个本地标签； 命令git push origin --tags可以推送全部未推送过的本地标签； 命令git tag -d &lt;tagname&gt;可以删除一个本地标签； 命令git push origin :refs/tags/&lt;tagname&gt;可以删除一个远程标签。","categories":[{"name":"Git","slug":"Git","permalink":"https://blog.zhimma.com/categories/Git/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://blog.zhimma.com/tags/Git/"}]},{"title":"Git生成多个ssh key","slug":"Git生成多个ssh-key","date":"2018-11-29T16:00:00.000Z","updated":"2019-01-24T09:11:10.419Z","comments":true,"path":"2018/11/30/Git生成多个ssh-key/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/Git生成多个ssh-key/","excerpt":"当使用了多个不同的git版本控制系统，分别有不同账号时，如一个在github上面有项目，一个在coding或者开源中国上面的有项目时，如果2者的邮箱不同时，就会涉及一个问题，生成的ssh key 会相互覆盖，必然有一个无法使用； 下面记录下解决方法：","text":"当使用了多个不同的git版本控制系统，分别有不同账号时，如一个在github上面有项目，一个在coding或者开源中国上面的有项目时，如果2者的邮箱不同时，就会涉及一个问题，生成的ssh key 会相互覆盖，必然有一个无法使用； 下面记录下解决方法： 生成ssh-key 12//创建github的ssh keyssh-keygen -t rsa -C &quot;your_email@example.com&quot; -f /c/user/username/.ssh/github_rsa 123456789101112131415161718192021222324252627example：//我的秘钥保存路径C:\\Users\\MMA\\.ssh\\test,邮箱使用your_email@example.comssh-keygen -t rsa -C &quot;your_email@example.com&quot; -f /c/Users/MMA/.ssh/test/github_rsa //运行之后弹出$ ssh-keygen -t rsa -C &quot;your_email@example.com&quot; -f /c/User/MMA/.ssh/test/test/github_rsaGenerating public/private rsa key pair.Enter passphrase (empty for no passphrase):回车默认即可，出现下面提示，则创建成功Your identification has been saved in /c/Users/MMA/.ssh/test/github_rsa.Your public key has been saved in /c/Users/MMA/.ssh/test/github_rsa.pub.The key fingerprint is:SHA256:AjEAqCT5VeTZsdHpklyMIVQQWbBVtAufE/P/GzkEw9I your_email@example.comThe key&apos;s randomart image is:+---[RSA 2048]----+|oo..o+OBBBo. ||+. .+.*.+=.o ||+. .. +.+++. E ||. . . +o.*. o || . S.= . . || . . .. .|| .+ || .o|| .o|+----[SHA256]-----+ 查看本地目录，GitHub ssh key生成成功 123456789101112131415161718192021222324//创建coding的ssh keyssh-keygen -t rsa -C &quot;my_email@example.com&quot; -f /c/Users/MMA/.ssh/test/coding_rsa和创建github的相似$ ssh-keygen -t rsa -C &quot;my_email@example.com&quot; -f /c/Users/MMA/.ssh/test/coding_rsaGenerating public/private rsa key pair.Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /c/Users/MMA/.ssh/test/coding_rsa.Your public key has been saved in /c/Users/MMA/.ssh/test/coding_rsa.pub.The key fingerprint is:SHA256:aqcMT/i9ZgXB6aCHus1zdf5JQlrLBYfUnnIBNJ37WIQ my_email@example.comThe key&apos;s randomart image is:+---[RSA 2048]----+| . o=+ o || . +. oE . || o o .o..= || o . o .o= . || . . S.oo.+ || . . ..=oo. . || +o +.o++ . || . +B.+o .o . || o=oo. .o |+----[SHA256]-----+ 添加私钥到SSH agent中 12ssh-add /c/Users/MMA/.ssh/test/coding_rsassh-add/c/Users/MMA/.ssh/test/github_rsa 如果执行ssh-add时提示&quot;Could not open a connection to your authentication agent&quot;，可以现执行命令：ssh-agent bash 重新添加即可 123456789101112MMA@MMA-PC MINGW64 ~/Desktop$ ssh-add /c/Users/MMA/.ssh/test/github_rsaIdentity added: /c/Users/MMA/.ssh/test/github_rsa (/c/Users/MMA/.ssh/test/github_rsa)MMA@MMA-PC MINGW64 ~/Desktop$ ssh-add /c/Users/MMA/.ssh/test/coding_rsaIdentity added: /c/Users/MMA/.ssh/test/coding_rsa (/c/Users/MMA/.ssh/test/coding_rsa)MMA@MMA-PC MINGW64 ~/Desktop$ ssh-add -l2048 SHA256:aqcMT/i9ZgXB6aCHus1zdf5JQlrLBYfUnnIBNJ37WIQ /c/Users/MMA/.ssh/test/coding_rsa (RSA)2048 SHA256:AjEAqCT5VeTZsdHpklyMIVQQWbBVtAufE/P/GzkEw9I /c/Users/MMA/.ssh/test/github_rsa (RSA) // 可以通过 ssh-add -l 来确私钥列表 ssh-add -l // 可以通过 ssh-add -D 来清空私钥列表 ssh-add -D 修改config文件 在/c/Users/MMA/.ssh/test 目录下新建一个config文件 12345678# coding Host git.coding.net PreferredAuthentications publickey IdentityFile /c/Users/MMA/.ssh/test/coding_rsa# github Host github.com PreferredAuthentications publickey IdentityFile /c/Users/MMA/.ssh/test/github_rsa 添加公钥到git平台 [coding教程]https://coding.net/help/doc/git/ssh-key.html) [github教程]https://help.github.com/articles/adding-a-new-ssh-key-to-your-github-account)","categories":[{"name":"Git","slug":"Git","permalink":"https://blog.zhimma.com/categories/Git/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://blog.zhimma.com/tags/Git/"}]},{"title":"JSON和JSONP","slug":"JSON和JSONP","date":"2018-11-29T16:00:00.000Z","updated":"2019-01-24T09:11:55.623Z","comments":true,"path":"2018/11/30/JSON和JSONP/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/JSON和JSONP/","excerpt":"","text":"有一篇文章介绍了跨域问题的几种解决方法,其中有一种是使用jsonP方式解决,那么今天来详细说说Json和JsonP 前言说到AJAX就会不可避免的面临两个问题,第一个是AJAX以何种格式来交换数据?第二个是跨域的需求如何解决?,目前为止最被推崇或者说首选的方案还是用JSON来传数据,靠JSONP来跨域.而这就是本文将要讲述的内容 JSON和JSONP虽然只有一个字母的差别,但其实他们根本不是一回事儿: JSON是一种数据交换格式 JSONP是一种依靠开发人员的聪明才智创造出的一种非官方跨域数据交互协议. 我们拿最近比较火的谍战片来打个比方,JSON是地下党们用来书写和交换情报的“暗号”,而JSONP则是把用暗号书写的情报传递给自己同志时使用的接头方式.看到没?一个是描述信息的格式,一个是信息传递双方约定的方法. 什么是JSONJSON是一种基于文本的数据交换方式,或者叫做数据描述格式,你是否该选用他首先肯定要关注它所拥有的优点. JSON的优点: 基于纯文本,跨平台传递极其简单; Javascript原生支持,后台语言几乎全部支持; 轻量级数据格式,占用字符数量极少,特别适合互联网传递; 可读性较强,虽然比不上XML那么一目了然,但在合理的依次缩进之后还是很容易识别的; 容易编写和解析,当然前提是你要知道数据结构; JSON的格式或者叫规则JSON能够以非常简单的方式来描述数据结构,XML能做的它都能做,因此在跨平台方面两者完全不分伯仲. JSON只有两种数据类型描述符,大括号{}和方括号[],其余英文冒号:是映射符,英文逗号,是分隔符,英文双引号””是定义符. 大括号{}用来描述一组“不同类型的无序键值对集合”（每个键值对可以理解为OOP的属性描述）,方括号[]用来描述一组“相同类型的有序数据集合”（可对应OOP的数组）. 上述两种集合中若有多个子项,则通过英文逗号,进行分隔. 键值对以英文冒号:进行分隔,并且建议键名都加上英文双引号””,以便于不同语言的解析. JSON内部常用数据类型无非就是字符串、数字、布尔、日期、null 这么几个,字符串必须用双引号引起来,其余的都不用,日期类型比较特殊,这里就不展开讲述了,只是建议如果客户端没有按日期排序功能需求的话,那么把日期时间直接作为字符串传递就好,可以省去很多麻烦. JSON实例:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172// 描述一个人var person = &#123;&quot;Name&quot;: &quot;Bob&quot;,&quot;Age&quot;: 32,&quot;Company&quot;: &quot;IBM&quot;,&quot;Engineer&quot;: true&#125;// 获取这个人的信息var personAge = person.Age;// 描述几个人var members = [&#123;&quot;Name&quot;: &quot;Bob&quot;,&quot;Age&quot;: 32,&quot;Company&quot;: &quot;IBM&quot;,&quot;Engineer&quot;: true&#125;,&#123;&quot;Name&quot;: &quot;John&quot;,&quot;Age&quot;: 20,&quot;Company&quot;: &quot;Oracle&quot;,&quot;Engineer&quot;: false&#125;,&#123;&quot;Name&quot;: &quot;Henry&quot;,&quot;Age&quot;: 45,&quot;Company&quot;: &quot;Microsoft&quot;,&quot;Engineer&quot;: false&#125;]// 读取其中John的公司名称var johnsCompany = members[1].Company;// 描述一次会议var conference = &#123;&quot;Conference&quot;: &quot;Future Marketing&quot;,&quot;Date&quot;: &quot;2012-6-1&quot;,&quot;Address&quot;: &quot;Beijing&quot;,&quot;Members&quot;:[&#123;&quot;Name&quot;: &quot;Bob&quot;,&quot;Age&quot;: 32,&quot;Company&quot;: &quot;IBM&quot;,&quot;Engineer&quot;: true&#125;,&#123;&quot;Name&quot;: &quot;John&quot;,&quot;Age&quot;: 20,&quot;Company&quot;: &quot;Oracle&quot;,&quot;Engineer&quot;: false&#125;,&#123;&quot;Name&quot;: &quot;Henry&quot;,&quot;Age&quot;: 45,&quot;Company&quot;: &quot;Microsoft&quot;,&quot;Engineer&quot;: false&#125;]&#125;// 读取参会者Henry是否工程师var henryIsAnEngineer = conference.Members[2].Engineer; 什么是JSONPJSONP的产生 一个众所周知的问题,Ajax直接请求普通文件存在跨域无权限访问的问题,甭管你是静态页面、动态网页、web服务、WCF,只要是跨域请求,一律不准; 不过我们又发现,Web页面上调用js文件时则不受是否跨域的影响（不仅如此,我们还发现凡是拥有”src”这个属性的标签都拥有跨域的能力,比如、、）; 于是可以判断,当前阶段如果想通过纯web端（ActiveX控件、服务端代理、属于未来的HTML5之Websocket等方式不算）跨域访问数据就只有一种可能,那就是在远程服务器上设法把数据装进js格式的文件里,供客户端调用和进一步处理; 恰巧我们已经知道有一种叫做JSON的纯字符数据格式可以简洁的描述复杂数据,更妙的是JSON还被js原生支持,所以在客户端几乎可以随心所欲的处理这种格式的数据; 这样子解决方案就呼之欲出了,web客户端通过与调用脚本一模一样的方式,来调用跨域服务器上动态生成的js格式文件（一般以JSON为后缀）,显而易见,服务器之所以要动态生成JSON文件,目的就在于把客户端需要的数据装入进去. 客户端在对JSON文件调用成功之后,也就获得了自己所需的数据,剩下的就是按照自己需求进行处理和展现了,这种获取远程数据的方式看起来非常像AJAX,但其实并不一样. 为了便于客户端使用数据,逐渐形成了一种非正式传输协议,人们把它称作JSONP,该协议的一个要点就是允许用户传递一个callback参数给服务端,然后服务端返回数据时会将这个callback参数作为函数名来包裹住JSON数据,这样客户端就可以随意定制自己的函数来自动处理返回数据了. JSONP的客户端具体实现 我们知道,哪怕跨域js文件中的代码（当然指符合web脚本安全策略的）,web页面也是可以无条件执行的. 远程服务器remoteserver.com根目录下有个remote.js文件代码如下: 1alert(&apos;我是远程文件&apos;); 本地服务器localserver.com下有个jsonp.html页面代码如下: 12345678910&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.0 Transitional//EN&quot; &quot;http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd&quot;&gt;&lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&gt;&lt;head&gt; &lt;title&gt;&lt;/title&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;http://remoteserver.com/remote.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt; 结果是弹出一个alert框,则跨域调用成功！ 现在我们在jsonp.html页面定义一个函数,然后在远程remote.js中传入数据进行调用. jsonp.html页面代码如下: 123456789101112131415&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.0 Transitional//EN&quot; &quot;http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd&quot;&gt;&lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&gt;&lt;head&gt; &lt;title&gt;&lt;/title&gt; &lt;script type=&quot;text/javascript&quot;&gt; var localHandler = function(data)&#123; alert(&apos;我是本地函数,可以被跨域的remote.js文件调用,远程js带来的数据是:&apos; + data.result); &#125;; &lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;http://remoteserver.com/remote.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt; remote.js文件代码如下: 1localHandler(&#123;&quot;result&quot;:&quot;我是远程js带来的数据&quot;&#125;); 运行之后查看结果,页面成功弹出提示窗口,显示本地函数被跨域的远程js调用成功,并且还接收到了远程js带来的数据. 很欣喜,跨域远程获取数据的目的基本实现了,但是又一个问题出现了,我怎么让远程js知道它应该调用的本地函数叫什么名字呢?毕竟是jsonp的服务者都要面对很多服务对象,而这些服务对象各自的本地函数都不相同啊?我们接着往下看 聪明的开发者很容易想到,只要服务端提供的js脚本是动态生成的就行了呗,这样调用者可以传一个参数过去告诉服务端“我想要一段调用XXX函数的js代码,请你返回给我”,于是服务器就可以按照客户端的需求来生成js脚本并响应了. 12345678910111213141516171819202122&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.0 Transitional//EN&quot; &quot;http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd&quot;&gt;&lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&gt;&lt;head&gt; &lt;title&gt;&lt;/title&gt; &lt;script type=&quot;text/javascript&quot;&gt; // 得到航班信息查询结果后的回调函数 var flightHandler = function(data)&#123; alert(&apos;你查询的航班结果是:票价 &apos; + data.price + &apos; 元,&apos; + &apos;余票 &apos; + data.tickets + &apos; 张.&apos;); &#125;; // 提供jsonp服务的url地址（不管是什么类型的地址,最终生成的返回值都是一段javascript代码） var url = &quot;http://flightQuery.com/jsonp/flightResult.aspx?code=CA1998&amp;callback=flightHandler&quot;; // 创建script标签,设置其属性 var script = document.createElement(&apos;script&apos;); script.setAttribute(&apos;src&apos;, url); // 把script标签加入head,此时调用开始 document.getElementsByTagName(&apos;head&apos;)[0].appendChild(script); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt; 这次的代码变化比较大,不再直接把远程js文件写死,而是编码实现动态查询,而这也正是jsonp客户端实现的核心部分,本例中的重点也就在于如何完成jsonp调用的全过程. 我们看到调用的url中传递了一个code参数,告诉服务器我要查的是CA1998次航班的信息,而callback参数则告诉服务器,我的本地回调函数叫做flightHandler,所以请把查询结果传入这个函数中进行调用. OK,服务器很聪明,这个叫做flightResult.aspx的页面生成了一段这样的代码提供给jsonp.html（服务端的实现这里就不演示了,与你选用的语言无关,说到底就是拼接字符串）: 12345flightHandler(&#123;&quot;code&quot;: &quot;CA1998&quot;,&quot;price&quot;: 1780,&quot;tickets&quot;: 5&#125;); 我们看到,传递给flightHandler函数的是一个json,它描述了航班的基本信息.运行一下页面,成功弹出提示窗口,jsonp的执行全过程顺利完成！ 到这里为止的话,相信你已经能够理解jsonp的客户端实现原理了吧?剩下的就是如何把代码封装一下,以便于与用户界面交互,从而实现多次和重复调用. 想知道jQuery如何实现jsonp调用?请看下文（我们依然沿用上面那个航班信息查询的例子,假定返回jsonp结果不变）: 123456789101112131415161718192021222324252627&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.0 Transitional//EN&quot; &quot;http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd&quot;&gt;&lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot; &gt;&lt;head&gt; &lt;title&gt;Untitled Page&lt;/title&gt; &lt;script type=&quot;text/javascript&quot; src=jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; jQuery(document).ready(function()&#123; $.ajax(&#123; type: &quot;get&quot;, async: false, url: &quot;http://flightQuery.com/jsonp/flightResult.aspx?code=CA1998&quot;, dataType: &quot;jsonp&quot;, jsonp: &quot;callback&quot;,//传递给请求处理程序或页面的,用以获得jsonp回调函数名的参数名(一般默认为:callback) jsonpCallback:&quot;flightHandler&quot;,//自定义的jsonp回调函数名称,默认为jQuery自动生成的随机函数名,也可以写&quot;?&quot;,jQuery会自动为你处理数据 success: function(json)&#123; alert(&apos;您查询到航班信息:票价: &apos; + json.price + &apos; 元,余票: &apos; + json.tickets + &apos; 张.&apos;); &#125;, error: function()&#123; alert(&apos;fail&apos;); &#125; &#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt; 是不是有点奇怪? 为什么我这次没有写flightHandler这个函数呢? 而且竟然也运行成功了！哈哈,这就是jQuery的功劳了,jquery在处理jsonp类型的ajax时（还是忍不住吐槽,虽然jquery也把jsonp归入了ajax,但其实它们真的不是一回事儿）,自动帮你生成回调函数并把数据取出来供success属性方法来调用,是不是很爽呀? 作者补充 ajax和jsonp这两种技术在调用方式上“看起来”很像,目的也一样,都是请求一个url,然后把服务器返回的数据进行处理,因此jquery和ext等框架都把jsonp作为ajax的一种形式进行了封装; 但ajax和jsonp其实本质上是不同的东西.ajax的核心是通过XmlHttpRequest获取非本页内容,而jsonp的核心则是动态添加标签来调用服务器提供的js脚本. 所以说,其实ajax与jsonp的区别不在于是否跨域,ajax通过服务端代理一样可以实现跨域,jsonp本身也不排斥同域的数据的获取. 还有就是,jsonp是一种方式或者说非强制性协议,如同ajax一样,它也不一定非要用json格式来传递数据,如果你愿意,字符串都行,只不过这样不利于用jsonp提供公开服务. 总而言之,jsonp不是ajax的一个特例,哪怕jquery等巨头把jsonp封装进了ajax,也不能改变着一点 最后本文属于原创,我只是个搬运工,在此记录下,已备不时之需;","categories":[{"name":"Cors","slug":"Cors","permalink":"https://blog.zhimma.com/categories/Cors/"}],"tags":[{"name":"Cors","slug":"Cors","permalink":"https://blog.zhimma.com/tags/Cors/"}]},{"title":"Mac环境ELK搭建","slug":"Mac环境ELK搭建","date":"2018-11-29T16:00:00.000Z","updated":"2019-03-14T03:17:51.611Z","comments":true,"path":"2018/11/30/Mac环境ELK搭建/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/Mac环境ELK搭建/","excerpt":"","text":"安装Java略。。。 这里我使用brew install java命令安装 1234☁ ~ java -versionjava version &quot;11&quot; 2018-09-25Java(TM) SE Runtime Environment 18.9 (build 11+28)Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11+28, mixed mode) 不要使用上述方式安装，有坑 下载这个文件安装java:https://edelivery.oracle.com/otn-pub/java/jdk/8u181-b13/96a7b8442fe848ef90c96a2fad6ed6d1/jdk-8u181-macosx-x64.dmg Elasticsearch安装配置安装官网下载对应平台的安装包 放到合适的位置，我放置后解压的目录是/Users/zhimma/Soft/elasticsearch-6.4.1 配置配置 Elasticsearch下面是我的配置文件内容： 123456789☁ config pwd/Users/zhimma/Soft/elasticsearch-6.4.1/config☁ config grep &apos;^[a-z]&apos; elasticsearch.ymlcluster.name: elk-stackpath.data: /Users/zhimma/Data/elk_stack/datapath.logs: /Users/zhimma/Data/elk_stack/logsbootstrap.memory_lock: falsenetwork.host: 0.0.0.0http.port: 9200 配置 Elasticsearch 内存占用配置 jvm 最大堆和最小堆，一般为服务器物理内存的一半，最大不超过 32g 123456☁ config pwd/Users/zhimma/Soft/elasticsearch-6.4.1/config☁ config vi jvm.options -Xms8g-Xmx8g 启动进入bin目录启动Elasticsearch 123☁ bin pwd/Users/zhimma/Soft/elasticsearch-6.4.1/bin☁ bin ./elasticsearch kibana安装配置安装官网下载对应平台的安装包 放到合适的位置，我放置后解压的目录是/Users/zhimma/Soft/kibana-6.4.1-darwin-x86_64 配置凭感觉配置了一些，如下所示： 12345☁ config grep &apos;^[a-z]&apos; kibana.ymlserver.port: 5601server.host: &quot;0.0.0.0&quot;elasticsearch.url: &quot;http://0.0.0.0:9200&quot;kibana.index: &quot;.kibana&quot; 启动进入bin目录启动Kibana 123☁ bin pwd/Users/zhimma/Soft/kibana-6.4.1-darwin-x86_64/bin☁ bin ./kibana Logstash安装配置安装官网下载对应平台的安装包 放到合适的位置，我放置后解压的目录是/Users/zhimma/Soft/logstash-6.4.1 配置配置 Logstash不是很了解，暂时使用默认配置 配置 Logstash 内存占用配置 jvm 最大堆和最小堆，一般为服务器物理内存的一半，最大不超过 32g 123456☁ config pwd/Users/zhimma/Soft/logstash-6.4.1/config☁ config vi jvm.options -Xms8g-Xmx8g 添加项目或自定义配置文件Logstash收集日志时候，可以对日志进行一定的操作和过滤，这里需要自定义不同的配置文件来实现，针对我们目前的项目，我简单的创建了下面的配置文件 在/Users/zhimma/Soft/logstash-6.4.1/config目录下创建conf.d文件夹，这个文件夹下存放我们所有的自定义配置文件: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546input &#123; file &#123; path =&gt; [ &quot;/data/www/XX_project/trunk/Common/Runtime/Apps/Api/*.log&quot; ] start_position =&gt; &quot;beginning&quot; ignore_older =&gt; 0 sincedb_path =&gt; &quot;/dev/null&quot; type =&gt; &quot;Api&quot; codec =&gt; multiline &#123; pattern =&gt; &quot;^\\[&quot; negate =&gt; true what =&gt; &quot;previous&quot; &#125; &#125;, file &#123; path =&gt; [ &quot;/data/www/XX_project/trunk/Common/Runtime/Apps/SDK/*.log&quot; ] start_position =&gt; &quot;beginning&quot; ignore_older =&gt; 0 sincedb_path =&gt; &quot;/dev/null&quot; type =&gt; &quot;SDK&quot; codec =&gt; multiline &#123; pattern =&gt; &quot;^\\[&quot; negate =&gt; true what =&gt; &quot;previous&quot; &#125; &#125;&#125;filter &#123;&#125;output &#123; if [type] == &quot;Api&quot; &#123; elasticsearch &#123; hosts =&gt; [ &quot;127.0.0.1:9200&quot; ] index =&gt; &quot;api&quot; &#125; &#125;, if [type] == &quot;SDK&quot; &#123; elasticsearch &#123; hosts =&gt; [ &quot;127.0.0.1:9200&quot; ] index =&gt; &quot;sdk&quot; &#125; &#125; stdout &#123; codec =&gt; rubydebug &#125;&#125; 启动进入bin目录启动Llogstash 123☁ bin pwd/Users/zhimma/Soft/logstash-6.4.1/bin☁ bin ./logstash -f /Users/zhimma/Soft/logstash-6.4.1/config/conf.d/default.conf ## 访问浏览器访问0.0.0.0:5601即可","categories":[{"name":"Mac","slug":"Mac","permalink":"https://blog.zhimma.com/categories/Mac/"},{"name":"ELK","slug":"ELK","permalink":"https://blog.zhimma.com/categories/ELK/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"https://blog.zhimma.com/tags/Mac/"},{"name":"ELK","slug":"ELK","permalink":"https://blog.zhimma.com/tags/ELK/"}]},{"title":"Laravel开发前期准备","slug":"Laravel开发-工具类","date":"2018-11-29T16:00:00.000Z","updated":"2019-01-24T09:12:16.127Z","comments":true,"path":"2018/11/30/Laravel开发-工具类/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/Laravel开发-工具类/","excerpt":"环境 ： windows10 本地开发环境 ：SalamanderWnmp Phpstrom版本：Phpstrom 2017.1 Laravel版本：Laravel5.4","text":"环境 ： windows10 本地开发环境 ：SalamanderWnmp Phpstrom版本：Phpstrom 2017.1 Laravel版本：Laravel5.4 编辑器插件Laravel Plugin ctrl+alt+s打开PHPStrom设置页面，按如下操作 安装完成后，启用插件，如下图所示 然后再去写代码的时候就会提供controllers,views, routes, configuration, translations等的代码提示功能。 Laravel IDE Helper 有时候你会发现Route::之类的没有代码提示或者自动补全，这时候我们需要安装 Laravel IDE Helper 项目地址：GitHub 使用composer命令安装:require barryvdh/laravel-ide-helper```123456安装之后你需要把Laravel IDE Helper以服务的形式注册到应用中。修改**app/config/app.php**,添加```Barryvdh\\LaravelIdeHelper\\IdeHelperServiceProvider::class```, 到**providers**元素下。执行下面的命令 php artisan clear-compiled php artisan ide-helper:generate php artisan optimize ` [参考资料]：https://confluence.jetbrains.com/display/PhpStorm/Laravel+Development+using+PhpStorm","categories":[{"name":"Laravel","slug":"Laravel","permalink":"https://blog.zhimma.com/categories/Laravel/"}],"tags":[{"name":"Laravel","slug":"Laravel","permalink":"https://blog.zhimma.com/tags/Laravel/"}]},{"title":"Mac重启php-fpm失败","slug":"Mac重启php-fpm失败","date":"2018-11-29T16:00:00.000Z","updated":"2019-02-01T07:35:26.980Z","comments":true,"path":"2018/11/30/Mac重启php-fpm失败/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/Mac重启php-fpm失败/","excerpt":"","text":"Mac重启php-fpm失败关闭 killall -HUP php-fpm 重启，这里报错，提示找不到文件 1234☁ ~ php-fpm[27-May-2018 11:27:47] ERROR: failed to open configuration file &apos;/private/etc/php-fpm.conf&apos;: No such file or directory (2)[27-May-2018 11:27:47] ERROR: failed to load configuration file &apos;/private/etc/php-fpm.conf&apos;[27-May-2018 11:27:47] ERROR: FPM initialization failed 检查了下本机的php-fpm.conf放在 1/usr/local/etc/php/7.1/php-fpm.conf 于是修改配置 1php-fpm --fpm-config /usr/local/etc/php/7.1/php-fpm.conf 继续报错 1234☁ ~ php-fpm --fpm-config /usr/local/etc/php/7.1/php-fpm.conf[27-May-2018 11:28:47] ERROR: failed to open error_log (/usr/var/log/php-fpm.log): No such file or directory (2)[27-May-2018 11:28:47] ERROR: failed to post process the configuration[27-May-2018 11:28:47] ERROR: FPM initialization failed 错误信息显示：不能正确的打开”日志“文件，原因是默认在/usr/var目录下工作，可以修改配置文件指定正确的日志文件路径 1php-fpm --fpm-config /usr/local/etc/php/7.1/php-fpm.conf --prefix /usr/local/var 可以成功运行了，错误文件会放在 /usr/local/var/log/php-fpm.log [TOC] Mac 启动、停止、重启 Nginx,重启 php-fpmnginx 启动：sudo nginx 停止： -ef | grep nginxsudo kill -INT [进程号]12ps -ef | grep nginxsudo kill -INT [进程号] 重启：sudo nginx -s reload 重启之前一定要 执行nginx -t检查配置文件是否有问题 php-fpm 关闭 12ps -ef | grep php-fpmsudo killall php-fpm 重启 /usr/local/Cellar/php71/7.1.12_23/sbin/php71-fpm start","categories":[{"name":"Mac","slug":"Mac","permalink":"https://blog.zhimma.com/categories/Mac/"},{"name":"PHP","slug":"PHP","permalink":"https://blog.zhimma.com/categories/PHP/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"https://blog.zhimma.com/tags/Mac/"},{"name":"PHP","slug":"PHP","permalink":"https://blog.zhimma.com/tags/PHP/"}]},{"title":"MySQL中binlog文件","slug":"MySQL中Binlog记录","date":"2018-11-29T16:00:00.000Z","updated":"2019-01-24T09:14:17.495Z","comments":true,"path":"2018/11/30/MySQL中Binlog记录/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/MySQL中Binlog记录/","excerpt":"","text":"[DCOT] 概念MySQL Server 有四种类型的日志——Error Log、General Query Log、Binary Log 和 Slow Query Log。 Error Log：错误日志，记录 mysqld 的一些错误 General Query Log：一般查询日志，记录 mysqld 正在做的事情，比如客户端的连接和断开、来自客户端每条 Sql Statement 记录信息；如果你想准确知道客户端到底传了什么瞎 [哔哔] 玩意儿给服务端，这个日志就非常管用了，不过它非常影响性能。 Binlog：Mysql sever层维护的一种二进制日志,Binlog中包含了一些事件，这些事件描述了数据库的改动，如建表、数据改动等，也包括一些潜在改动，比如 DELETE FROM ran WHERE bing = luan，然而一条数据都没被删掉的这种情况。除非使用 Row-based logging，否则会包含所有改动数据的 SQL Statement。 Slow Query Log：慢查询日志，记录一些查询比较慢的 SQL 语句——这种日志非常常用，主要是给开发者调优用的。 用途Binlog的作用主要有： Replication（主从数据库）:在master端开启binary log后，log会记录所有数据库的改动，然后slave端获取这个Log文件内容就可以在slave端进行同样的操作。 备份（数据恢复 ）：在某个时间点a做了一次备份，然后利用binary log记录从这个时间点a后的所有数据库的改动，然后下一次还原的时候，利用时间点a的备份文件和这个binary log文件，就可以将数据还原。 我们执行SELECT等不设计数据变更的语句是不会记录Binlog的，而涉及到数据更新则会记录。要注意的是，对支持事务的引擎如InnoDB而言，必须要提交了事务才会记录Binlog。Binlog是在事务最终commit前写入的，binlog什么时候刷新到磁盘跟参数sync_binlog相关。如果设置为0，则表示MySQL不控制binlog的刷新，由文件系统去控制它缓存的刷新，而如果设置为不为0的值则表示每sync_binlog次事务，MySQL调用文件系统的刷新操作刷新binlog到磁盘中。设为1是最安全的，在系统故障时最多丢失一个事务的更新，但是会对性能有所影响，一般情况下会设置为100或者0，牺牲一定的一致性来获取更好的性能。 Binlog的用法常用命令1.开启Binlog 通过配置/etc/my.cnf配置文件的log-bin选项： 12[mysqld]log-bin=mysql-bin 修改后重启MySql服务； 可以使用SET SQL_LOG_BIN=0命令停止使用日志文件，然后可以通过SET SQL_LOG_BIN=1命令来启用。 2.查看Binlog文件 1234567891011121314151617181920212223242526272829303132333435363738394041mysql&gt; show binary logs; +------------------+-----------+| Log_name | File_size |+------------------+-----------+| mysql-bin.000001 | 1045 || mysql-bin.000002 | 201 || mysql-bin.000003 | 201 || mysql-bin.000004 | 201 || mysql-bin.000005 | 201 || mysql-bin.000006 | 201 || mysql-bin.000007 | 201 || mysql-bin.000008 | 201 || mysql-bin.000009 | 18949 || mysql-bin.000010 | 471 || mysql-bin.000011 | 1072 || mysql-bin.000012 | 177 || mysql-bin.000013 | 1171 || mysql-bin.000014 | 3548 |+------------------+-----------+14 rows in set (0.00 sec)mysql&gt; show master logs;+------------------+-----------+| Log_name | File_size |+------------------+-----------+| mysql-bin.000001 | 1045 || mysql-bin.000002 | 201 || mysql-bin.000003 | 201 || mysql-bin.000004 | 201 || mysql-bin.000005 | 201 || mysql-bin.000006 | 201 || mysql-bin.000007 | 201 || mysql-bin.000008 | 201 || mysql-bin.000009 | 18949 || mysql-bin.000010 | 471 || mysql-bin.000011 | 1072 || mysql-bin.000012 | 177 || mysql-bin.000013 | 1171 || mysql-bin.000014 | 3548 |+------------------+-----------+14 rows in set (0.00 sec) 3.查看Binlog记录时长,设置时长(过期自动删除) 123456789101112131415161718mysql&gt; show variables like &apos;expire_logs_days&apos;;+------------------+-------+| Variable_name | Value |+------------------+-------+| expire_logs_days | 7 |+------------------+-------+1 row in set (0.00 sec)mysql&gt; set global expire_logs_days=15;Query OK, 0 rows affected (0.00 sec)mysql&gt; show variables like &apos;expire_logs_days&apos;;+------------------+-------+| Variable_name | Value |+------------------+-------+| expire_logs_days | 15 |+------------------+-------+1 row in set (0.00 sec) 4.删除Binlog 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455mysql&gt; flush logs;Query OK, 0 rows affected (0.35 sec)mysql&gt; reset master;Query OK, 0 rows affected (0.35 sec)mysql&gt; show binary logs;+------------------+-----------+| Log_name | File_size |+------------------+-----------+| mysql-bin.000001 | 154 |+------------------+-----------+1 row in set (0.00 sec)mysql&gt; show master logs;+------------------+-----------+| Log_name | File_size |+------------------+-----------+| mysql-bin.000001 | 154 |+------------------+-----------+1 row in set (0.00 sec)查看Binlog文件目录[root@b04f945297ac mysql]# ls -alh. . .drwxr-x--- 1 mysql mysql 4.0K Apr 8 04:12 mysql-rw-r----- 1 mysql mysql 1.1K May 22 07:41 mysql-bin.000001-rw-r----- 1 mysql mysql 201 May 22 07:41 mysql-bin.000002-rw-r----- 1 mysql mysql 201 May 22 07:41 mysql-bin.000003-rw-r----- 1 mysql mysql 201 May 22 07:41 mysql-bin.000004-rw-r----- 1 mysql mysql 201 May 22 07:41 mysql-bin.000005-rw-r----- 1 mysql mysql 201 May 22 07:41 mysql-bin.000006-rw-r----- 1 mysql mysql 201 May 22 07:41 mysql-bin.000007-rw-r----- 1 mysql mysql 201 May 22 07:41 mysql-bin.000008-rw-r----- 1 mysql mysql 19K May 22 10:09 mysql-bin.000009-rw-r----- 1 mysql mysql 471 May 23 01:28 mysql-bin.000010-rw-r----- 1 mysql mysql 1.1K May 24 02:28 mysql-bin.000011-rw-r----- 1 mysql mysql 177 May 24 02:28 mysql-bin.000012-rw-r----- 1 mysql mysql 1.2K May 25 02:15 mysql-bin.000013-rw-r----- 1 mysql mysql 2.0K May 25 06:36 mysql-bin.000014-rw-r----- 1 mysql mysql 266 May 25 02:15 mysql-bin.indexsrwxrwxrwx 1 mysql mysql 0 May 25 02:15 mysql.sock-rw------- 1 mysql mysql 3 May 25 02:15 mysql.sock.lock. . .删除之后的目录[root@b04f945297ac mysql]# ls -alhtotal 313M. . .drwxr-x--- 1 mysql mysql 4.0K Apr 8 12:12 mysql-rw-r----- 1 mysql mysql 154 May 25 15:03 mysql-bin.000001-rw-r----- 1 mysql mysql 19 May 25 15:03 mysql-bin.indexsrwxrwxrwx 1 mysql mysql 0 May 25 10:15 mysql.sock-rw------- 1 mysql mysql 3 May 25 10:15 mysql.sock.lock. . . 5.自动清理Binlog日志 如果堆积的binlog非常多，不要轻易设置改参数，可以使用purge命令 部分老化binlog 12mysql&gt; purge master logs before &apos;2018-05-25 16:25:00&apos;;//删除指定日期前的日志索引中binlog日志文件mysql&gt; purge master logs to &apos;binlog.000001&apos;;//删除指定日志文件 Binlog日志格式binary log可以通过mysqlbinlog命令来将log的信息打印出来，binlog模式总共可分为以下三种： statement ,row,mixed Statement:基于SQL语句的复制（statement-based replication, SBR） 每一条会修改数据的sql都会记录在binlog中。 优点：不需要记录每一行的变化，减少了binlog日志量，节约了IO, 提高了性能。 缺点：由于记录的只是执行语句，为了这些语句能在slave上正确运行，因此还必须记录每条语句在执行的时候的一些相关信息，以保证所有语句能在slave得到和在master端执行的时候相同的结果。另外mysql的复制，像一些特定函数的功能，slave可与master上要保持一致会有很多相关问题。 Row:基于行的复制（row-based replication, RBR） 5.1.5版本的MySQL才开始支持row level的复制,它不记录sql语句上下文相关信息，仅保存哪条记录被修改。 优点： binlog中可以不记录执行的sql语句的上下文相关的信息，仅需要记录那一条记录被修改成什么了。所以row的日志内容会非常清楚的记录下每一行数据修改的细节。而且不会出现某些特定情况下的存储过程，或function，以及trigger的调用和触发无法被正确复制的问题. 缺点:所有的执行的语句当记录到日志中的时候，都将以每行记录的修改来记录，这样可能会产生大量的日志内容 新版本的MySQL中对row level模式也被做了优化，并不是所有的修改都会以row level来记录，像遇到表结构变更的时候就会以statement模式来记录，如果sql语句确实就是update或者delete等修改数据的语句，那么还是会记录所有行的变更。 Mixed:混合模式复制（mixed-based replication, MBR） 从5.1.8版本开始，MySQL提供了Mixed格式，实际上就是Statement与Row的结合。在Mixed模式下，一般的语句修改使用statment格式保存binlog，如一些函数，statement无法完成主从复制的操作，则采用row格式保存binlog，MySQL会根据执行的每一条具体的sql语句来区分对待记录的日志形式，也就是在Statement和Row之间选择一种。 简而言之 statement：基于 SQL 语句的模式，binlog 数据量小，但是某些语句和函数在复制过程可能导致数据不一致甚至出错； mixed：混合模式，根据语句来选用是 statement 还是 row 模式； row：基于行的模式，记录的是行的完整变化。安全，但 binlog 会比其他两种模式大很多； 查看当前Binlog日志格式 1234567mysql&gt; show variables like &apos;binlog_format&apos;;+---------------+-------+| Variable_name | Value |+---------------+-------+| binlog_format | ROW |+---------------+-------+1 row in set (0.00 sec) 设置Binlog日志格式 123456789101112修改my.cnflog-bin=mysql-bin #binlog_format=STATEMENT #binlog_format=ROW binlog_format=MIXED 或者在运行时设置：mysql&gt; SET SESSION binlog_format = &apos;STATEMENT&apos;; mysql&gt; SET SESSION binlog_format = &apos;ROW&apos;; mysql&gt; SET SESSION binlog_format = &apos;MIXED&apos;; mysql&gt; SET GLOBAL binlog_format = &apos;STATEMENT&apos;; mysql&gt; SET GLOBAL binlog_format = &apos;ROW&apos;; mysql&gt; SET GLOBAL binlog_format = &apos;MIXED&apos;; 查看Binlog日志文件内容1.查看某个具体binlog文件的内容 12345678910111213mysql&gt; show binlog events in &quot;mysql-bin.000001&quot;;+------------------+-----+----------------+-----------+-------------+---------------------------------------+| Log_name | Pos | Event_type | Server_id | End_log_pos | Info |+------------------+-----+----------------+-----------+-------------+---------------------------------------+| mysql-bin.000001 | 4 | Format_desc | 1 | 123 | Server ver: 5.7.21-log, Binlog ver: 4 || mysql-bin.000001 | 123 | Previous_gtids | 1 | 154 | || mysql-bin.000001 | 154 | Anonymous_Gtid | 1 | 219 | SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos; || mysql-bin.000001 | 219 | Query | 1 | 302 | BEGIN || mysql-bin.000001 | 302 | Table_map | 1 | 378 | table_id: 180 (tourism.user_tags) || mysql-bin.000001 | 378 | Write_rows | 1 | 445 | table_id: 180 flags: STMT_END_F || mysql-bin.000001 | 445 | Xid | 1 | 476 | COMMIT /* xid=71618 */ |+------------------+-----+----------------+-----------+-------------+---------------------------------------+7 rows in set (0.00 sec) 2.查看某个具体binlog文件的内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960[root@b04f945297ac mysql]# pwd/var/lib/mysql[root@b04f945297ac mysql]# mysqlbinlog mysql-bin.000001 mysqlbinlog: [ERROR] unknown variable &apos;default-character-set=utf8mb4&apos;如果报错，加上--no-defaults参数运行即可[root@b04f945297ac mysql]# mysqlbinlog --no-defaults mysql-bin.000001 /*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=1*/;/*!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE,COMPLETION_TYPE=0*/;DELIMITER /*!*/;# at 4#180525 16:02:24 server id 1 end_log_pos 123 CRC32 0x62d58cd0 Start: binlog v 4, server v 5.7.21-log created 180525 16:02:24 at startup# Warning: this binlog is either in use or was not closed properly.ROLLBACK/*!*/;BINLOG &apos;EMMHWw8BAAAAdwAAAHsAAAABAAQANS43LjIxLWxvZwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQwwdbEzgNAAgAEgAEBAQEEgAAXwAEGggAAAAICAgCAAAACgoKKioAEjQAAdCM1WI=&apos;/*!*/;# at 123#180525 16:02:24 server id 1 end_log_pos 154 CRC32 0x57b5fe8d Previous-GTIDs# [empty]# at 154#180525 16:02:48 server id 1 end_log_pos 219 CRC32 0xa3058217 Anonymous_GTID last_committed=0 sequence_number=1 rbr_only=yes/*!50718 SET TRANSACTION ISOLATION LEVEL READ COMMITTED*//*!*/;SET @@SESSION.GTID_NEXT= &apos;ANONYMOUS&apos;/*!*/;# at 219#180525 16:02:48 server id 1 end_log_pos 302 CRC32 0xa849ecb0 Query thread_id=633 exec_time=0 error_code=0SET TIMESTAMP=1527235368/*!*/;SET @@session.pseudo_thread_id=633/*!*/;SET @@session.foreign_key_checks=1, @@session.sql_auto_is_null=0, @@session.unique_checks=1, @@session.autocommit=1/*!*/;SET @@session.sql_mode=1073741824/*!*/;SET @@session.auto_increment_increment=1, @@session.auto_increment_offset=1/*!*/;/*!\\C utf8mb4 *//*!*/;SET @@session.character_set_client=224,@@session.collation_connection=224,@@session.collation_server=8/*!*/;SET @@session.time_zone=&apos;SYSTEM&apos;/*!*/;SET @@session.lc_time_names=0/*!*/;SET @@session.collation_database=DEFAULT/*!*/;BEGIN/*!*/;# at 302#180525 16:02:48 server id 1 end_log_pos 378 CRC32 0x66324940 Table_map: `tourism`.`user_tags` mapped to number 180# at 378#180525 16:02:48 server id 1 end_log_pos 445 CRC32 0x25589cf0 Write_rows: table id 180 flags: STMT_END_FBINLOG &apos;KMMHWxMBAAAATAAAAHoBAAAAALQAAAAAAAEAB3RvdXJpc20ACXVzZXJfdGFncwALA/wPAQEPEREREREKAlAA/AMAAAAAAMAHQEkyZg==KMMHWx4BAAAAQwAAAL0BAAAAALQAAAAAAAEAAgAL///A/AMAAAAAAANjY2MBAAcAczowOiIiO1sIM6hbCDOo8JxYJQ==&apos;/*!*/;# at 445#180525 16:02:48 server id 1 end_log_pos 476 CRC32 0xd1963b7d Xid = 71618COMMIT/*!*/;SET @@SESSION.GTID_NEXT= &apos;AUTOMATIC&apos; /* added by mysqlbinlog */ /*!*/;DELIMITER ;# End of log file/*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/;/*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=0*/; 这里Binlog 显示和选择存储的格式相关 执行下面语句，查看对应Binlog记录格式 INSERT INTOstaff(name) VALUES (&#39;zhimma&#39;) row格式对应格式：1234BINLOG &apos;Cb8PWxMBAAAAOgAAAF0BAAAAAGQAAAAAAAEABHRlc3QABXN0YWZmAAYDDw8DAxIF/QL9AgAAkkVdDg==Cb8PWx4BAAAAPwAAAJwBAAAAAGQAAAAAAAEAAgAG/8ADAAAABgB6aGltbWEAAAAAAAAAAAAAgAAA statement格式对应格式：1234use `test`/*!*/;SET TIMESTAMP=1527758668/*!*/;INSERT INTO `staff` (`name`) VALUES (&apos;zhimma&apos;)/*!*/; mixd格式对应格式：1234BINLOG &apos;S74PWw8BAAAAdwAAAHsAAAAAAAQANS43LjIxLWxvZwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABLvg9bEzgNAAgAEgAEBAQEEgAAXwAEGggAAAAICAgCAAAACgoKKioAEjQAAY4lTRU= 提取Binlog文件提取指定的binlog日志123[root@b04f945297ac mysql]# mysqlbinlog --no-defaults /var/lib/mysql/mysql-bin.000001 |grep INSERT SET INSERT_ID=4/*!*/;INSERT INTO `staff` (`name`) VALUES (&apos;zhimma&apos;) 提取指定position位置的binlog日志123[root@b04f945297ac mysql]# mysqlbinlog --no-defaults --start-position=&quot;154&quot; --stop-position=&quot;481&quot; /var/lib/mysql/mysql-bin.000001 | grep INSERTSET INSERT_ID=5/*!*/;INSERT INTO `staff` (`name`) VALUES (&apos;zhimma&apos;) 提取指定position位置的binlog日志并输出到压缩文件12345[root@b04f945297ac binlog]# mysqlbinlog --no-defaults --start-position=&quot;154&quot; --stop-position=&quot;481&quot; /var/lib/mysql/mysql-bin.000001 |gzip &gt; extra_01.sql.gz[root@b04f945297ac binlog]# lsextra_01.sql.gz建议加上过滤[root@b04f945297ac binlog]# mysqlbinlog --no-defaults --start-position=&quot;154&quot; --stop-position=&quot;481&quot; /var/lib/mysql/mysql-bin.000001 | grep INSERT | gzip &gt; extra_01.sql.gz 提取指定position位置的binlog日志导入数据库1[root@b04f945297ac binlog]# mysqlbinlog --no-defaults --start-position=&quot;154&quot; --stop-position=&quot;481&quot; /var/lib/mysql/mysql-bin.000001 | mysql -uroot -p 提取指定时间区间的binlog并输出到日志文件1[root@b04f945297ac binlog]# mysqlbinlog --no-defaults --start-datetime=&quot;2018-05-30 00:00:00&quot; --stop-datetime=&quot;2018-06-30 00:00:00&quot; /var/lib/mysql/mysql-bin.000001 --result-file=20180530_20180630.sql 提取指定位置的多个binlog日志文件1[root@b04f945297ac binlog]# mysqlbinlog --no-defaults --start-datetime=&quot;2018-05-30 00:00:00&quot; --stop-datetime=&quot;2018-06-30 00:00:00&quot; /var/lib/mysql/mysql-bin.000001 /var/lib/mysql/mysql-bin.000002 --result-file=20180530_20180630.sql 提取指定数据库binlog并转换字符集到UTF81[root@b04f945297ac binlog]# mysqlbinlog --no-defaults --database=test --set-charset=utf8 --start-datetime=&quot;2018-05-30 00:00:00&quot; --stop-datetime=&quot;2018-06-30 00:00:00&quot; /var/lib/mysql/mysql-bin.000001 /var/lib/mysql/mysql-bin.000002 &gt; test_20180530_20180630.sql 远程提取使用row格式的binlog日志并输出到本地文件1[root@b04f945297ac binlog]# mysqlbinlog --no-defaults -urobin -p -P3606 -h192.168.1.1 --read-from-remote-server -vv --database=test --set-charset=utf8 --start-datetime=&quot;2018-05-30 00:00:00&quot; --stop-datetime=&quot;2018-06-30 00:00:00&quot; /var/lib/mysql/mysql-bin.000001 &gt; remote_20180530_20180630.sql 有了sql语句，数据就好恢复了 binlog2sql点击查看文档 https://blog.csdn.net/shudaqi2010/article/details/54412895","categories":[{"name":"Mysql","slug":"Mysql","permalink":"https://blog.zhimma.com/categories/Mysql/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://blog.zhimma.com/tags/Mysql/"}]},{"title":"MySQL数据类型","slug":"MySQL数据类型","date":"2018-11-29T16:00:00.000Z","updated":"2019-01-24T09:14:50.105Z","comments":true,"path":"2018/11/30/MySQL数据类型/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/MySQL数据类型/","excerpt":"","text":"MySQL数据类型char和varcharvarchar数据类型的变化MySQL4.1以下的版本中，varchar数据类型的最大长度限制为255，其数据范围可以是0~255或1~255（根据不同版本数据库来定） MySQL5.0以上的版本中，varchar数据类型的长度支持到了65535，也就是说可以存放65532个字节的数据，起始位和结束位占去了3个字节 Mysql5根据编码不同,存储大小也不同，具体有以下规则： 存储限制 varchar 字段是将实际内容单独存储在聚簇索引之外，内容开头用1到2个字节表示实际长度（长度超过255时需要2个字节），因此最大长度不能超过65535。 编码长度限制 字符类型若为gbk，每个字符最多占2个字节，最大长度不能超过32766; 字符类型若为utf8，每个字符最多占3个字节，最大长度不能超过21845。 若定义的时候超过上述限制，则varchar字段会被强行转为text类型，并产生warning。 行长度限制 导致实际应用中varchar长度限制的是一个行定义的长度。 MySQL要求一个行的定义长度不能超过65535。若定义的表长度超过这个值，则提示 ERROR 1118 (42000): Row size too large. The maximum row size for the used table type, not counting BLOBs, is 65535. You have to change some columns to TEXT or BLOBs。 char(M), varchar(M)不同之处char(M)定义的列的长度为固定的，M取值可以为0～255之间，当保存char值时，在它们的右边填充空格以达到指定的长度。当检索到CHAR值时，尾部的空格被删除掉。在存储或检索过程中不进行大小写转换。char存储定长数据很方便，char字段上的索引效率级高，比如定义char(10)，那么不论你存储的数据是否达到了10个字节，都要占去10个字节的空间,不足的自动用空格填充。 varchar(M)定义的列的长度为可变长字符串，M取值可以为0~65535之间，(VARCHAR的最大有效长度由最大行大小和使用的字符集确定。整体最大长度是65,532字节）。varchar值保存时只保存需要的字符数，另加一个字节来记录长度(如果列声明的长度超过255，则使用两个字节)。varchar值保存时不进行填充。当值保存和检索时尾部的空格仍保留，符合标准SQL。varchar存储变长数据，但存储效率没有char高。如果一个字段可能的值是不固定长度的，我们只知道它不可能超过10个字符，把它定义为 VARCHAR(10)是最合算的。VARCHAR类型的实际长度是它的值的实际长度+1。为什么”+1”呢？这一个字节用于保存实际使用了多大的长度。从空间上考虑，用varchar合适；从效率上考虑，用char合适，关键是根据实际情况找到权衡点。 char和varchar最大的不同就是一个是固定长度，一个是可变长度。由于是可变长度，因此实际存储的时候是实际字符串再加上一个记录字符串长度的字节(如果超过255则需要两个字节)。如果分配给char或varchar列的值超过列的最大长度，则对值进行裁剪以使其适合。如果被裁掉的字符不是空格，则会产生一条警告。如果裁剪非空格字符，则会造成错误(而不是警告)并通过使用严格SQL模式禁用值的插入。 VARCHAR和TEXT、BlOB类型的区别VARCHAR，BLOB和TEXT类型是变长类型，对于其存储需求取决于列值的实际长度(在前面的表格中用L表示)，而不是取决于类型的最大可能尺寸。例如，一个VARCHAR(10)列能保存最大长度为10个字符的一个字符串，实际的存储需要是字符串的长度 ，加上1个字节以记录字符串的长度。对于字符串’abcd’，L是4而存储要求是5个字节。 BLOB和TEXT类型需要1，2，3或4个字节来记录列值的长度，这取决于类型的最大可能长度。VARCHAR需要定义大小，有65535字节的最大限制；TEXT则不需要。如果你把一个超过列类型最大长度的值赋给一个BLOB或TEXT列，值被截断以适合它。 一个BLOB是一个能保存可变数量的数据的二进制的大对象。4个BLOB类型TINYBLOB、BLOB、MEDIUMBLOB和LONGBLOB仅仅在他们能保存值的最大长度方面有所不同。 BLOB 可以储存图片,TEXT不行，TEXT只能储存纯文本文件。4个TEXT类型TINYTEXT、TEXT、MEDIUMTEXT和LONGTEXT对应于4个BLOB类型，并且有同样的最大长度和存储需求。在BLOB和TEXT类型之间的唯一差别是对BLOB值的排序和比较以大小写敏感方式执行，而对TEXT值是大小写不敏感的。换句话说，一个TEXT是一个大小写不敏感的BLOB 总结char，varchar，text区别长度的区别，char范围是0～255，varchar最长是64k，但是注意这里的64k是整个row的长度，要考虑到其它的column，还有如果存在not null的时候也会占用一位，对不同的字符集，有效长度还不一样，比如utf8的，最多21845，还要除去别的column，但是varchar在一般情况下存储都够用了。如果遇到了大文本，考虑使用text，最大能到4G。 效率来说基本是char&gt;varchar&gt;text，但是如果使用的是Innodb引擎的话，推荐使用varchar代替char char和varchar可以有默认值，text不能指定默认值 数据库选择合适的数据类型存储还是很有必要的，对性能有一定影响。这里在零碎记录两笔，对于int类型的，如果不需要存取负值，最好加上unsigned；对于经常出现在where语句中的字段，考虑加索引，整形的尤其适合加索引。","categories":[{"name":"Mysql","slug":"Mysql","permalink":"https://blog.zhimma.com/categories/Mysql/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://blog.zhimma.com/tags/Mysql/"}]},{"title":"MySQL添加新用户1364","slug":"MySQL添加新用户1364","date":"2018-11-29T16:00:00.000Z","updated":"2019-01-24T09:15:05.339Z","comments":true,"path":"2018/11/30/MySQL添加新用户1364/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/MySQL添加新用户1364/","excerpt":"","text":"MySQL5.7添加新用户 出现ERROR 1364 (HY000): Field &#39;ssl_cipher&#39; doesn&#39;t have a default value ## 解决方法1：insert into user (host,user,authentication_string,select_priv,insert_priv,update_priv) values (&#39;%&#39; , &#39;mma&#39; ,PASSWORD(&#39;123456&#39;),&#39;Y&#39;,&#39;Y&#39;,&#39;Y&#39;); 原因： 在我的配置文件my.cnf中有这样一条语句sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES 指定了严格模式，为了安全，严格模式禁止通过insert 这种形式直接修改MySQL库中的user表进行添加新用户 解决方法: 将配置文件中的STRICT_TRANS_TABLES删掉，即改为： sql_mode=NO_ENGINE_SUBSTITUTION 然后重启mysql即可 解决方法2(推荐)：添加用户： grant usage on *.* to &#39;mma&#39;@&#39;%&#39; identified by &#39;123456&#39; with grant option; 赋予权限 grant all privileges on *.* to &#39;mma&#39;@&#39;%&#39; identified by &#39;123456&#39;; or grant select,insert,update,delete,create,drop ON TUTORIALS.* TO &#39;mma&#39;@&#39;%&#39; IDENTIFIED BY &#39;123456&#39;; 刷新权限 flush privileges;","categories":[{"name":"Mysql","slug":"Mysql","permalink":"https://blog.zhimma.com/categories/Mysql/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://blog.zhimma.com/tags/Mysql/"}]},{"title":"MyCat实践","slug":"Mycat实践","date":"2018-11-29T16:00:00.000Z","updated":"2019-02-01T07:35:38.606Z","comments":true,"path":"2018/11/30/Mycat实践/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/Mycat实践/","excerpt":"","text":"接着上一篇MySql主从配置完成后，下面实战下Mycat做读写分离和分库分表 参考： https://foofish.net/mysql-for-mycat.html https://www.cnblogs.com/joylee/p/7513038.html [TOC] 环境说明：windows 下4台服务器，真实机IP：192.168.2.107，其中 服务器 IP 说明 master 172.17.0.2 物理数据库，主库 mycat 172.17.0.3 mycat服务器，连接数据库时，连接此服务器 slave1 172.17.0.4 物理数据库，从库 slave2 172.17.0.5 物理数据库，从库 123456C:\\Users\\mma&gt;docker psCONTAINER ID PORTS NAMESb04f945297ac 0.0.0.0:1024-&gt;1024/tcp, 0.0.0.0:6379-&gt;6379/tcp, 0.0.0.0:9001-&gt;9001/tcp, 0.0.0.0:220-&gt;22/tcp, 0.0.0.0:8080-&gt;80/tcp, 0.0.0.0:33060-&gt;3306/tcp masterdcccd23b6177 0.0.0.0:8066-&gt;8066/tcp, 0.0.0.0:223-&gt;22/tcp, 0.0.0.0:8083-&gt;80/tcp, 0.0.0.0:31024-&gt;1024/tcp, 0.0.0.0:33063-&gt;3306/tcp, 0.0.0.0:36379-&gt;6379/tcp, 0.0.0.0:9004-&gt;9001/tcp myCat9ae039d46474 0.0.0.0:221-&gt;22/tcp, 0.0.0.0:8081-&gt;80/tcp, 0.0.0.0:10241-&gt;1024/tcp, 0.0.0.0:33061-&gt;3306/tcp, 0.0.0.0:16379-&gt;6379/tcp, 0.0.0.0:9002-&gt;9001/tcp slave106e5a050e74b 0.0.0.0:222-&gt;22/tcp, 0.0.0.0:8082-&gt;80/tcp, 0.0.0.0:10242-&gt;1024/tcp, 0.0.0.0:33062-&gt;3306/tcp, 0.0.0.0:26379-&gt;6379/tcp, 0.0.0.0:9003-&gt;9001/tcp slave2 master:123456mysql&gt; show master status;+------------------+----------+--------------+------------------+-------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+-------------------+| mysql-bin.000001 | 154 | zhimma | | |+------------------+----------+--------------+------------------+-------------------+ mycat:12[root@dcccd23b6177 mycat]# mycat statusMycat-server is running (317). slave1:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263mysql&gt; show slave status\\G;*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 172.17.0.2 Master_User: zhimma Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000001 Read_Master_Log_Pos: 154 Relay_Log_File: 9ae039d46474-relay-bin.000003 Relay_Log_Pos: 367 Relay_Master_Log_File: mysql-bin.000001 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 154 Relay_Log_Space: 581 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 Master_UUID: f781e2b4-28e1-11e8-a1c0-0242ac110002 Master_Info_File: /var/lib/mysql/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec)ERROR: No query specified salve2:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263mysql&gt; show slave status \\G;*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.2.107 Master_User: zhimma Master_Port: 33060 Connect_Retry: 60 Master_Log_File: mysql-bin.000001 Read_Master_Log_Pos: 154 Relay_Log_File: 06e5a050e74b-relay-bin.000003 Relay_Log_Pos: 367 Relay_Master_Log_File: mysql-bin.000001 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 154 Relay_Log_Space: 581 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 Master_UUID: f781e2b4-28e1-11e8-a1c0-0242ac110002 Master_Info_File: /var/lib/mysql/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec)ERROR: No query specified 安装省略 目录结构： 12345[root@dcccd23b6177 mycat]# pwd/etc/mycat[root@dcccd23b6177 mycat]# lsbin catlet conf lib logs tmlogs version.txt[root@dcccd23b6177 mycat]# 目录 说明 bin mycat命令，启动、重启、停止等 catlet catlet为Mycat的一个扩展功能 conf Mycat 配置信息,重点关注 lib Mycat引用的jar包，Mycat是java开发的 logs 日志文件，包括Mycat启动的日志和运行的日志。 配置Mycat的配置文件都在conf目录里面，这里介绍几个常用的文件： 文件 说明 server.xml Mycat的配置文件，设置账号、参数等 schema.xml Mycat对应的物理数据库和数据库表的配置 rule.xml Mycat分片（分库分表）规则 Mycat的架构Mycat的架构其实很好理解，Mycat是代理，Mycat后面就是物理数据库。和Web服务器的Nginx类似。对于使用者来说，访问的都是Mycat，不会接触到后端的数据库。 配置server.xml1234&lt;user name=\"root\" defaultAccount=\"true\"&gt; &lt;property name=\"password\"&gt;123456&lt;/property&gt; &lt;property name=\"schemas\"&gt;zhimma_mycat&lt;/property&gt;&lt;/user&gt; 重点关注下面这段，其他默认即可。 参数 说明 user 用户配置节点 –name 登录的用户名，也就是连接Mycat的用户名 –password 登录的密码，也就是连接Mycat的密码 –schemas 数据库名，这里会和schema.xml中的配置关联，多个用逗号分开，例如需要这个用户需要管理两个数据库db1,db2，则配置db1,db2 –privileges 配置用户针对表的增删改查的权限，具体见文档吧 我这里配置了一个账号root密码是123456,针对数据库zhimma_mycat,读写权限都有，没有针对表做任何特殊的权限。 schema.xml12345678910111213141516171819202122&lt;?xml version=\"1.0\"?&gt;&lt;!DOCTYPE mycat:schema SYSTEM \"schema.dtd\"&gt;&lt;mycat:schema xmlns:mycat=\"http://io.mycat/\"&gt; &lt;schema name=\"zhimma_mycat\" checkSQLschema=\"false\" sqlMaxLimit=\"100\"&gt; &lt;table name=\"users\" dataNode=\"dn1\" /&gt; &lt;table name=\"articles\" dataNode=\"dn1\" /&gt; &lt;table name=\"categories\" dataNode=\"dn1\" /&gt; &lt;/schema&gt; &lt;dataNode name=\"dn1\" dataHost=\"localhost1\" database=\"zhimma\" /&gt; &lt;dataHost name=\"localhost1\" maxCon=\"1000\" minCon=\"10\" balance=\"1\" writeType=\"0\" dbType=\"mysql\" dbDriver=\"native\" switchType=\"1\" slaveThreshold=\"100\"&gt; &lt;heartbeat&gt;select users()&lt;/heartbeat&gt; &lt;!-- can have multi write hosts --&gt; &lt;writeHost host=\"hostM1\" url=\"192.168.2.107:33060\" user=\"root\" password=\"123456\"&gt; &lt;!-- can have multi read hosts --&gt; &lt;readHost host=\"hostS1\" url=\"192.168.2.107:33061\" user=\"root\" password=\"123456\" /&gt; &lt;readHost host=\"hostS2\" url=\"192.168.2.107:33062\" user=\"root\" password=\"123456\" /&gt; &lt;/writeHost&gt; &lt;/dataHost&gt;&lt;/mycat:schema&gt; 参数 说明 schema 数据库设置，此数据库为逻辑数据库，name与server.xml中schema对应,此处为zhimma_mycat dataNode 分片信息，也就是分库相关配置 dataHost 物理数据库，真正存储数据的数据库 每个节点的属性逐一说明： schema: 属性 说明 name 逻辑数据库名，与server.xml中的schema对应 checkSQLschema 布尔值，默认和推荐都是关闭。如果开启则会拦截SQL语句，将其中的mycat相关字段删除如：select from zhimma_mycat.users;将会把SQL语句变为：select from users; sqlMaxLimit SQL返回条数，最好加上限制，防止查询量过大导致卡死 table: 属性 说明 name 表名，物理数据库中表名 dataNode 表存储到哪些节点，多个节点用逗号分隔。节点为下文dataNode设置的name primaryKey 主键字段名，自动生成主键时需要设置 autoIncrement 是否自增 rule 分片规则名，具体规则下文rule详细介绍 dataNode 属性 说明 name 节点名，与table中dataNode对应 datahost 物理数据库名，与datahost中name对应 database 物理数据库中数据库名 dataHost 属性 说明 name 物理数据库名，与dataNode中dataHost对应 balance 均衡负载的方式 writeType 写入方式 dbType 数据库类型 heartbeat 心跳检测语句，注意语句结尾的分号要加。","categories":[{"name":"Mysql","slug":"Mysql","permalink":"https://blog.zhimma.com/categories/Mysql/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://blog.zhimma.com/tags/Mysql/"},{"name":"MyCat","slug":"MyCat","permalink":"https://blog.zhimma.com/tags/MyCat/"}]},{"title":"Nginx","slug":"Nginx","date":"2018-11-29T16:00:00.000Z","updated":"2019-01-24T09:15:24.600Z","comments":true,"path":"2018/11/30/Nginx/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/Nginx/","excerpt":"","text":"安装版本说明Mainline version 开发版本 Stable version 稳定版本 Legacy versions 历史版本 安装源To set up the yum repository for RHEL/CentOS, create the file named /etc/yum.repos.d/nginx.repo with the following contents: 12345678&gt; [nginx]&gt; name=nginx repo&gt; baseurl=http://nginx.org/packages/OS/OSRELEASE/$basearch/&gt; gpgcheck=0&gt; enabled=1&gt; &gt; baseurl=http://nginx.org/packages/centos/7/$basearch/&gt; Replace “OS” with “rhel” or “centos”, depending on the distribution used, and “OSRELEASE” with “6” or “7”, for 6.x or 7.x versions, respectively. 查看nginx安装包yum list | grep nginx 安装nginxyum install nginx -y 查看版本信息12345678[root@host ~]# nginx -vnginx version: nginx/1.14.2[root@host ~]# nginx -Vnginx version: nginx/1.14.2built by gcc 4.8.5 20150623 (Red Hat 4.8.5-28) (GCC)built with OpenSSL 1.0.2k-fips 26 Jan 2017TLS SNI support enabledconfigure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -fPIC' --with-ld-opt='-Wl,-z,relro -Wl,-z,now -pie' nginx 配置解析 user 设置nginx服务的系统使用用户 worker_processes 工作进程数 error_log nginx的错误日志 pid nginx服务 12345678user www www;worker_processes auto;worker_cpu_affinity auto;error_log /home/wwwlogs/nginx_error.log error;# pid /usr/local/nginx/logs/nginx.pid;pid /run/nginx.pid;#Specifies the value for maximum file descriptors that can be opened by this process.worker_rlimit_nofile 65535; events worker_connections 每个工作进程运行的最大链接数 use 工作进程模式 1234events &#123; use epoll; worker_connections 65535;&#125; nginx 模块stub_status123location /nginx &#123; stub_status;&#125; 1234Active connections: 20 server accepts handled requests 274 274 784 Reading: 0 Writing: 1 Waiting: 19","categories":[{"name":"Nginx","slug":"Nginx","permalink":"https://blog.zhimma.com/categories/Nginx/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://blog.zhimma.com/tags/Nginx/"}]},{"title":"Nginx与PHP的交互机制(1)","slug":"Nginx与PHP的交互机制","date":"2018-11-29T16:00:00.000Z","updated":"2019-02-01T07:35:56.483Z","comments":true,"path":"2018/11/30/Nginx与PHP的交互机制/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/Nginx与PHP的交互机制/","excerpt":"","text":"Nginx与PHP的交互机制(1)from there https://www.awaimai.com/371.html 在搭建 LAMP/LNMP 服务器时，会经常遇到 PHP-FPM、FastCGI和CGI 这几个概念。如果对它们一知半解，很难搭建出高性能的服务器。接下来我们就以图形方式，解释这些概念之间的关系。 基础在整个网站架构中，Web Server（如Apache）只是内容的分发者。举个栗子，如果客户端请求的是 index.html，那么Web Server会去文件系统中找到这个文件，发送给浏览器，这里分发的是静态数据。 如果请求的是 index.php，根据配置文件，Web Server知道这个不是静态文件，需要去找 PHP 解析器来处理，那么他会把这个请求简单处理，然后交给PHP解析器。 当Web Server收到 index.php 这个请求后，会启动对应的 CGI 程序，这里就是PHP的解析器。接下来PHP解析器会解析php.ini文件，初始化执行环境，然后处理请求，再以规定CGI规定的格式返回处理后的结果，退出进程，Web server再把结果返回给浏览器。这就是一个完整的动态PHP Web访问流程 CGICGI是 Web Server 与 Web Application 之间数据交换的一种协议。CGI全称是“公共网关接口”(Common Gateway Interface)，描述的是服务器和请求处理程序之间传输数据的一种标准（服务器与你的或其它机器上的程序进行“交谈”的一种工具）。 所以，CGI是一种协议。CGI可用于任何语言，只要该语言具有标准的输出、输入以及环境变量。如perl、php等语言。 以nginx和php为例，我们可以理解为，这是php在与nginx服务器之间交互时，对传输数据的一种约定。 CGI的原理是什么?当需要请求使用网关的资源时，服务器会请辅助应用程序来处理请求（比如nginx会请php程序来处理请求）。 服务器会将辅助应用程序的数据传送给网关。然后网关会向服务器返回一条响应或者响应数据，服务器再将响应或响应数据转发给客户端。 由此我们可以清楚两点： 服务器和网关是相互独立的应用程序 服务器是应用程序和网关之间的一座桥梁 由此，我们可知CGI有一个致命的弱点，即应用程序的每次请求，都要引发一个全新的进程。所以，服务器和网关之间的分离会造成性能的 耗费，会限制使用CGI的服务器的性能，并且会加重服务端机器资源的负担。好啦，重角要登场了。后来为了解决这个问题，出现了FastCGI，也就是快速的CGI。 接下来，我们再详细的了解下FastCGI。 FastCGI FastCGI:(Fast Common Gateway Interface),即快速通用网关接口，是一种让交互程序与Web服务器通信的协议。它是CGI的增强版本 FastCGI致力于减少网页服务器与CGI程序之间互动的开销，从而使服务器可以同时处理更多的网页请求。以上来自维基百科，我们可以由此了解到，FastCGI，同CGI一样，也是一种协议，只不过它是CGI的增强版本。那FastCGI是如何增强性能的呢？ FastCGI接口模拟了CGI，但FastCGI是作为持久守护进程运行的，消除了为每个请求建立或拆除新进程所带来的性能损耗。也就是允许，一个进程内可以处理多个请求。 也就说CGI解释器保持在内存中，并接受了FastCGI进程管理和调度，所以它可以提供更好的性能，可扩展性，故障切换等特点 FastCGI的特点: FastCGI与语言无关 FastCGI应用在进程中，独立于核心网络服务器，提供了一个比API环境更安全的环境。 APIs的代码和web服务器的应用的核心是 紧紧关联的。这也就意味着在API应用程序的错误可能会损坏其它应用程序或核心服务器。恶意API应用程序代码甚至可以窃取另一个应用程序或核心服务器密钥。 FastCGI技术目前支持PHP,C/C++, Java lanuage, Perl, Tcl, Python, SmallTalk, Ruby etc.. 它在Apache, ISS, Lighttpd和其他流行的 服务器中的相关模块都是可以使用的。FastCGI不依赖于任何服务器体系结构，所以即使服务器在技术上改变了，FastCGI还是稳定的 FastCGI的工作原理 Web Server 启动时载入FastCGI进程管理器 (IIS ISAPI 或Apache Module) FastCGI进程管理器首先初始化自己，启动一批CGI解释器进程（可见多个php-cgi），然后等待来自Web Server的连接。 当Web Server中的一个客户端请求达到时， FastCGI进程管理器会选择并连接一个CGI解释器。Web server的CGI环境变量和标准输入会被送达FastCGI进程的php-cgi。 FastCGI子进程从同一连接完成返还给Web Server的标准输出和错误信息。当请求进程完成后，FastCGI进程会关闭此连接。FastCGI会等待并出来来自FastCGI进程管理器（运行在Web Server中的）的下一个连接。 在CGI模式，php-cgi然后会退出。 如上图所示，FastCGI的下游，是CGI-APP，在我们的LNMP架构里，这个CGI-APP就是PHP程序。而FastCGI的上游是Nginx，他们之间有一个通信载体，即图中的socket。上图中的Pre-fork，则对应着我们PHP-FPM的启动，也就是在我们启动PHP-FPM时便会根据用户配置启动诸多FastCGI触发器（FastCGI Wrapper）。 FastCGI的不足 因为是多进程，所以比CGI多线程消耗更多的服务器内存，PHP-CGI解释器每进程消耗7至25兆内存，将这个数字乘以50或100就是很大的内存数。 Nginx 0.8.46+PHP 5.2.14(FastCGI)服务器在3万并发连接下，开启的10个Nginx进程消耗150M内存（15M_10=150M），开启的64个php-cgi进程消耗1280M内存（20M_64=1280M），加上系统自身消耗的内存，总共消耗不到2GB内存。 如果服务器内存较小，完全可以只开启25个php-cgi进程，这样php-cgi消耗的总内存数才500M。 上面的数据摘自Nginx 0.8.x + PHP 5.2.13(FastCGI)搭建胜过Apache十倍的Web服务器(第6版) PHP-CGIPHP-CGI：是 PHP （Web Application）对 Web Server 提供的 CGI 协议的接口程序,是PHP自带的FastCGI管理器。 PHP-CGI的不足: php-cgi变更php.ini配置后需重启php-cgi才能让新的php-ini生效，不可以平滑重启 直接杀死php-cgi进程,php就不能运行了。(PHP-FPM和Spawn-FCGI就没有这个问题,守护进程会平滑从新生成新的子进程。） PHP-FPMPHP-FPM：是 PHP（Web Application）对 Web Server 提供的 FastCGI 协议的接口程序，额外还提供了相对智能一些任务管理。PHP-FPM的全称是PHP FastCGI Process Manager。它是 PHP 针对 FastCGI 协议的具体实现，它会通过用户配置来管理一批FastCGI进程.因此它也是PHP 在多种服务器端应用编程端口（SAPI：cgi、fast-cgi、cli、isapi、apache）里使用最普遍、性能最佳的一款进程管理器。在PHP-FPM管理下的某个FastCGI进程挂了，PHP-FPM会根据用户配置来看是否要重启补全。PHP-FPM更像是管理器，负责管理PHP FastCGI，而真正衔接Nginx与PHP的则是FastCGI进程。因此，CGI是通用网关协议，FastCGI则是一种常驻进程的CGI模式程序，而PHP-FPM更像是管理器，用于管理FastCGI进程。WEB 中: Web Server 一般指Apache、Nginx、IIS、Lighttpd、Tomcat等服务器， Web Application 一般指PHP、Java、Asp.net等应用程序。 Nginx+PHP的工程模式下，两位主角分工明确，Nginx负责承载HTTP请求的响应与返回，以及超时控制记录日志等HTTP相关的功能，而PHP则负责处理具体请求要做的业务逻辑，它们俩的这种合作模式也是常见的分层架构设计中的一种，在它们各有专注面的同时，FastCGI又很好的将两块衔接，保障上下游通信交互，这种通过某种协议或规范来衔接好上下游的模式，在我们日常的PHP应用开发中也有这样的思想落地，譬如我们所开发的高性能API，具体的Client到底是PC、APP还是某个其他程序，我们不关心，而这些PC、APP、第三方程序也不关心我们的PHP代码实现，他们按照API的规范来请求做处理即可","categories":[{"name":"Nginx","slug":"Nginx","permalink":"https://blog.zhimma.com/categories/Nginx/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://blog.zhimma.com/tags/PHP/"},{"name":"Nginx","slug":"Nginx","permalink":"https://blog.zhimma.com/tags/Nginx/"}]},{"title":"Nginx配置整理","slug":"Nginx配置整理","date":"2018-11-29T16:00:00.000Z","updated":"2019-01-24T09:16:04.115Z","comments":true,"path":"2018/11/30/Nginx配置整理/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/Nginx配置整理/","excerpt":"","text":"Nginx配置整理https://www.zybuluo.com/phper/note/89391 nginx.conf配置概览1234567891011121314151617181920212223242526272829mainevents &#123; ...&#125;http &#123; ... upstram myObject &#123; ... &#125; server1 &#123; ... location &#123; ... &#125; &#125; ... serverN &#123; ... location &#123; ... &#125; &#125;&#125; nginx配置文件主要分为六个区域： main(全局设置)、event(nginx工作模式)、http(http设置)、upstream(负载均衡服务器设置)、server(主机设置)、location(URL匹配)。 main模块1234567891011121314// 来指定Nginx Worker进程运行的用户及用户组，默认由nobody账号运行。user nobody nobody;// 来指定Nginx要快起的子进程数。每个Nginx进程平均耗费10M~12M内存。根据经验一般指定一个进程就足够了，如果是多核CPU，建议数量和CPU数量保持一样即可。worker_processes 1;// 用来定义全局错误日志文件。日志输出级别有debug、info、notice、warn、error、crit可供选择，其中，debug输出日志最为最详细，而crit输出日志最少。#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;// 用来指定进程id的存储文件位置#pid logs/nginx.pid;// 用于指定一个nginx进程最多可以打开的最多文件描述符数目worker_rlimit_nofile 1024; events模块1234events &#123; use kqueue; #mac平台 worker_connections 1024;&#125; use用来指定Nginx的工作模式。Nginx支持的工作模式有select、poll、kqueue、epoll、rtsig和/dev/poll。其中select和poll都是标准的工作模式，kqueue和epoll是高效的工作模式，不同的是epoll用在Linux平台上，而kqueue用在BSD系统中，因为Mac基于BSD,所以Mac也得用这个模式，对于Linux系统，epoll工作模式是首选。 worker_connections用于定义Nginx每个进程的最大连接数，即接收前端的最大请求数，默认是1024。最大客户端连接数由worker_processes和worker_connections决定，即Max_clients=worker_processes*worker_connections，在作为反向代理时，Max_clients变为：Max_clients = worker_processes * worker_connections/4。进程的最大连接数受Linux系统进程的最大打开文件数限制，在执行操作系统命令“ulimit -n 65536”后worker_connections的设置才能生效。 http 模块Http模块是核心的模块，负责HTTP服务相关属性的配置，包含server和upstream子模块。1234567891011121314151617181920212223242526272829http &#123; // incluede 用来设定文件的mime类型，类型在配置文件目录下的mime.type文件定义，用来告诉nginx来识别文件类型 include mime.types; // 设置了默认类型为二进制流，也就是说当文件类型未定义时使用这种方式。 default_type application/octet-stream; client_max_body_size 120m; // 设置日志格式和记录那修参数 这里设置为main，刚好用于access_log来记录这种类型 log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; // 用来纪录每次的访问日志的文件地址，后面的`main`是日志的格式样式，对应于log_format的main。 access_log /var/log/nginx/access.log main; // 用于开启高效文件传输模式，将tcp_nopush和tcp_nodelay两个指令设置为on用于防止网络阻塞 sendfile on; tcp_nopush on; // 设置客户端连接保持活动的超时时间。在超过这个时间之后，服务器会关闭该连接。 #keepalive_timeout 0; keepalive_timeout 65; #gzip on; upstream myproject &#123; ... &#125; server &#123; ... &#125; server模块12345678910111213141516171819// 标志定义虚拟主机开始server &#123; // 用于指定虚拟主机的服务端口 listen 80; // 用来指定IP地址或者域名，多个域名直接用空格分个格 server_name localhost 192.168.12.10 www.yangyi.com; // 表示虚拟主机的root web目录 root /home/www; // 全局定义访问的默认首页地址，需要和locate&#123;&#125;下面定义的区分开来 index index.php index.html index.htm; // 设置网页的默认编码格式 charset utf-8; // 用来存放次虚拟主机的日志目录，最后的main用来指定访问日志的输出格式 access_log usr/local/var/log/host.access.log main; aerror_log usr/local/var/log/host.error.log error; location / &#123; &#125;&#125; location 模块location模块是nginx中用的最多的，也是最重要的模块了，什么负载均衡啊、反向代理啊、虚拟域名啊都与它相关location 根据它字面意思就知道是来定位的，定位URL，解析URL，所以，它也提供了强大的正则匹配功能，也支持条件判断匹配，用户可以通过location指令实现Nginx对动、静态网页进行过滤处理。像我们的php环境搭建就是用到了它。1234567// 匹配访问根目录location / &#123; // root指令用于指定访问根目录时，虚拟主机的web目录，这个目录可以是相对路径（相对路径是相对于nginx的安装目录）。也可以是绝对路径。 root /home/www; // index用于设定我们只输入域名后访问的默认首页地址，有个先后顺序：index.php index.html index.htm，如果没有开启目录浏览权限，又找不到这些默认首页，就会报403错误。 index index.php index.html index.htm;&#125; 下面这个例子是运用正则匹配来链接php。我们之前搭建环境也是这样做：123456789// 正则匹配.php结尾的URLlocation ~ \\.php$ &#123; root /home/www; // fast_pass链接的是php-fpm 的地址 fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fastcgi.conf;&#125; upstream 模块upstream 模块负债负载均衡模块，通过一个简单的调度算法来实现客户端IP到后端服务器的负载均衡。1234567upstream demo.com&#123; ip_hash; server 192.168.12.1:80; server 192.168.12.2:80 down; server 192.168.12.3:8080 max_fails=3 fail_timeout=20s; server 192.168.12.4:8080;&#125; 在上面的例子中，通过upstream指令指定了一个负载均衡器的名称demo.com。这个名称可以任意指定，在后面需要的地方直接调用即可。 里面是ip_hash这是其中的一种负载均衡调度算法。紧接着就是各种服务器了。用server关键字表识，后面接ip。 Nginx的负载均衡模块目前支持4种调度算法: weight 轮询（默认）。每个请求按时间顺序逐一分配到不同的后端服务器，如果后端某台服务器宕机，故障系统被自动剔除，使用户访问不受影响。weight。指定轮询权值，weight值越大，分配到的访问机率越高，主要用于后端每个服务器性能不均的情况下。 ip_hash。每个请求按访问IP的hash结果分配，这样来自同一个IP的访客固定访问一个后端服务器，有效解决了动态网页存在的session共享问题。 fair。比上面两个更加智能的负载均衡算法。此种算法可以依据页面大小和加载时间长短智能地进行负载均衡，也就是根据后端服务器的响应时间来分配请求，响应时间短的优先分配。Nginx本身是不支持fair的，如果需要使用这种调度算法，必须下载Nginx的upstream_fair模块。 url_hash。按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，可以进一步提高后端缓存服务器的效率。Nginx本身是不支持url_hash的，如果需要使用这种调度算法，必须安装Nginx 的hash软件包。 在HTTP Upstream模块中，可以通过server指令指定后端服务器的IP地址和端口，同时还可以设定每个后端服务器在负载均衡调度中的状态。常用的状态有： down，表示当前的server暂时不参与负载均衡。 backup，预留的备份机器。当其他所有的非backup机器出现故障或者忙的时候，才会请求backup机器，因此这台机器的压力最轻。 max_fails，允许请求失败的次数，默认为1。当超过最大次数时，返回proxy_next_upstream 模块定义的错误。 fail_timeout，在经历了max_fails次失败后，暂停服务的时间。max_fails可以和fail_timeout一起使用。 注意 当负载调度算法为ip_hash时，后端服务器在负载均衡调度中的状态不能是weight和backup。","categories":[{"name":"Nginx","slug":"Nginx","permalink":"https://blog.zhimma.com/categories/Nginx/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://blog.zhimma.com/tags/Nginx/"}]},{"title":"PHP-FPM配置详解","slug":"PHP-FPM配置详解","date":"2018-11-29T16:00:00.000Z","updated":"2019-01-24T09:16:53.540Z","comments":true,"path":"2018/11/30/PHP-FPM配置详解/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/PHP-FPM配置详解/","excerpt":"","text":"PHP-FPM配置文件参数解释 PHP-FPM配置详解FPM配置文件为php-fpm.conf，其语法类似 php.ini 。其php手册上也有详细的讲解：http://php.net/manual/zh/install.fpm.configuration.php php-fpm.conf全局配置段12345678# 包含其他POOL定义配置文件include=/etc/php-fpm.d/*.conf# 全局配置段，定义PID文件的位置和错误日志的位置[global]daemonize = yespid = /var/run/php-fpm/php-fpm.piderror_log = /var/log/php-fpm/error.log 一般在主配置文件php-fpm.conf全局配置段中的配置非常少，php-fpm可以配置多个pool，每个pool都是以一个独立的配置文件来运作，默认都会定义在主配置文件的include包含文件目录中。php默认会提供一个www的pool，大概配置如下。 123456789101112$ cat /etc/php-fpm.d/www.conf[www]user = apachegroup = apache listen = 127.0.0.1:9000listen.allowed_clients = 127.0.0.1pm = dynamicpm.max_children = 50pm.start_servers = 5pm.min_spare_servers = 5pm.max_spare_servers = 35 每个pool配置文件参数可以独立，也可以设置在主配置文件的全局配置段中，这样每个pool就共用一个参数。建议最好分开设置。大概常用参数如下： daemonize = yes#后台执行fpm，默认值为yes，如果为了调试可以改为no。在FPM中，可以使用不同的设置来运行多个进程池。 这些设置可以针对每个进程池单独设置。 listen = 127.0.0.1:9000#fpm监听端口，即nginx中php处理的地址，一般默认值即可。可用格式为: ‘ip:port’, ‘port’, ‘/path/to/unix/socket’，每个进程池都需要设置。如果nginx和php在不同的机器上，分布式处理，就设置ip这里就可以了。 listen.backlog = -1#backlog数，设置 listen 的半连接队列长度，-1表示无限制，由操作系统决定，此行注释掉就行。backlog含义参考：http://www.3gyou.cc/?p=41。 log_level = notice#错误级别. 上面的php-fpm.log纪录的登记。可用级别为: alert（必须立即处理）, error（错误情况）, warning（警告情况）, notice（一般重要信息）, debug（调试信息）. 默认: notice。 emergency_restart_threshold = 60emergency_restart_interval = 60s#表示在emergency_restart_interval所设值内出现SIGSEGV或者SIGBUS错误的php-cgi进程数如果超过 emergency_restart_threshold个，php-fpm就会优雅重启。这两个选项一般保持默认值。0 表示 ‘关闭该功能’. 默认值: 0 (关闭). process_control_timeout = 0#设置子进程接受主进程复用信号的超时时间. 可用单位: s(秒), m(分), h(小时), 或者 d(天) 默认单位: s(秒). 默认值: 0. listen.allowed_clients = 127.0.0.1#允许访问FastCGI进程的IP白名单，设置any为不限制IP，如果要设置其他主机的nginx也能访问这台FPM进程，listen处要设置成本地可被访问的IP。默认值是any。每个地址是用逗号分隔. 如果没有设置或者为空，则允许任何服务器请求连接。 listen.owner = wwwlisten.group = wwwlisten.mode = 0666#unix socket设置选项，如果使用tcp方式访问，这里注释即可。 user = wwwgroup = www#启动进程的用户和用户组，FPM 进程运行的Unix用户, 必须要设置。用户组，如果没有设置，则默认用户的组被使用。 pm = dynamic#php-fpm进程启动模式，pm可以设置为static和dynamic和ondemand。如果选择static，则进程数就是固定的，由pm.max_children指定固定的子进程数。如果选择dynamic，则进程数是动态变化的，由以下参数决定： pm.max_children = 50#子进程能开启的最大数。 pm.start_servers = 2#启动时的进程数，默认值为: min_spare_servers + (max_spare_servers – min_spare_servers) / 2。 pm.min_spare_servers = 1#保证空闲进程数最小值，如果空闲进程小于此值，则创建新的子进程。 pm.max_spare_servers = 3#保证空闲进程数最大值，如果空闲进程大于此值，此进行清理。 pm.max_requests = 500#设置每个子进程重生之前服务的请求数. 对于可能存在内存泄漏的第三方模块来说是非常有用的. 如果设置为 ‘0’ 则一直接受请求. 等同于 PHP_FCGI_MAX_REQUESTS 环境变量. 默认值: 0。 pm.status_path = /status#FPM状态页面的网址. 如果没有设置, 则无法访问状态页面. 默认值: none. munin监控会使用到 ping.path = /ping#FPM监控页面的ping网址. 如果没有设置, 则无法访问ping页面. 该页面用于外部检测FPM是否存活并且可以响应请求. 请注意必须以斜线开头 (/)。 ping.response = pong#用于定义ping请求的返回相应. 返回为 HTTP 200 的 text/plain 格式文本. 默认值: pong. access.log = log/$pool.access.log#每一个请求的访问日志，默认是关闭的。 access.format = “%R – %u %t \\”%m %r%Q%q\\” %s %f %{mili}d %{kilo}M %C%%”#设定访问日志的格式。 slowlog = log/$pool.log.slow#慢请求的记录日志,配合request_slowlog_timeout使用，默认关闭 request_slowlog_timeout = 10s#当一个请求该设置的超时时间后，就会将对应的PHP调用堆栈信息完整写入到慢日志中. 设置为 ‘0’ 表示 ‘Off’ request_terminate_timeout = 0#设置单个请求的超时中止时间. 该选项可能会对php.ini设置中的’max_execution_time’因为某些特殊原因没有中止运行的脚本有用. 设置为 ‘0’ 表示 ‘Off’.当经常出现502错误时可以尝试更改此选项。 rlimit_files = 1024#设置文件打开描述符的rlimit限制. 默认值: 系统定义值默认可打开句柄是1024，可使用 ulimit -n查看，ulimit -n 2048修改。 rlimit_core = 0#设置核心rlimit最大限制值. 可用值: ‘unlimited’ 、0或者正整数. 默认值: 系统定义值。 chroot = /data/app#启动时的Chroot目录. 所定义的目录需要是绝对路径. 如果没有设置, 则chroot不被使用。 chdir = /data/app#设置启动目录，启动时会自动Chdir到该目录. 所定义的目录需要是绝对路径. 默认值: 当前目录，或者/目录（chroot时）。 catch_workers_output = yes#重定向运行过程中的stdout和stderr到主要的错误日志文件中. 如果没有设置, stdout 和 stderr 将会根据FastCGI的规则被重定向到 /dev/null . 默认值: 空。 当然还有一些无关紧要的设置，用到了再说吧。 PHP-FPM重要的设置PHP-FPM重要的设置在之前的文章中就说过了。在fasgcgi模式下，php会启动多个php-fpm进程，来接收nginx发来的请求，那是不是进程越多，速度就越快呢？这可不一定！得根据我们的机器配置和业务量来决定。 我们先来看下，设定进程的配置在哪里？ pm = static | dynamic | ondemandpm可以设置成这样3种，我们用的最多的就上前面2种。 pm = static模式 pm = static 表示我们创建的php-fpm子进程数量是固定的，那么就只有pm.max_children = 50这个参数生效。你启动php-fpm的时候就会一起全部启动51(1个主＋50个子)个进程，颇为壮观。 pm = dynamic模式 pm = dynamic模式，表示启动进程是动态分配的，随着请求量动态变化的。他由pm.max_children，pm.start_servers，pm.min_spare_servers，pm.max_spare_servers 这几个参数共同决定。 上面已经讲过，这里再重申一下吧： pm.max_children ＝ 50是最大可创建的子进程的数量。必须设置。这里表示最多只能50个子进程。 pm.start_servers = 20随着php-fpm一起启动时创建的子进程数目。默认值：min_spare_servers + (max_spare_servers – min_spare_servers) / 2。这里表示，一起启动会有20个子进程。 pm.min_spare_servers = 10 设置服务器空闲时最小php-fpm进程数量。必须设置。如果空闲的时候，会检查如果少于10个，就会启动几个来补上。 pm.max_spare_servers = 30 设置服务器空闲时最大php-fpm进程数量。必须设置。如果空闲时，会检查进程数，多于30个了，就会关闭几个，达到30个的状态。到底选择static还数dynamic? 很多人恐惧症来袭，不知道选什么好？ 一般原则是：动态适合小内存机器，灵活分配进程，省内存。静态适用于大内存机器，动态创建回收进程对服务器资源也是一种消耗。 如果你的内存很大，有8-20G，按照一个php-fpm进程20M算，100个就2G内存了，那就可以开启static模式。如果你的内存很小，比如才256M，那就要小心设置了，因为你的机器里面的其他的进程也算需要占用内存的，所以设置成dynamic是最好的，比如：pm.max_chindren = 8, 占用内存160M左右，而且可以随时变化，对于一半访问量的网站足够了。 慢日志查询我们有时候会经常饱受500,504问题困扰。当nginx收到如上错误码时，可以确定后端php-fpm解析php出了某种问题，比如，执行错误，执行超时。 这个时候，我们是可以开启慢日志功能的。 slowlog = /usr/local/var/log/php-fpm.log.slowrequest_slowlog_timeout = 15s当一个请求该设置的超时时间15秒后，就会将对应的PHP调用堆栈信息完整写入到慢日志中。 php-fpm慢日志会记录下进程号，脚本名称，具体哪个文件哪行代码的哪个函数执行时间过长： [21-Nov-2013 14:30:38] [pool www] pid 11877script_filename = /usr/local/lnmp/nginx/html/www.quancha.cn/www/fyzb.php[0xb70fb88c] file_get_contents() /usr/local/lnmp/nginx/html/www.quancha.cn/www/fyzb.php:2 通过日志，我们就可以知道第2行的file_get_contents 函数有点问题，这样我们就能追踪问题了。","categories":[{"name":"PHP","slug":"PHP","permalink":"https://blog.zhimma.com/categories/PHP/"}],"tags":[{"name":"PHP-FPM","slug":"PHP-FPM","permalink":"https://blog.zhimma.com/tags/PHP-FPM/"}]},{"title":"PHP中匿名函数","slug":"PHP中匿名函数","date":"2018-11-29T16:00:00.000Z","updated":"2019-01-24T09:18:13.867Z","comments":true,"path":"2018/11/30/PHP中匿名函数/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/PHP中匿名函数/","excerpt":"匿名函数,说白了就是”没有名字的函数” 123$foo = function()&#123; //this is a closure;&#125; 上面声明的就是匿名函数,没有什么特别的,和一般函数结构神似,唯独少了个函数名。并且这个函数可以作为一个值被赋予一个变量或者对象属性。这种特性是的拥有匿名函数的编程语言在设计一些应用时,更为灵活。 匿名函数和普通函数最大的区别是在于： 匿名函数可以作为一个具体的”值”赋予给变量或者对象属性 匿名函数可以被定义在不同地方,使得它可以有效利用他所在的局域内的变量（或者说上下文环境）","text":"匿名函数,说白了就是”没有名字的函数” 123$foo = function()&#123; //this is a closure;&#125; 上面声明的就是匿名函数,没有什么特别的,和一般函数结构神似,唯独少了个函数名。并且这个函数可以作为一个值被赋予一个变量或者对象属性。这种特性是的拥有匿名函数的编程语言在设计一些应用时,更为灵活。 匿名函数和普通函数最大的区别是在于： 匿名函数可以作为一个具体的”值”赋予给变量或者对象属性 匿名函数可以被定义在不同地方,使得它可以有效利用他所在的局域内的变量（或者说上下文环境） 1234567891011class foo&#123; public function exec(Closure $callback) &#123; echo $callback();//hi ,nick &#125;&#125;$name = &quot;nick&quot;;(new foo())-&gt;exec(function() use ($name)&#123; return &quot;hi ,&quot;.$name;&#125;); 可以看到,匿名函数使用使用了上下文中的变量$name。而实际上,这个匿名函数实在另一个地方被执行（foo类里面）。这样使得我们不必将变量$name的值通过参数传递到foo类的exec方法中,而且可以减少在exec方法中不必要的处理逻辑,使得类跟家专注于自己的职责； 匿名函数定义时不会被执行,除非被调用,上文中的例子就是这样,利用这种特性,我们可以利用它来实现一个控制反转（IoC）容器。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869/** * 一个简单的IoC容器 */class Container&#123; protected static $bindings; public static function bind($abstract, Closure $concrete) &#123; static::$bindings[$abstract] = $concrete; &#125; public static function make($abstract) &#123; return call_user_func(static::$bindings[$abstract]); &#125;&#125; /** * 示例用的 talk 类 */class talk&#123; public function greet($target) &#123; echo &apos;hi, &apos; . $target-&gt;getName(); &#125;&#125; /** * 示例用的 A 类 */class A&#123; public function getName() &#123; return &apos;Nick&apos;; &#125;&#125; /** * 示例用的 B 类 */class B&#123; public function getName() &#123; return &apos;Amy&apos;; &#125;&#125; // 以下代码是主要示例代码 // 创建一个talk类的实例$talk = new talk; // 将A类绑定至容器,命名为fooContainer::bind(&apos;foo&apos;, function() &#123; return new A;&#125;); // 将B类绑定至容器,命名为barContainer::bind(&apos;bar&apos;, function() &#123; return new B;&#125;); // 通过容器取出实例$talk-&gt;greet(Container::make(&apos;foo&apos;)); // hi, Nick$talk-&gt;greet(Container::make(&apos;bar&apos;)); // hi, Amy 上述例子中,只有通过make方法获取实例的时候,实例才被创建,使得我们可以实现容器。","categories":[{"name":"PHP","slug":"PHP","permalink":"https://blog.zhimma.com/categories/PHP/"}],"tags":[{"name":"闭包/匿名函数","slug":"闭包-匿名函数","permalink":"https://blog.zhimma.com/tags/闭包-匿名函数/"}]},{"title":"PhpStrom中安装CodeSniffer","slug":"PhpStrom中安装CodeSniffer","date":"2018-11-29T16:00:00.000Z","updated":"2019-02-01T07:36:30.294Z","comments":true,"path":"2018/11/30/PhpStrom中安装CodeSniffer/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/PhpStrom中安装CodeSniffer/","excerpt":"环境 ： windows10 本地开发环境 ：phpStudy Phpstrom版本：Phpstrom 2017.1","text":"环境 ： windows10 本地开发环境 ：phpStudy Phpstrom版本：Phpstrom 2017.1 Phpstrom Phpstorm是JetBrains 公司开发的跨平台的PHP IDE。在用Phpstorm编辑PHP，HTML和JavaScript的代码时，Phpstorm提供实施代码分析，错误提示和自动格式化等功能。支持的PHP版本包括5.3, 5.4, 5.5, 5.6 和 7.0。一款便携又强大的IDE，很多功能能够帮助你更好的进行开发。 PHP CodeSniffer PHP CodeSniffer是PEAR中的一个用PHP5写的一个PHP的代码风格检测器，它根据预先设定好的PHP编码风格和规则，去检查应用中的代码风格情况是否有违反一组预先设置好的编码标准，内置了ZEND，PEAR的编码风格规则，当然也支持自己定制。PHP CodeSniffer 是确保代码简洁一致的必不可少的开发工具，甚至还可以帮助程序员减少一些语义错误。 安装 PHP扩展安装 下载PEAR文件 切换到PHP软件目录，运行php go-pear.phar 按照提示指令输入，完成安装 装PHP Code Snifferpear install PHP_CodeSniffer 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263E:\\phpStudy\\php\\php-5.6.27-nts&gt;php go-pear.pharAre you installing a system-wide PEAR or a local copy?(system|local) [system] :Below is a suggested file layout for your new PEAR installation. Tochange individual locations, type the number in front of thedirectory. Type &apos;all&apos; to change all of them or simply press Enter toaccept these locations. 1. Installation base ($prefix) : E:\\phpStudy\\php\\php-5.6.27-nts 2. Temporary directory for processing : E:\\phpStudy\\php\\php-5.6.27-nts\\tmp 3. Temporary directory for downloads : E:\\phpStudy\\php\\php-5.6.27-nts\\tmp 4. Binaries directory : E:\\phpStudy\\php\\php-5.6.27-nts 5. PHP code directory ($php_dir) : E:\\phpStudy\\php\\php-5.6.27-nts\\pear 6. Documentation directory : E:\\phpStudy\\php\\php-5.6.27-nts\\docs 7. Data directory : E:\\phpStudy\\php\\php-5.6.27-nts\\data 8. User-modifiable configuration files directory : E:\\phpStudy\\php\\php-5.6.27-nts\\cfg 9. Public Web Files directory : E:\\phpStudy\\php\\php-5.6.27-nts\\www10. System manual pages directory : E:\\phpStudy\\php\\php-5.6.27-nts\\man11. Tests directory : E:\\phpStudy\\php\\php-5.6.27-nts\\tests12. Name of configuration file : C:\\Windows\\pear.ini13. Path to CLI php.exe : E:\\phpStudy\\php\\php-5.6.27-nts1-13, &apos;all&apos; or Enter to continue:Beginning install...Configuration written to C:\\Windows\\pear.ini...Initialized registry...Preparing to install...installing phar://E:/phpStudy/php/php-5.6.27-nts/go-pear.phar/PEAR/go-pear-tarballs/Archive_Tar-1.4.2.tar...installing phar://E:/phpStudy/php/php-5.6.27-nts/go-pear.phar/PEAR/go-pear-tarballs/Console_Getopt-1.4.1.tar...installing phar://E:/phpStudy/php/php-5.6.27-nts/go-pear.phar/PEAR/go-pear-tarballs/PEAR-1.10.3.tar...installing phar://E:/phpStudy/php/php-5.6.27-nts/go-pear.phar/PEAR/go-pear-tarballs/Structures_Graph-1.1.1.tar...installing phar://E:/phpStudy/php/php-5.6.27-nts/go-pear.phar/PEAR/go-pear-tarballs/XML_Util-1.4.2.tar...install ok: channel://pear.php.net/Archive_Tar-1.4.2install ok: channel://pear.php.net/Console_Getopt-1.4.1install ok: channel://pear.php.net/Structures_Graph-1.1.1install ok: channel://pear.php.net/XML_Util-1.4.2install ok: channel://pear.php.net/PEAR-1.10.3PEAR: Optional feature webinstaller available (PEAR&apos;s web-based installer)PEAR: Optional feature gtkinstaller available (PEAR&apos;s PHP-GTK-based installer)PEAR: Optional feature gtk2installer available (PEAR&apos;s PHP-GTK2-based installer)PEAR: To install optional features use &quot;pear install pear/PEAR#featurename&quot;** WARNING! Old version found at E:\\phpStudy\\php\\php-5.6.27-nts, please remove it or be sure to use the new e:\\phpstudy\\php\\php-5.6.27-nts\\pear.bat commandThe &apos;pear&apos; command is now at your service at e:\\phpstudy\\php\\php-5.6.27-nts\\pear.bat* WINDOWS ENVIRONMENT VARIABLES *For convenience, a REG file is available under E:\\phpStudy\\php\\php-5.6.27-ntsPEAR_ENV.reg .This file creates ENV variables for the current user.Double-click this file to add it to the current user registry.E:\\phpStudy\\php\\php-5.6.27-nts&gt;pear install PHP_CodeSnifferWARNING: channel &quot;pear.php.net&quot; has updated its protocols, use &quot;pear channel-update pear.php.net&quot; to updatedownloading PHP_CodeSniffer-2.8.1.tgz ...Starting to download PHP_CodeSniffer-2.8.1.tgz (522,712 bytes).........................................................................................................done: 522,712 bytesinstall ok: channel://pear.php.net/PHP_CodeSniffer-2.8.1 Phpstrom配置 下载对应的标准到Standard（E:\\phpStudy\\php\\php-5.6.27-nts\\pear\\PHP\\CodeSniffer\\Standards）目录下面 这里我省略了这一步 打开Phpstrom设置,依次打开Setting-&gt;Languages and Frameworks-&gt;PHP-&gt;Code Sniffer，按照下图操作 Standard配置，按照下图操作 最好保存即可预览效果","categories":[{"name":"PHP","slug":"PHP","permalink":"https://blog.zhimma.com/categories/PHP/"},{"name":"PhpStorm","slug":"PhpStorm","permalink":"https://blog.zhimma.com/categories/PhpStorm/"}],"tags":[{"name":"PhpStorm","slug":"PhpStorm","permalink":"https://blog.zhimma.com/tags/PhpStorm/"}]},{"title":"PhpStorm中使用PSR2编码规范phpcbf脚本自动修正代码格式","slug":"PhpStorm中使用PSR2编码规范phpcbf脚本自动修正代码格式","date":"2018-11-29T16:00:00.000Z","updated":"2019-02-01T07:36:21.174Z","comments":true,"path":"2018/11/30/PhpStorm中使用PSR2编码规范phpcbf脚本自动修正代码格式/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/PhpStorm中使用PSR2编码规范phpcbf脚本自动修正代码格式/","excerpt":"","text":"安装CodeSniffer安装CodeSniffer1brew install php-code-sniffer 安装完成后的路径:/usr/local/Cellar/php-code-sniffer 123456&gt; ☁ bin pwd&gt; /usr/local/Cellar/php-code-sniffer/3.3.1/bin&gt; ☁ bin ls&gt; phpcbf phpcs&gt; &gt; 配置phpcbf12./phpcs --config-set default_standard PSR2./phpcbf --config-set default_standard PSR2 123&gt;☁ bin ls&gt;CodeSniffer.conf phpcbf phpcs&gt; 配置PhpStorm基本配置 打开PhpStorm的设置页（File-&gt;Setting或者Command+,），到Editor/Code Style页PHP中选择风格为 PSR1/2 设置Code Sniffer 选择之前phpcs的路径，填写后可以点击Validate按钮验证 现在使用PhpStorm的格式化，将会自动格式化成psr-2的风格 参考地址","categories":[{"name":"PHP","slug":"PHP","permalink":"https://blog.zhimma.com/categories/PHP/"},{"name":"PhpStorm","slug":"PhpStorm","permalink":"https://blog.zhimma.com/categories/PhpStorm/"}],"tags":[{"name":"PhpStorm","slug":"PhpStorm","permalink":"https://blog.zhimma.com/tags/PhpStorm/"}]},{"title":"PHP中匿名函数和闭包初探","slug":"PHP中匿名函数和闭包初探","date":"2018-11-29T16:00:00.000Z","updated":"2019-01-24T09:18:28.664Z","comments":true,"path":"2018/11/30/PHP中匿名函数和闭包初探/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/PHP中匿名函数和闭包初探/","excerpt":"「闭包」和「匿名」的区别首先，虽然闭包和匿名在PHP中对应的都是一个东西，但是闭包和匿名并不等价。 匿名是指这个函数可以想变量一样操作，例如可以赋值给一个变量或者作为参数传递，作为函数的返回值等。 闭包则是指这个函数可以从上下文中捕获变量（不是通过传参获取），例如PHP使用use这个子句来完成这个操作； 实际上，闭包和匿名函数是伪装成函数的对象。他们是Closure类的实例。闭包和字符串、整数一样，是一等值类型","text":"「闭包」和「匿名」的区别首先，虽然闭包和匿名在PHP中对应的都是一个东西，但是闭包和匿名并不等价。 匿名是指这个函数可以想变量一样操作，例如可以赋值给一个变量或者作为参数传递，作为函数的返回值等。 闭包则是指这个函数可以从上下文中捕获变量（不是通过传参获取），例如PHP使用use这个子句来完成这个操作； 实际上，闭包和匿名函数是伪装成函数的对象。他们是Closure类的实例。闭包和字符串、整数一样，是一等值类型 使用举例 提到闭包就不得不想起匿名函数，也叫闭包函数（closures），貌似PHP闭包实现主要就是靠它.所以，在PHP中闭包（Closure）就是匿名函数; 声明一个匿名函数12$func = function()&#123;&#125;; 可以看到，匿名函数因为没有名字，如果要使用它，需要将其返回给一个变量。匿名函数也像普通函数一样可以声明参数，调用方法也相同：1234567$message = function($name)&#123; echo &apos;hello &apos;.$name;&#125;;$message(&apos;world&apos;);//输出hello world 通常会把闭包当做函数的回调来使用 我们之所以能调用$message变量，是因为这个变量的值是一个闭包，而且闭包对象实现了invoke()魔术方法。只要变量名后有(),PHP就会查找并调用invoke()方法。 array_map(), preg_replace_callback()方法都会用到回调函数，这是使用闭包的最佳时机！ 12345$numbersPlusOne = array_map(function ($number) &#123; return $number + 1;&#125;, [1, 2, 3]);print_r($numbersPlusOne);//输出[2, 3, 4] use关键字1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;?php// 一个基本的购物车，包括一些已经添加的商品和每种商品的数量。// 其中有一个方法用来计算购物车中所有商品的总价格。该方法使用了一个closure作为回调函数。class Cart&#123; const PRICE_BUTTER = 1.00; const PRICE_MILK = 3.00; const PRICE_EGGS = 6.95; protected $products = array(); public function add($product, $quantity) &#123; $this-&gt;products[$product] = $quantity; &#125; public function getQuantity($product) &#123; return isset($this-&gt;products[$product]) ? $this-&gt;products[$product] : FALSE; &#125; public function getTotal($tax) &#123; $total = 0.00; $callback = function ($quantity, $product) use ($tax, &amp;$total) &#123; $pricePerItem = constant(__CLASS__ . &quot;::PRICE_&quot; . strtoupper($product)); $total += ($pricePerItem * $quantity) * ($tax + 1.0); &#125;; array_walk($this-&gt;products, $callback); return round($total, 2);; &#125;&#125;$my_cart = new Cart;// 往购物车里添加条目$my_cart-&gt;add(&apos;butter&apos;, 1);$my_cart-&gt;add(&apos;milk&apos;, 3);$my_cart-&gt;add(&apos;eggs&apos;, 6);// 打出出总价格，其中有 5% 的销售税.print $my_cart-&gt;getTotal(0.05) . &quot;\\n&quot;;// The result is 54.29?&gt; 匿名函数不会自动从父作用域中继承变量，注意从父作用域继承变量和使用全局变量是不同的。 如果父作用域本身就是全局的 情况下就不存在从父作用域继承变量了，如果不是全局的话，想要使用父作用域中的变量，必须在声明匿名函数时候使用use换键字 来定义继承父作用域的变量。","categories":[{"name":"PHP","slug":"PHP","permalink":"https://blog.zhimma.com/categories/PHP/"}],"tags":[{"name":"闭包/匿名函数","slug":"闭包-匿名函数","permalink":"https://blog.zhimma.com/tags/闭包-匿名函数/"}]},{"title":"MySQL中按照姓名或者中文首字母区间查询排序","slug":"mysql中按照姓名或者中文首字母区间查询排序","date":"2018-11-29T16:00:00.000Z","updated":"2019-01-24T09:14:33.002Z","comments":true,"path":"2018/11/30/mysql中按照姓名或者中文首字母区间查询排序/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/mysql中按照姓名或者中文首字母区间查询排序/","excerpt":"英文26个字母中除了i,u,v三个字母不能成为汉语拼音的首字母外,其它的字母都可以。先上一个表格,说明下各个字母字母的拼音编码的开始值和结束值","text":"英文26个字母中除了i,u,v三个字母不能成为汉语拼音的首字母外,其它的字母都可以。先上一个表格,说明下各个字母字母的拼音编码的开始值和结束值 字母 起值 止值 A 45217 45252 B 45253 45760 C 45761 46317 D 46318 46825 E 46826 47009 F 47010 47296 G 47297 47613 H 47614 48118 J 48119 49061 K 49062 49323 L 49324 49895 M 49896 50370 N 50371 50613 O 50614 50621 P 50622 50905 Q 50906 51386 R 51387 51445 S 51446 52217 T 52218 52697 W 52698 52979 X 52980 53688 Y 53689 54480 Z 54481 55289 用途：假如需要查询数据库中 以A-G字母开头的省,并且按照字母顺序排序：数据库中数据 需要的结果 类似这种结构或者这种数据排序,我们直接可以执行下面的sql语句查询 select * from wr_province where CONV(HEX(left(CONVERT(province_name USING gbk),1)),16,10) between 45217 and 47613 ORDER BY convert(province_name USING gbk) asc; 下面有一个我在自己项目中使用的例子,用的是循环查询,大家可以看看结果 1234567891011121314151617$sort = array( &apos;A-G&apos; =&gt; array(&apos;45217&apos;, &apos;47613&apos;), &apos;H-K&apos; =&gt; array(&apos;47614&apos;, &apos;49323&apos;), &apos;L-S&apos; =&gt; array(&apos;49324&apos;, &apos;52217&apos;), &apos;T-Z&apos; =&gt; array(&apos;52218&apos;, &apos;55289&apos;));$model = D(&apos;Province&apos;);$array = array();foreach ($sort as $key =&gt; $value)&#123; $sql = &quot;select * from wr_province where CONV(HEX(left(CONVERT(province_name USING gbk),1)),16,10) between &#123;$value[0]&#125; and &#123;$value[1]&#125; ORDER BY convert(province_name USING gbk) asc&quot;; $data = $model-&gt;query($sql); foreach ($data as $value)&#123; $array[$key][] = array(&apos;code&apos;=&gt;$value[&apos;province_id&apos;],&apos;address&apos;=&gt;$value[&apos;province_name&apos;]); &#125;&#125;debug(json_encode($array));//结果： 满足需求;","categories":[{"name":"Mysql","slug":"Mysql","permalink":"https://blog.zhimma.com/categories/Mysql/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://blog.zhimma.com/tags/Mysql/"}]},{"title":"SSH 密钥创建及密钥登录","slug":"SSH 密钥创建及密钥登录","date":"2018-11-29T16:00:00.000Z","updated":"2019-01-24T09:20:12.972Z","comments":true,"path":"2018/11/30/SSH 密钥创建及密钥登录/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/SSH 密钥创建及密钥登录/","excerpt":"","text":"SSH 密钥创建及密钥登录本文基本引自这里：https://blog.csdn.net/nahancy/article/details/79059135 在我们平时使用Linux系统时候，通常使用的Linux SSH登录方式是用户名加密码的登录方式，今天来探讨另外的一种相对安全的登录方式——密钥登录 我们知道SSH登录是用的RSA非对称加密的，所以我们在SSH登录的时候就可以使用RSA密钥登录，SSH有专门创建SSH密钥的工具ssh-keygen，下面就来一睹风采。 首先进入Linux系统的用户目录下的.ssh目录下，root用户是/root/.ssh，普通用户是/home/您的用户名/.ssh，我们以root用户为例 123[root@b04f945297ac .ssh]# cd /root/.ssh/[root@b04f945297ac .ssh]# lsknown_hosts 上面是没有创建过ssh秘钥的样子 执行ssh-keygen命令创建密钥对： 123456789101112131415161718192021[root@b04f945297ac .ssh]# ssh-keygen -t rsa -b 4096Generating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): /root/.ssh/zhimma_id_rsaEnter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/zhimma_id_rsa.Your public key has been saved in /root/.ssh/zhimma_id_rsa.pub.The key fingerprint is:SHA256:WrIN4U/dk+1KdTxv0t5zFeXCeWWpvaNtFFMxbkDY2Cw root@b04f945297acThe key&apos;s randomart image is:+---[RSA 4096]----+| Bo oo|| E +o.*|| . ..o*+|| . . . . +=*o|| + S . + o+B|| X + *+|| o o . * *|| . o Bo|| . . =|+----[SHA256]-----+ -b 参数，指定了长度，也可以不加-b参数，直接使用ssh-keygen -t rsa 这里我重新命名了下秘钥文件名 zhimma_id_rsa 密钥生成后会在当前目录下多出两个文件，zhimma_id_rsa和zhimma_id_rsa.pub，其中zhimma_id_rsa是私钥（敲黑板：这个很重要，不能外泄），zhimma_id_rsa.pub这个是公钥 123[root@b04f945297ac .ssh]# lsknown_hosts zhimma_id_rsa zhimma_id_rsa.pub[root@b04f945297ac .ssh]# 放置公钥ssh-copy-id把公钥拷贝到需要登录的远程服务器或Linux系统上，这里可以使用ssh-copy-id ssh-copy-id默认端口是22，如果要重新指定端口，则使用-p 端口号命令 12345678910[root@b04f945297ac .ssh]# ssh-copy-id -i /root/.ssh/zhimma_id_rsa.pub -p 203 root@192.168.2.107 /usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/zhimma_id_rsa.pub&quot;/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keysroot@192.168.2.107&apos;s password: Number of key(s) added: 1Now try logging into the machine, with: &quot;ssh -p &apos;203&apos; &apos;root@192.168.2.107&apos;&quot;and check to make sure that only the key(s) you wanted were added. 这样就把公钥加的指定的服务器上了 尝试登陆下 123[root@b04f945297ac .ssh]# ssh -i /root/.ssh/zhimma_id_rsa -p 203 root@192.168.2.107Last login: Wed Jun 6 04:16:09 2018 from 172.17.0.1[root@e91b4a662023 ~]# 登陆成功！ 手动放置进入远程服务器.ssh目录，创建authorized_keys 文件，赋权限600 123456789[root@01bb4850cc8c .ssh]# touch authorized_keys[root@01bb4850cc8c .ssh]# chmod -R 600 authorized_keys [root@01bb4850cc8c .ssh]# ls -altotal 16drwx------ 1 root root 4096 Jun 6 07:02 .dr-xr-x--- 1 root root 4096 Apr 27 08:15 ..-rw------- 1 root root 0 Jun 6 07:02 authorized_keysdrwxr-xr-x 2 root root 4096 Jun 6 07:02 dd-rw-r--r-- 1 root root 824 Mar 16 08:59 known_hosts 复制zhimma_id_rsa.pub的内容进入这个文件 尝试登陆 123[root@b04f945297ac .ssh]# ssh -i /root/.ssh/zhimma_id_rsa -p 204 root@192.168.2.107Last login: Wed Jun 6 04:18:46 2018 from 172.17.0.1[root@01bb4850cc8c ~]# 登陆成功！ ssh-keygen可用的参数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455ssh-keygen可用的参数选项有： -a trials 在使用 -T 对 DH-GEX 候选素数进行安全筛选时需要执行的基本测试数量。 -B 显示指定的公钥/私钥文件的 bubblebabble 摘要。 -b bits 指定密钥长度。对于RSA密钥，最小要求768位，默认是2048位。DSA密钥必须恰好是1024位(FIPS 186-2 标准的要求)。 -C comment 提供一个新注释 -c 要求修改私钥和公钥文件中的注释。本选项只支持 RSA1 密钥。 程序将提示输入私钥文件名、密语(如果存在)、新注释。 -D reader 下载存储在智能卡 reader 里的 RSA 公钥。 -e 读取OpenSSH的私钥或公钥文件，并以 RFC 4716 SSH 公钥文件格式在 stdout 上显示出来。 该选项能够为多种商业版本的 SSH 输出密钥。 -F hostname 在 known_hosts 文件中搜索指定的 hostname ，并列出所有的匹配项。 这个选项主要用于查找散列过的主机名/ip地址，还可以和 -H 选项联用打印找到的公钥的散列值。 -f filename 指定密钥文件名。 -G output_file 为 DH-GEX 产生候选素数。这些素数必须在使用之前使用 -T 选项进行安全筛选。 -g 在使用 -r 打印指纹资源记录的时候使用通用的 DNS 格式。 -H 对 known_hosts 文件进行散列计算。这将把文件中的所有主机名/ip地址替换为相应的散列值。 原来文件的内容将会添加一个&quot;.old&quot;后缀后保存。这些散列值只能被 ssh 和 sshd 使用。 这个选项不会修改已经经过散列的主机名/ip地址，因此可以在部分公钥已经散列过的文件上安全使用。 -i 读取未加密的SSH-2兼容的私钥/公钥文件，然后在 stdout 显示OpenSSH兼容的私钥/公钥。 该选项主要用于从多种商业版本的SSH中导入密钥。 -l 显示公钥文件的指纹数据。它也支持 RSA1 的私钥。 对于RSA和DSA密钥，将会寻找对应的公钥文件，然后显示其指纹数据。 -M memory 指定在生成 DH-GEXS 候选素数的时候最大内存用量(MB)。 -N new_passphrase 提供一个新的密语。 -P passphrase 提供(旧)密语。 -p 要求改变某私钥文件的密语而不重建私钥。程序将提示输入私钥文件名、原来的密语、以及两次输入新密语。 -q 安静模式。用于在 /etc/rc 中创建新密钥的时候。 -R hostname 从 known_hosts 文件中删除所有属于 hostname 的密钥。 这个选项主要用于删除经过散列的主机(参见 -H 选项)的密钥。 -r hostname 打印名为 hostname 的公钥文件的 SSHFP 指纹资源记录。 -S start 指定在生成 DH-GEX 候选模数时的起始点(16进制)。 -T output_file 测试 Diffie-Hellman group exchange 候选素数(由 -G 选项生成)的安全性。 -t type 指定要创建的密钥类型。可以使用：&quot;rsa1&quot;(SSH-1) &quot;rsa&quot;(SSH-2) &quot;dsa&quot;(SSH-2) -U reader 把现存的RSA私钥上传到智能卡 reader -v 详细模式。ssh-keygen 将会输出处理过程的详细调试信息。常用于调试模数的产生过程。 重复使用多个 -v 选项将会增加信息的详细程度(最大3次)。 -W generator 指定在为 DH-GEX 测试候选模数时想要使用的 generator -y 读取OpenSSH专有格式的公钥文件，并将OpenSSH公钥显示在 stdout 上。 ###","categories":[{"name":"SSH","slug":"SSH","permalink":"https://blog.zhimma.com/categories/SSH/"}],"tags":[{"name":"ssh","slug":"ssh","permalink":"https://blog.zhimma.com/tags/ssh/"}]},{"title":"Mac安装Swoole扩展问题记录","slug":"mac安装swoole扩展问题记录","date":"2018-11-29T16:00:00.000Z","updated":"2019-10-22T02:06:14.287Z","comments":true,"path":"2018/11/30/mac安装swoole扩展问题记录/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/mac安装swoole扩展问题记录/","excerpt":"","text":"[TOC] openssl/ssl.h’ file not found`/private/tmp/pear/temp/swoole/include/swoole.h:438:10: fatal error: &#39;openssl/ssl.h&#39; file not found 原因：openssl 未安装或 openssl 库不在标准位置中解决方案： 确认是否安装了 opensslbrew search openssl若未安装则执行命令brew install openssl进行安装 确认 openssl 库是否在标准位置中 12☁ ~ ls /usr/local/include/openssl☁ ~ No such file or directory 这就是问题所在了，找到 openssl/include/openssl 目录，并 cp 到 /usr/local/include 目录中。 ☁ ~ ln -s /usr/local/Cellar/openssl/1.0.2p/include/openssl/ /usr/local/include/ 一般情况下就可以解决该问题了。 但是，也可能会遇到很诡异的状况，上步没有解决问题，依然找不到 openssl/ssl.h 等文件复制 openssl 源文件到 swoole 的源码目录中，编译就可以了。 cp -R /usr/local/Cellar/openssl/1.0.2p/include/openssl swoole-src-2.1.3/include 这个肯定能解决问题了 Enable openssl support, require openssl library./private/tmp/pear/temp/swoole/php_swoole.h:137:2: error: &quot;Enable openssl support, require openssl library.&quot; 上面的问题解决了，再次pecl install swoole时候报了这个错，找了很久都么有找到解决方案，偶然看到pecl 安装的过程： 123456789enable debug/trace log support? [no] : yesenable sockets supports? [no] : yesenable openssl support? [no] : yesenable http2 support? [no] : yesenable async-redis support? [no] : yesenable mysqlnd support? [no] : yesenable postgresql coroutine client support? [no] : nobuilding in /private/tmp/pear/temp/pear-build-zhimmaSwGIQ1/swoole-4.2.1running: /private/tmp/pear/temp/swoole/configure --with-php-config=/usr/local/opt/php@7.1/bin/php-config --enable-debug-log=yes --enable-sockets=yes --enable-openssl=yes --enable-http2=yes --enable-async-redis=yes --enable-mysqlnd=yes --enable-coroutine-postgresql=no ，于是猜想如果指定openssl的目录，是否可以解决，先看看openssl目录： 12☁ ~ which openssl/usr/local/opt/openssl/bin/openssl 于是安装过程就变成下面： 123456789enable debug/trace log support? [no] : yesenable sockets supports? [no] : yesenable openssl support? [no] : yes --with-openssl-dir=/usr/local/opt/openssl/bin/opensslenable http2 support? [no] : yesenable async-redis support? [no] : yesenable mysqlnd support? [no] : yesenable postgresql coroutine client support? [no] : nobuilding in /private/tmp/pear/temp/pear-build-zhimmaN9CyFV/swoole-4.2.1running: /private/tmp/pear/temp/swoole/configure --with-php-config=/usr/local/opt/php@7.1/bin/php-config --enable-debug-log=yes --enable-sockets=yes --enable-openssl=yes --with-openssl-dir=/usr/local/opt/openssl/bin/openssl --enable-http2=yes --enable-async-redis=yes --enable-mysqlnd=yes --enable-coroutine-postgresql=no 错误解决 Enable http2 support, require nghttp2 library./private/tmp/pear/temp/swoole/php_swoole.h:148:2: error: &quot;Enable http2 support, require nghttp2 library. 解决方案： brew install nghttp2 未能解决 hiredis/hiredis.h&#39; file not found/private/tmp/pear/temp/swoole/swoole_redis.c:20:10: fatal error: hiredis/hiredis.h&#39; file not found 解决方案： brew install hiredis 编译参数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455--disable-option-checking ignore unrecognized --enable/--with options --disable-FEATURE do not include FEATURE (same as --enable-FEATURE=no) --enable-FEATURE[=ARG] include FEATURE [ARG=yes] --with-PACKAGE[=ARG] use PACKAGE [ARG=yes] --without-PACKAGE do not use PACKAGE (same as --with-PACKAGE=no) --with-libdir=NAME Look for libraries in .../NAME rather than .../lib --with-php-config=PATH Path to php-config php-config --enable-swoole-debug Enable swoole debug 启用swoole的调试日志。不要在生产环境中启用此配置。 --enable-trace-log Enable swoole trace log --enable-sockets Do you have sockets extension? 启用对 sockets 的支持。依赖 sockets --enable-async-redis Do you have hiredis? 启用对异步Redis客户端的支持。依赖 hiredis --enable-coroutine-postgresql Do you install postgresql? 启用协程 Postgresql 客户端，依赖 libpq --enable-openssl Use openssl? 启用openssl支持。依赖 libssl.so --enable-http2 Use http2.0? 启用HTTP2的支持。依赖 nghttp2 --enable-thread Experimental: Use thread? 启用线程支持 //实验性功能。请勿在生产环境中使用此功能 --enable-hugepage Experimental: Use hugepage? 启用 hugepage //使用大内存页优化性能，具体鸟哥在他的博客中讲到。 如果已经开启了 jemalloc，再开启hugepage 印象性能 https://blog.digitalocean.com/transparent-huge-pages-and-alternative-memory-allocators/ 实验性功能。请勿在生产环境中使用此功能 --enable-swoole Enable swoole support --enable-swoole-static Enable swoole static compile support --with-swoole With swoole support --with-libpq-dir=DIR Include libpq support (requires libpq &gt;= 9.5) --with-openssl-dir=DIR Include OpenSSL support (requires OpenSSL &gt;= 0.9.6) 设置openssl库的路径，例如：--with-openssl-dir=/opt/openssl/. --with-jemalloc-dir=DIR Include jemalloc support 使用 jemalloc 进行内存优化支持 --enable-mysqlnd Do you have mysqlnd? 启用对 mysqlnd 的支持，依赖 mysqlnd --enable-coroutine Enable coroutine (requires PHP &gt;= 5.5) 启用协程 --enable-asan Enable asan 启用 Address-Sanitizier 内存检测工具 //只有开启debug才有效 --enable-picohttpparser Experimental: Do you have picohttpparser? 启用 picohttpparser 支持 //这是一个超高性能的http解析器，实验性功能。请勿在生产环境中使用此功能 --enable-timewheel Experimental: Enable timewheel heartbeat? 启用时间轮算法并优化心跳算法 //实验性功能。请勿在生产环境中使用此功能 --enable-debug, compile with debug symbols 编译时加入符号表 //使用gdb调试时有用 --enable-shared=PKGS Build shared libraries default=yes --enable-static=PKGS Build static libraries default=yes --enable-fast-install=PKGS Optimize for fast installation default=yes --with-gnu-ld Assume the C compiler uses GNU ld default=no --disable-libtool-lock Avoid locking (might break parallel builds) --with-pic Try to use only PIC/non-PIC objects default=use both --with-tags=TAGS Include additional configurations automatic centos 安装swoole 扩展__builtin_saddl_overflow __builtin_saddl_overflow’ was not declared in this scope这是一个已知的问题 - 问题是 CentOS 上的默认 gcc 缺少必需的定义，即使在升级 gcc 之后，PECL 也会找到旧的编译器。要安装驱动程序，必须首先通过安装 devtoolset 集合来升级 gcc，如下所示： sudo yum install centos-release-scl sudo yum install devtoolset-7 scl enable devtoolset-7 bash","categories":[{"name":"Mac","slug":"Mac","permalink":"https://blog.zhimma.com/categories/Mac/"},{"name":"Swoole","slug":"Swoole","permalink":"https://blog.zhimma.com/categories/Swoole/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"https://blog.zhimma.com/tags/Mac/"},{"name":"Swoole","slug":"Swoole","permalink":"https://blog.zhimma.com/tags/Swoole/"}]},{"title":"PHP-FPM如何合理设置max_chindren和pm模式，包括开启status监听","slug":"php-fpm如何合理设置max_chindren和pm模式，包括开启status监听","date":"2018-11-29T16:00:00.000Z","updated":"2019-02-01T07:36:06.082Z","comments":true,"path":"2018/11/30/php-fpm如何合理设置max_chindren和pm模式，包括开启status监听/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/php-fpm如何合理设置max_chindren和pm模式，包括开启status监听/","excerpt":"","text":"php-fpm如何合理设置max_chindren和pm模式，包括开启status监听开启status获得执行状态启用php-fpm状态功能123vi /etc/php-fpm.d/www.conf修改：pm.status_path = /status nginx配置1234567891011server &#123; . . . location ~ ^/status$ &#123; include fastcgi_params; fastcgi_pass 127.0.0.1:9000; fastcgi_param SCRIPT_FILENAME $fastcgi_script_name; &#125;&#125; 重启nginx和php-fpm打开status页面123456789101112131415[root@b04f945297ac ~]# curl http://visit.ma/statuspool: wwwprocess manager: dynamicstart time: 23/Jul/2018:15:32:09 +0800start since: 1055accepted conn: 9listen queue: 0max listen queue: 0listen queue len: 128idle processes: 5active processes: 1total processes: 6max active processes: 1max children reached: 0slow requests: 0 参数详解pool – fpm池子名称，大多数为wwwprocess manager – 进程管理方式,值：static, dynamic or ondemand. dynamicstart time – 启动日期,如果reload了php-fpm，时间会更新start since – 运行时长accepted conn – 当前池子接受的请求数listen queue – 请求等待队列，如果这个值不为0，那么要增加FPM的进程数量max listen queue – 请求等待队列最高的数量listen queue len – socket等待队列长度idle processes – 空闲进程数量active processes – 活跃进程数量total processes – 总进程数量max active processes – 最大的活跃进程数量（FPM启动开始算）max children reached - 大道进程最大数量限制的次数，如果这个数量不为0，那说明你的最大进程数量太小了，请改大一点。slow requests – 启用了php-fpm slow-log，缓慢请求的数量 php-fpm其他参数php-fpm状态页比较个性化的一个地方是它可以带参数，可以带参数json、xml、html并且前面三个参数可以分别和full做一个组合 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849curl http://visit.ma/status?jsoncurl http://visit.ma/status?xmlcurl http://visit.ma/status?htmlcurl http://visit.ma/status?full[root@b04f945297ac ~]# curl http://visit.ma/status?fullpool: wwwprocess manager: dynamicstart time: 23/Jul/2018:15:32:09 +0800start since: 1240accepted conn: 10listen queue: 0max listen queue: 0listen queue len: 128idle processes: 5active processes: 1total processes: 6max active processes: 1max children reached: 0slow requests: 0************************pid: 5466state: Idlestart time: 23/Jul/2018:15:32:09 +0800start since: 1240requests: 2request duration: 99request method: GETrequest URI: /statuscontent length: 0user: -script: -last request cpu: 0.00last request memory: 2097152************************pid: 5467state: Idlestart time: 23/Jul/2018:15:32:09 +0800start since: 1240requests: 2request duration: 184request method: GETrequest URI: /status?fullcontent length: 0user: -script: -last request cpu: 0.00last request memory: 2097152 这里重点说下full参数详解 pid – 进程PID，可以单独kill这个进程. You can use this PID to kill a long running process.state – 当前进程的状态 (Idle, Running, …)start time – 进程启动的日期start since – 当前进程运行时长requests – 当前进程处理了多少个请求request duration – 请求时长（微妙）request method – 请求方法 (GET, POST, …)request URI – 请求URIcontent length – 请求内容长度 (仅用于 POST)user – 用户 (PHP_AUTH_USER) (or ‘-’ 如果没设置)script – PHP脚本 (or ‘-’ if not set)last request cpu – 最后一个请求CPU使用率last request memorythe - 上一个请求使用的内存 合理设置max_chindren和pm模式 使用htop命令查看单个php-fpm所申请的VIRT大小，我32G服务器是400左右（实际要除以8=M，就是：50M左右），如果按照每个进程消耗50M*1.5倍=75M左右，如果你的服务器内存是32G，我们假设可用于php-fpm的内存为60%=20G，则：20*1024/75=273，所以，一般我们建议max_chindren最大为273，最好还是设置为：8的倍数，所以我设置为256. 然后我们可以根据域名/status的结果来合理设置其他参数（pm.start_servers和pm.min_spare_servers和pm.max_spare_servers） 在php.ini中，我们可以看到memory_limit有一句这样的原文，Maximum amount of memory a script may consume (128MB)，就是说单个进程使用的最大内存大小，这个参数吧，当然不能低于刚刚计算的75M了，一般我们可以设置为3倍，则75*3=225M左右（建议：128，256，512，1024…） 这里假如有攻击的话，max_chindren=256，memory_limit=256，256*256=64G，很明显会导致内存爆满，所以如果想又保持性能，又能一定程度上防止内存爆满，可以将max_chindren设置的低一点，memory_limit可以设置为每个进程消耗的值（一般不建议低于128M吧，如果是独立服务器的话）。 PHP-FPM 子进程数量，是不是越多越好？当然不是，pm.max_chindren，进程多了，增加进程管理的开销以及上下文切换的开销。 更核心的是，能并发执行的 php-fpm 进程不会超过 cpu 个数。 如何设置，取决于你的代码 如果代码是 CPU 计算密集型的，pm.max_chindren 不能超过 CPU 的内核数。 如果不是，那么将 pm.max_chindren 的值大于 CPU 的内核数，是非常明智的。国外技术大拿给出这么个公式： 在 N + 20% 和 M/m 之间。 N 是 CPU 内核数量。M 是 PHP 能利用的内存数量。m 是每个 PHP 进程平均使用的内存数量。适用于 dynamic 方式。 static方式：M/(m * 1.2) 当然，还有一种保险的方式，来配置 max_children。适用于 static 方式。 先把 max_childnren 设置成一个比较大的值。稳定运行一段时间后，观察 php-fpm 的 status 里的 maxactive processes 是多少然后把 max_children 配置比它大一些就可以了。pm.max_requests：指的是每个子进程在处理了多少个请求数量之后就重启。 这个参数，理论上可以随便设置，但是为了预防内存泄漏的风险，还是设置一个合理的数比较好 所以，我的服务器32G内存设置为： 123456memory_limit = 256Mpm = dynamicpm.max_children = 256pm.start_servers = 32pm.min_spare_servers = 16pm.max_spare_servers = 32 pm.max_children：静态方式下开启的php-fpm进程数量。 pm.start_servers：动态方式下的起始php-fpm进程数量。 pm.min_spare_servers：动态方式下的最小php-fpm进程数量。 pm.max_spare_servers：动态方式下的最大php-fpm进程数量。 如果dm设置为static，那么其实只有pm.max_children这个参数生效。系统会开启设置数量的php-fpm进程。 如果dm设置为static，那么其实只有pm.max_children这个参数生效。系统会开启设置数量的php-fpm进程。 http://www.zhanghongliang.com/article/1300 http://www.ttlsa.com/php/use-php-fpm-status-page-detail/","categories":[{"name":"Nginx","slug":"Nginx","permalink":"https://blog.zhimma.com/categories/Nginx/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://blog.zhimma.com/tags/Nginx/"},{"name":"PHP-FPM","slug":"PHP-FPM","permalink":"https://blog.zhimma.com/tags/PHP-FPM/"}]},{"title":"sea.js中加载layer，缺少css文件解决","slug":"sea.js中加载layer，缺少css文件解决","date":"2018-11-29T16:00:00.000Z","updated":"2019-01-24T09:19:23.896Z","comments":true,"path":"2018/11/30/sea.js中加载layer，缺少css文件解决/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/sea.js中加载layer，缺少css文件解决/","excerpt":"今天做项目时遇到了一个问题，看下图页面错乱，询问前端工程师后，定位问题应该是缺少layer.css导致的，这就很奇怪了，之前的项目使用layer的时候，也只是单单的引入jquery和layer.min.js为什么现在这个新项目就不行了呢？ 答案就是：新项目使用sea.js进行js文件加载，可能内部的路径问题导致layer.js无法加载到自己的css文件，在网上查找一番之后，给出如下解决办法：","text":"今天做项目时遇到了一个问题，看下图页面错乱，询问前端工程师后，定位问题应该是缺少layer.css导致的，这就很奇怪了，之前的项目使用layer的时候，也只是单单的引入jquery和layer.min.js为什么现在这个新项目就不行了呢？ 答案就是：新项目使用sea.js进行js文件加载，可能内部的路径问题导致layer.js无法加载到自己的css文件，在网上查找一番之后，给出如下解决办法：123456789manage.js 使用的js文件define(function (require) &#123; var layer = require(&apos;layer&apos;); var paths = JSON.parse(PATH_INFO); layer.config(&#123; path: paths.remote_asset+&apos;/Common/Manage/plugins/layer/&apos; //layer.js所在的目录，可以是绝对目录，也可以是相对目录 &#125;);&#125;) 手动配置layer的路径，可以是绝对路径也可以是相对路径，只要能找得到layer的文件夹 1234567891011config.js sea.js配置文件alias: &#123; jquery: &apos;remote_asset/Manage/js/jquery.min.js&apos;, bootstrap: &apos;remote_asset/Manage/plugins/bootstrap-3.3.5/js/bootstrap.min.js&apos;, layer: &apos;remote_asset/Manage/plugins/layer/layer.min.js&apos;, validate: &apos;remote_asset/Manage/plugins/jquery-validation-1.13.1/jquery.validate.min.js&apos;, &#125;, preload: [ &apos;jquery&apos; ], 就这样，重新配置layer,让它自己能找到css文件并且加载，看看修改后的效果吧：","categories":[{"name":"js","slug":"js","permalink":"https://blog.zhimma.com/categories/js/"}],"tags":[{"name":"seajs","slug":"seajs","permalink":"https://blog.zhimma.com/tags/seajs/"}]},{"title":"Redis安装与配置","slug":"redis安装与配置","date":"2018-11-29T16:00:00.000Z","updated":"2019-01-24T09:18:48.670Z","comments":true,"path":"2018/11/30/redis安装与配置/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/redis安装与配置/","excerpt":"","text":"redis安装与配置安装 redis下载地址 解压缩tar -zxf redis-4.0.2.tar.gz cd redis-4.0.2 make cd src make install ​ 到此就安装完成。但是，由于安装redis的时候，我们没有选择安装路径，故是默认位置安装。在此，我们可以将可执行文件和配置文件移动到习惯的目录。 /usr/local 123456mkdir -p /usr/local/redis/bin mkdir -p /usr/local/redis/etc cd /usr/local/redis-4.0.2 mv ./redis.conf /usr/local/redis/etc cd src mv mkreleasehdr.sh redis-benchmark redis-check-aof redis-check-dump redis-cli redis-server redis-sentinel /usr/local/redis/bin 比较重要的3个可执行文件：redis-server：Redis服务器程序redis-cli：Redis客户端程序，它是一个命令行操作工具。也可以使用telnet根据其纯文本协议操作。redis-benchmark：Redis性能测试工具，测试Redis在你的系统及配置下的读写性能 Redis的启动命令：/usr/local/redis/bin/redis-server或cd /usr/local/redis/bin./redis-server /usr/local/redis/etc/redis.conf为redis-server指定配置文件 Redis的配置下面列举了Redis中的一些常用配置项：daemonize 如果需要将Redis服务以守护进程在后台运行，则把该项的值改为yes pidfile 配置多个pid的地址，默认在/var/run/redis/pid bind 绑定ip，设置后只接受来自该ip的请求 port 监听端口，默认是6379 timeout 客户端连接超时的设定，单位是秒 loglevel 分为4级，debug、verbose、notice、warning logfile 配置log文件地址 databases 设置数据库的个数，默认使用的数据库为0 save 设置redis进行数据库镜像的频率 rdbcompression 在进行镜像备份时，是否进行压缩 Dbfilename 镜像备份文件的文件名 Dir 数据库镜像备份文件的存放路径 Slaveof 设置数据库为其他数据库的从数据库 Masterauth 主数据库连接需要的密码验证Requirepass 设置登录时，需要使用的密码Maxclients 设置同时连接的最大客户端数量Maxmemory 设置redis能够使用的最大内存Appendonly 开启append only模式Appendfsync 设置对appendonly.aof文件同步的频率vm-enabled 是否开启虚拟内存支持vm-swap-file 设置虚拟内存的交换文件路径vm-max-memory 设置redis能够使用的最大虚拟内存vm-page-size 设置虚拟内存的页大小vm-pages 设置交换文件的总的page数量vm-max-threads 设置VMIO同时使用的线程数量Glueoutputbuf 把小的输出缓存存放在一起hash-max-zipmap-entries 设置hash的临界值Activerehashing 重新hash 修改redis的配置参数：vi /usr/local/redis/etc/redis.conf将daemonize no改为daemonize yes，保存退出。再来启动redis服务器cd /usr/local/redis/bin./redis-server /usr/local/redis/etc/redis.conf 启动redis并指定配置文件 ps aux | grep redis 查看redis是否启动成功 netstat -tlun 查看主机的6379端口是否在使用（监听） ./redis-cli 打开redis的客户端 quit 退出redis的客户端 pkill redis-server 关闭redis服务器 ./redis-cli shutdown 也可以通过这条命令关闭redis服务器 自启动只有两个步骤： 设置redis.conf中daemonize为yes,确保守护进程开启。 编写开机自启动脚本 基本原理为：系统开机启动时会去加载/etc/init.d/下面的脚本，通常而言每个脚本文件会自定义实现程序的启动；若想将新的程序开机自启动，只需在该目录下添加一个自定义启动程序的脚本，然后设置相应规则即可。如在这里我们在/etc/init.d/下新建一个 redis 的脚本，开机启动时会去加载执行该脚本。 1xxxxxxxxxx #!/bin/sh## Simple Redis init.d script conceived to work on Linux systems# as it does use of the /proc filesystem.REDISPORT=6379EXEC=/usr/local/redis/bin/redis-serverCLIEXEC=/usr/local/redis/bin/redis-cliPIDFILE=/var/run/redis_$&#123;REDISPORT&#125;.pidCONF=&quot;/usr/local/redis/etc/$&#123;REDISPORT&#125;.conf&quot;case &quot;$1&quot; in start) if [ -f $PIDFILE ] then echo &quot;$PIDFILE exists, process is already running or crashed&quot; else echo &quot;Starting Redis server...&quot; $EXEC $CONF fi ;; stop) ​ 设置可执行权限：chmod 755 redis 启动/etc/init.d/redis start 设置开机自启动:chkconfig redis on https://blog.csdn.net/qq_38158631/article/details/78644274 https://blog.csdn.net/baidu_30000217/article/details/51558408","categories":[{"name":"Redis","slug":"Redis","permalink":"https://blog.zhimma.com/categories/Redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"https://blog.zhimma.com/tags/redis/"}]},{"title":"基于Docker容器的MySQL主从配置","slug":"基于Docker容器的MySQL主从配置","date":"2018-11-29T16:00:00.000Z","updated":"2019-02-01T07:22:08.346Z","comments":true,"path":"2018/11/30/基于Docker容器的MySQL主从配置/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/基于Docker容器的MySQL主从配置/","excerpt":"","text":"基于Docker容器的MySQL主从配置我本机的镜像 1234docker imagesREPOSITORY TAG IMAGE ID ssh_network_vim_lnmp_redis_swoole_supervisor latest 6da5efb40932 环境搭建及MySQL安装步骤省略，根据已有的镜像创建容器 master数据库 docker run -it -d --privileged=true --name master -p 33060:3306 -p 220:22 -p 8080:80 -p 1024:1024 -p 16379:6379 -p 9001:9001 -v E:\\www\\:/home/www ssh_network_vim_lnmp_redis_swoole_supervisor /usr/sbin/init slave1数据库 docker run -it -d --privileged=true --name slave1 -p 33061:3306 -p 221:22 -p 8081:80 -p 2024:1024 -p 26379:6379 -p 9002:9001 -v E:\\www\\:/home/www ssh_network_vim_lnmp_redis_swoole_supervisor /usr/sbin/init slave2数据库 docker run -it -d --privileged=true --name slave2 -p 33062:3306 -p 222:22 -p 8082:80 -p 3024:1024 -p 36379:6379 -p 9003:9001 -v E:\\www\\:/home/www ssh_network_vim_lnmp_redis_swoole_supervisor /usr/sbin/init ​ …… slaveN binlog方式master数据库真实机IP：192.168.2.107 容器IP:172.17.0.2 修改MySQL配置文件(my.cnf)12log-bin=mysql-bin #启动二进制文件 server_id=1 #服务器ID 附一份全一点的配置,指定需要同步的数据库和不需要同步的数据库 12345678910[mysqld]server-id=1log-bin=mysql-binlog-bin-index=master-bin.indexbinlog_format=mixed // binlog 日志文件格式sync-binlog=1 //binlog-ignore-db=mysqlbinlog-ignore-db=productbinlog-do-db=testbinlog-do-db=local 重启MySQL； 创建复制用户12345671. CREATE USER 'zhimma'@'%' IDENTIFIED BY '123456'; GRANT REPLICATION SLAVE ON . TO 'zhimma'@'%'; 或者2. grant replication slave on *.* to 'zhimma'@'%' identified by '123456';SHOW MASTER STATUS; 查看MySQLmaster status```123456789```mysqlmysql&gt; show master status;+------------------+----------+--------------+------------------+-------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+-------------------+| mysql-bin.000001 | 998 | | | |+------------------+----------+--------------+------------------+-------------------+1 row in set (0.00 sec) 如果是项目中途使用主从复制，可以使用下面方法迁移数据 锁定所有表 12mysql&gt; FLUSH TABLES WITH READ LOCK;Query OK, 0 rows affected (2.59 sec) 备份表 1[root@b04f945297ac ~]# mysqldump -uroot -p123456 --all-databases -l -F &gt;/tmp/all_db.sql 解锁 12mysql&gt; UNLOCK TABLES; Query OK, 0 rows affected (0.00 sec) 传输数据到从库 1scp -P 221/222 /tmp/all_db.sql root@192.168.2.107:/tmp slave数据库修改server-uuid将/var/lib/mysql/auto.conf 12[auto]server-uuid=f781e2b4-28e1-11e8-a1c0-0242ac110001 修改MySQL配置文件(my.cnf)12log-bin=mysql-bin #启动二进制文件 server_id=101 #服务器ID 重启MySQL； 导入主备份文件1[root@9ae039d46474 tmp]# mysql -uroot -p123456 &lt; /tmp/all_db.sql 开启slave同步1234567CHANGE MASTER TO MASTER_HOST=&apos;192.168.2.107&apos;,MASTER_PORT=33060, MASTER_USER=&apos;zhimma&apos;, MASTER_PASSWORD=&apos;123456&apos;, MASTER_LOG_FILE=&apos;mysql-bin.000001&apos;, MASTER_LOG_POS=998;或者CHANGE MASTER TO MASTER_HOST=&apos;172.17.0.2&apos;, MASTER_USER=&apos;zhimma&apos;, MASTER_PASSWORD=&apos;123456&apos;, MASTER_LOG_FILE=&apos;mysql-bin.000001&apos;, MASTER_LOG_POS=998;start slave 查看是否同步成功slave连接master成功12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364mysql&gt; start slave;Query OK, 0 rows affected (0.43 sec)mysql&gt; show slave status\\G;*************************** 1. row *************************** Slave_IO_State: Queueing master event to the relay log Master_Host: 192.168.2.107 Master_User: zhimma Master_Port: 33060 Connect_Retry: 60 Master_Log_File: mysql-bin.000003 Read_Master_Log_Pos: 4 Relay_Log_File: 06e5a050e74b-relay-bin.000001 Relay_Log_Pos: 4 Relay_Master_Log_File: mysql-bin.000001 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 998 Relay_Log_Space: 1483 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 Master_UUID: f781e2b4-28e1-11e8-a1c0-0242ac110002 Master_Info_File: /var/lib/mysql/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.20 sec) 12Slave_IO_Running: YesSlave_SQL_Running: Yes 同步成功 http://www.cnblogs.com/clsn/p/8150036.html","categories":[{"name":"Mysql","slug":"Mysql","permalink":"https://blog.zhimma.com/categories/Mysql/"},{"name":"Docker","slug":"Docker","permalink":"https://blog.zhimma.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://blog.zhimma.com/tags/Docker/"},{"name":"Mysql","slug":"Mysql","permalink":"https://blog.zhimma.com/tags/Mysql/"}]},{"title":"数据库之主键、外键、索引","slug":"数据库之主键、外键、索引","date":"2018-11-29T16:00:00.000Z","updated":"2019-02-01T07:34:16.248Z","comments":true,"path":"2018/11/30/数据库之主键、外键、索引/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/数据库之主键、外键、索引/","excerpt":"主键 主键用来标志唯一的某一行,主键也是数据表中唯一索引,比如用户表中有id,name,但是id是唯一的,你要找出一个用户,就只能根据id去找,才能找出唯一一个,另外主键必须非空且唯一,一个表最多只能有一个主键,主键是可选的,但是为每个表指定主键通常会更好.MySQL中使用PRIMARY KEY来指定主键,","text":"主键 主键用来标志唯一的某一行,主键也是数据表中唯一索引,比如用户表中有id,name,但是id是唯一的,你要找出一个用户,就只能根据id去找,才能找出唯一一个,另外主键必须非空且唯一,一个表最多只能有一个主键,主键是可选的,但是为每个表指定主键通常会更好.MySQL中使用PRIMARY KEY来指定主键, 创建表时指定主键demo1 1234CREATE TABLE user ( id INT PRIMARY KEY auto_increment, username VARCHAR (255)); demo2 12345CREATE TABLE user ( id INT auto_increment, username VARCHAR (255), PRIMARY KEY (id)); 为已存在的表创建主键例如已存在表user(id,username),现在将字段id指定为主键并且自增 12ALTER TABLE USER ADD PRIMARY KEY (id), MODIFY id INT AUTO_INCREMENT; 删除主键12ALTER TABLE USER DROP PRIMARY KEY, MODIFY id INT; 外键 外键是用来指定参照完整性约束,举个例子： 假如某个电脑生产商,它的数据库中保存着整机和配件的产品信息.用来保存整机产品信息的表叫做 Pc;用来保存配件供货信息的表叫做Parts. 在Pc表中有一个字段,用来描述这款电脑所使用的CPU型号; 在Parts 表中相应有一个字段,描述的正是CPU的型号,我们可以把它想成是全部CPU的型号列表. 很显然,这个厂家生产的电脑,其使用的CPU一定是供货信息表(parts)中存在的型号.这时,两个表中就存在一种约束关系(constraint)——Pc表中的CPU型号受到Parts 表中型号的约束.被指定为外键的列必须要有索引,外键参考列必须为令一个表的主键; 创建表时指定外键例如user(id,username)和表article(id,uid,title),其中article.uid时外键指向user.id主键 123456CREATE TABLE article ( id INT auto_increment, uid INT, PRIMARY KEY (id), CONSTRAINT fk_user_article_uid FOREIGN KEY (uid) REFERENCES USER (id)); 注意：如果外键列没有索引,则MySQL会自动为其添加一个和外键同名的索引 修改表时添加外键约束1ALTER TABLE article ADD CONSTRAINT fk_user_article_uid FOREIGN KEY (uid) REFERENCES USER (id); 删除外键1ALTER TABLE article DROP FOREIGN KEY fk_user_article_uid; 注意：删除外键时,该外键对应的所索引并不会被删除 索引 普通索引这是最基本的索引类型,而且它没有唯一性之类的限制.普通索引可以通过以下几种方式创建： 建表时创建索引12CREATE INDEX &lt;索引的名字&gt; ON tablename (列的列表);CREATE INDEX name ON user (username); 修改表时添加索引12ALTER TABLE tablename ADD INDEX [索引的名字] (列的列表);ALTER TABLE USER ADD INDEX username (username); 唯一性索引这种索引和前面的”普通索引”基本相同,但有一个区别：索引列的所有值都只能出现一次,即必须唯一.唯一性索引可以用以下几种方式创建: 建表时创建索引12CREATE UNIQUE INDEX &lt;索引的名字&gt; ON tablename (列的列表);CREATE UNIQUE INDEX name ON user (username); 修改表时添加索引12ALTER TABLE tablename ADD UNIQUE INDEX [索引的名字] (列的列表);ALTER TABLE USER ADD UNIQUE INDEX username (username); 主键索引上文介绍过,此处略 全文索引MySQL从3.23.23版开始支持全文索引和全文检索.在MySQL中,全文索引的索引类型为FULLTEXT.全文索引可以在VARCHAR或者TEXT类型的列上创建.它可以通过CREATE TABLE命令创建,也可以通过ALTER TABLE或CREATE INDEX命令创建.对于大规模的数据集,通过ALTER TABLE（或者CREATE INDEX）命令创建全文索引要比把记录插入带有全文索引的空表更快 索引的其他说明 单列索引与多列索引索引可以是单列索引,也可以是多列索引. 添加索引1create INDEX 索引名称 ON tablename (row1,row2,row3); 修改索引1ALTER TABLE tablename ADD INDEX 索引名称 (row1,row2,row3); 最左前缀多列索引还有另外一个优点,它通过称为最左前缀（Leftmost Prefixing）的概念体现出来,我们举个列子： 现在我们有一个name、age、phone列上的多列索引,我们称这个索引为indexs.当搜索条件是以下各种列的组合时,MySQL将使用indexs索引： 1234name,age,phonename,agename 从另一方面理解,它相当于我们创建了(name、age、phone)、(name,age)以及(name)这些列组合上的索引.下面这些查询都能够使用这个indexs索引： 1234SELECT * FROM user WHERE user=&apos;Mike&apos; AND age=&apos;18&apos; AND phone=&apos;117&apos;; SELECT * FROM user WHERE user=&apos;Mike&apos; AND age=&apos;19&apos;; SELECT * FROM user WHERE user=&apos;Mike&apos; AND age=&apos;20&apos;; 如何选择索引列在性能优化过程中,选择在哪些列上创建索引是最重要的步骤之一.可以考虑使用索引的主要有两种类型的列：在WHERE子句中出现的列,在join子句中出现的列.请看下面这个查询： 1234567SELECT age ## 不使用索引 FROM user WHERE name =&apos;Mike&apos; ## 考虑使用索引 AND age=&apos;18&apos; ## 考虑使用索引SELECT article.title, ##不使用索引 user.name ##不使用索引 FROM user LEFT JOIN article ON user.id=article.uid ##考虑使用索引 WHERE user.phone=&apos;229&apos; ##考虑使用索引 AND nickname=&apos;mma&apos; ##考虑使用索引","categories":[{"name":"Mysql","slug":"Mysql","permalink":"https://blog.zhimma.com/categories/Mysql/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://blog.zhimma.com/tags/Mysql/"}]},{"title":"常用的Git","slug":"常用的git","date":"2018-11-29T16:00:00.000Z","updated":"2019-01-24T09:21:39.933Z","comments":true,"path":"2018/11/30/常用的git/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/常用的git/","excerpt":"Git的概念Git是一款免费、开源的分布式版本控制系统，用于敏捷高效地处理任何或小或大的项目。Git是一个开源的分布式版本控制系统，可以有效、高速的处理从很小到非常大的项目版本管理。","text":"Git的概念Git是一款免费、开源的分布式版本控制系统，用于敏捷高效地处理任何或小或大的项目。Git是一个开源的分布式版本控制系统，可以有效、高速的处理从很小到非常大的项目版本管理。 工作原理 Workspace（工作区）： 执行git add *命令就把改动提交到了暂存区，执行git pull命令将远程仓库的数据拉到当前分支并合并，执行git checkout [branch-name]切换分支 Index（暂存区）： 执行git commit -m ‘说明’ 命令就把改动提交到了仓库区（当前分支） Repository（仓库区或本地仓库）： 执行git push origin master提交到远程仓库，执行git clone 地址将克隆远程仓库到本地 Remote（远程仓库）： 就是类似github，coding等网站所提供的仓库 Git术语仓库（Repository）：一个仓库包括了所有的版本信息、所有的分支和标记信息。在Git中仓库的每份拷贝都是完整的。仓库让你可以从中取得你的工作副本 分支（Branches）：一个分支意味着一个独立的、拥有自己历史信息的代码线（code line）。你可以从已有的代码中生成一个新的分支，这个分支与剩余的分支完全独立。默认的分支往往是叫master。用户可以选择一个分支，选择一个分支执行命令git checkout branch 标记（Tags）：一个标记指的是某个分支某个特定时间点的状态。通过标记，可以很方便的切换到标记时的状态，例如2016年11月17号在testing分支上的代码状态 提交（Commit）：提交代码后，仓库会创建一个新的版本。这个版本可以在后续被重新获得。每次提交都包括作者和提交者，作者和提交者可以是不同的人 修订（Revision）：用来表示代码的一个版本状态。Git通过用SHA1 hash算法表示的id来标识不同的版本。每一个 SHA1 id都是160位长，16进制标识的字符串.。最新的版本可以通过HEAD来获取。之前的版本可以通过”HEAD~1”来获取，以此类推 开始使用创建仓库12345678# 在当前目录新建一个Git代码库$ git init # 新建一个目录，将其初始化为Git代码库$ git init [project-name] # 下载一个项目和它的整个代码历史（各个分支提交记录等）$ git clone [url] git init后会出现.git文件夹，里面有配置文件，如果没有git bash里面输入ls -lah就可以看到了 配置Git的设置文件为.gitconfig，它可以在用户主目录下（全局配置），也可以在项目目录下（项目配置）123456789# 显示当前的Git配置$ git config --list # 编辑Git配置文件，只是配置用户信息的话直接看下面两行命令即可$ git config -e [--global] # 设置提交代码时的用户信息$ git config [--global] user.name &quot;[name]&quot;$ git config [--global] user.email &quot;[email address]&quot; 增加删除文件123456789101112131415161718192021# 添加指定文件到暂存区$ git add [file1] [file2] ... # 添加指定目录到暂存区，包括子目录$ git add [dir] # 添加当前目录的所有文件到暂存区$ git add . # 添加每个变化前，都会要求确认# 对于同一个文件的多处变化，可以实现分次提交$ git add -p # 删除工作区文件，并且将这次删除放入暂存区$ git rm [file1] [file2] ... # 停止追踪指定文件，但该文件会保留在工作区$ git rm --cached [file] # 改名文件，并且将这个改名放入暂存区$ git mv [file-original] [file-renamed] 提交文件123456789101112131415161718# 提交暂存区到仓库区$ git commit -m [message] # 提交暂存区的指定文件到仓库区$ git commit [file1] [file2] ... -m [message] # 提交工作区自上次commit之后的变化，直接到仓库区$ git commit -a # 提交时显示所有diff信息$ git commit -v # 使用一次新的commit，替代上一次提交# 如果代码没有任何新变化，则用来改写上一次commit的提交信息$ git commit --amend -m [message] # 重做上一次commit，并包括指定文件的新变化$ git commit --amend [file1] [file2] ... 推送远程服务器1234567891011121314151617181920212223# 下载远程仓库的所有变动$ git fetch [remote] # 显示所有远程仓库$ git remote -v # 显示某个远程仓库的信息$ git remote show [remote] # 增加一个新的远程仓库，并命名$ git remote add [shortname] [url] # 取回远程仓库的变化，并与本地分支合并$ git pull [remote] [branch] # 上传本地指定分支到远程仓库$ git push [remote] [branch] # 强行推送当前分支到远程仓库，即使有冲突$ git push [remote] --force # 推送所有分支到远程仓库$ git push [remote] --all 撤销12345678910111213141516171819202122232425262728293031# 恢复暂存区的指定文件到工作区$ git checkout [file] # 恢复某个commit的指定文件到暂存区和工作区$ git checkout [commit] [file] # 恢复暂存区的所有文件到工作区$ git checkout . # 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变$ git reset [file] # 重置暂存区与工作区，与上一次commit保持一致$ git reset --hard # 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变$ git reset [commit] # 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致$ git reset --hard [commit] # 重置当前HEAD为指定commit，但保持暂存区和工作区不变$ git reset --keep [commit] # 新建一个commit，用来撤销指定commit# 后者的所有变化都将被前者抵消，并且应用到当前分支$ git revert [commit] # 暂时将未提交的变化移除，稍后再移入$ git stash$ git stash pop 分支123456789101112131415161718192021222324252627282930313233343536373839404142# 列出所有本地分支$ git branch # 列出所有远程分支$ git branch -r # 列出所有本地分支和远程分支$ git branch -a # 新建一个分支，但依然停留在当前分支$ git branch [branch-name] # 新建一个分支，并切换到该分支$ git checkout -b [branch] # 新建一个分支，指向指定commit$ git branch [branch] [commit] # 新建一个分支，与指定的远程分支建立追踪关系$ git branch --track [branch] [remote-branch] # 切换到指定分支，并更新工作区$ git checkout [branch-name] # 切换到上一个分支$ git checkout - # 建立追踪关系，在现有分支与指定的远程分支之间$ git branch --set-upstream [branch] [remote-branch] # 合并指定分支到当前分支$ git merge [branch] # 选择一个commit，合并进当前分支$ git cherry-pick [commit] # 删除分支$ git branch -d [branch-name] # 删除远程分支$ git push origin --delete [branch-name]$ git branch -dr [remote/branch] 标签1234567891011121314151617181920212223242526# 列出所有tag$ git tag # 新建一个tag在当前commit$ git tag [tag] # 新建一个tag在指定commit$ git tag [tag] [commit] # 删除本地tag$ git tag -d [tag] # 删除远程tag$ git push origin :refs/tags/[tagName] # 查看tag信息$ git show [tag] # 提交指定tag$ git push [remote] [tag] # 提交所有tag$ git push [remote] --tags # 新建一个分支，指向某个tag$ git checkout -b [branch] [tag] 查看信息12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# 显示有变更的文件$ git status # 显示当前分支的版本历史$ git log # 显示commit历史，以及每次commit发生变更的文件$ git log --stat # 搜索提交历史，根据关键词$ git log -S [keyword] # 显示某个commit之后的所有变动，每个commit占据一行$ git log [tag] HEAD --pretty=format:%s # 显示某个commit之后的所有变动，其&quot;提交说明&quot;必须符合搜索条件$ git log [tag] HEAD --grep feature # 显示某个文件的版本历史，包括文件改名$ git log --follow [file]$ git whatchanged [file] # 显示指定文件相关的每一次diff$ git log -p [file] # 显示过去5次提交$ git log -5 --pretty --oneline # 显示所有提交过的用户，按提交次数排序$ git shortlog -sn # 显示指定文件是什么人在什么时间修改过$ git blame [file]$ git blame [file] # 显示暂存区和工作区的差异$ git diff # 显示暂存区和上一个commit的差异$ git diff --cached [file] # 显示工作区与当前分支最新commit之间的差异$ git diff HEAD # 显示两次提交之间的差异$ git diff [first-branch]...[second-branch] # 显示今天你写了多少行代码$ git diff --shortstat &quot;@&#123;0 day ago&#125;&quot; # 显示某次提交的元数据和内容变化$ git show [commit] # 显示某次提交发生变化的文件$ git show --name-only [commit] # 显示某次提交时，某个文件的内容$ git show [commit]:[filename] # 显示当前分支的最近几次提交$ git reflog 附录 点击下载 git for windows客户端 安装教程","categories":[{"name":"Git","slug":"Git","permalink":"https://blog.zhimma.com/categories/Git/"}],"tags":[{"name":"Git","slug":"Git","permalink":"https://blog.zhimma.com/tags/Git/"}]},{"title":"抽象类和接口","slug":"抽象类和接口","date":"2018-11-29T16:00:00.000Z","updated":"2019-02-01T07:21:47.575Z","comments":true,"path":"2018/11/30/抽象类和接口/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/抽象类和接口/","excerpt":"","text":"来自这里https://blog.csdn.net/sunlylorn/article/details/6124319 抽象类和接口抽象类 一个类中如果有一个方法是抽象方法，那么这个类必须定义为抽象类 抽象类是指在 class 前加了 abstract 关键字且存在抽象方法（在类方法 function 关键字前加了 abstract 关键字）的类。 抽象类不能直接实例化，抽象类中只定义（或部分实现）子类需要的方法。子类可以通过继承抽象类并通过实现抽象类中的所有抽象方法，使抽象类具体化。 如果子类需要实例化，前提是它实现了抽象类中的所有抽象方法。如果子类没有全部实现抽象类中的所有抽象方法，那么该子类也是一个抽象类，必须在 class 前面加上 abstract 关键字，并且不能被实例化。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647abstract class Service&#123; /** 抽象类中可以定义变量 */ protected $value1 = 0; private $value2 = 1; public $value3 = 2; /** * 大多数情况下，抽象类至少含有一个抽象方法。抽象方法用abstract关键字声明，其中不能有具体内容。 * 可以像声明普通类方法那样声明抽象方法，但是要以分号而不是方法体结束。也就是说抽象方法在抽象类中不能被实现，也就是没有函数体“&#123;some codes&#125;”。 */ abstract public function say(); abstract public function talk(); /** 也可以定义非抽象方法 */ public function run() &#123; echo \"run function\" .PHP_EOL; &#125;&#125;abstract class A extends Service&#123; public function say() &#123; echo \"say function\" .PHP_EOL; &#125; public function jump() &#123; $this-&gt;run(); &#125;&#125;class MiniA extends A&#123; public function talk() &#123; echo \"talk function \".PHP_EOL; &#125; public function other() &#123; $this-&gt;jump(); $this-&gt;say(); &#125;&#125;$class = new MiniA();$class-&gt;talk();$class-&gt;other(); ​ 接口类PHP接口类interface就是一个类的领导者，指明方向，子类必须完成它指定方法 抽象类提供了具体实现的标准，而接口则是纯粹的模版。接口只定义功能，而不包含实现的内容。接口用关键字 interface 来声明。 interface 是完全抽象的，只能声明方法，而且只能声明 public 的方法，不能声明 private 及 protected 的方法，不能定义方法体，也不能声明实例变量 。然而， interface 却可以声明常量变量 。但将常量变量放在 interface 中违背了其作为接口的作用而存在的宗旨，也混淆了 interface 与类的不同价值。如果的确需要，可以将其放在相应的 abstract class 或 Class 中。 1234567interface Bar&#123; const NAME = 'zhimma'; public function say(); public function talk(); &#125; echo Bar:: NAME; ​ 任何实现接口的类都要实现接口中所定义的所有方法,否则该类必须声明为 abstract 。 1234567891011121314class A implements Bar&#123; public function say() &#123; // TODO: Implement say() method. &#125; public function talk() &#123; // TODO: Implement talk() method. &#125;&#125;abstract class X implements Bar&#123;&#125; ​ 一个类可以在声明中使用 implements 关键字来实现某个接口。这么做之后，实现接口的具体过程和继承一个仅包含抽象方法的抽象类是一样的。一个类可以同时继承一个父类和实现任意多个接口。 extends 子句应该在 implements 子句之前。 PHP 只支持继承自一个父类，因此 extends 关键字后只能跟一个类名。 12345678910111213class Foo&#123;&#125;;class B extends Foo implements Bar&#123; public function say() &#123; // TODO: Implement say() method. &#125; public function talk() &#123; // TODO: Implement talk() method. &#125;&#125; ​ 接口不可以实现另一个接口，但可以继承多个 ​ ​ 抽象类对比接口相同点 两者都是抽象类，都不能实例化。 interface 实现类及 abstract class 的子类都必须要实现已经声明的抽象方法。 不同点 interface 需要实现，要用 implements ，而 abstract class 需要继承，要用 extends 。 一个类可以实现多个 interface ，但一个类只能继承一个 abstract class interface 强调特定功能的实现，而 abstract class 强调所属关系。 尽管 interface 实现类及 abstract class 的子类都必须要实现相应的抽象方法，但实现的形式不同。 interface 中的每一个方法都是抽象方法，都只是声明的 (declaration, 没有方法体 ) ，实现类必须要实现。而 abstract class 的子类可以有选择地实现。 abstract class 中并非所有的方法都是抽象的，只有那些冠有 abstract 的方法才是抽象的，子类必须实现。那些没有 abstract 的方法，在 abstract class 中必须定义方法体 abstract class 的子类在继承它时，对非抽象方法既可以直接继承，也可以覆盖；而对抽象方法，可以选择实现，也可以留给其子类来实现，但此类必须也声明为抽象类。既是抽象类，当然也不能实例化。 abstract class 是 interface 与 class 的中介。 abstract class 在 interface 及 class 中起到了承上启下的作用。一方面， abstract class 是抽象的，可以声明抽象方法，以规范子类必须实现的功能；另一方面，它又可以定义缺省的方法体，供子类直接使用或覆盖。另外，它还可以定义自己的实例变量，以供子类通过继承来使用。 接口中的抽象方法前不用也不能加 abstract 关键字，默认隐式就是抽象方法，也不能加 final关键字来防止抽象方法的继承。而抽象类中抽象方法前则必须加上 abstract 表示显示声明为抽象方法。 接口中的抽象方法默认是 public 的，也只能是 public 的，不能用 private ， protected 修饰符修饰。而抽象类中的抽象方法则可以用 public ， protected 来修饰，但不能用 private 应用场合interface 的应用场合 类与类之间需要特定的接口进行协调，而不在乎其如何实现 作为能够实现特定功能的标识存在，也可以是什么接口方法都没有的纯粹标识。 需要将一组类视为单一的类，而调用者只通过接口来与这组类发生联系。 需要实现特定的多项功能，而这些功能之间可能完全没有任何联系。 abstract 的应用场合在既需要统一的接口，又需要实例变量或缺省的方法的情况下，就可以使用它 定义了一组接口，但又不想强迫每个实现类都必须实现所有的接口。可以用 abstract class 定义一组方法体，甚至可以是空方法体，然后由子类选择自己所感兴趣的方法来覆盖 某些场合下，只靠纯粹的接口不能满足类与类之间的协调，还必需类中表示状态的变量来区别不同的关系。 abstract 的中介作用可以很好地满足这一点。 规范了一组相互协调的方法，其中一些方法是共同的，与状态无关的，可以共享的，无需子类分别实现；而另一些方法却需要各个子类根据自己特定的状态来实现特 定的功能 。","categories":[{"name":"PHP","slug":"PHP","permalink":"https://blog.zhimma.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://blog.zhimma.com/tags/PHP/"},{"name":"面向对象","slug":"面向对象","permalink":"https://blog.zhimma.com/tags/面向对象/"}]},{"title":"搞懂JWT","slug":"搞懂JWT","date":"2018-11-29T16:00:00.000Z","updated":"2019-01-24T09:22:15.529Z","comments":true,"path":"2018/11/30/搞懂JWT/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/搞懂JWT/","excerpt":"","text":"搞懂JWT本文基本一字不差的转载至这里 JSON Web Token（JWT）是一个非常轻巧的规范。这个规范允许我们使用JWT在用户和服务器之间传递安全可靠的信息 让我们来假想一下一个场景。在A用户关注了B用户的时候，系统发邮件给B用户，并且附有一个链接“点此关注A用户”。链接的地址可以是这样的https://your.awesome-app.com/make-friend/?from_user=B&amp;target_user=A上面的URL主要通过URL来描述这个当然这样做有一个弊端，那就是要求用户B用户是一定要先登录的。可不可以简化这个流程，让B用户不用登录就可以完成这个操作。JWT就允许我们做到这点。 JWT的组成一个JWT实际上就是一个字符串，它由三部分组成，头部、载荷与签名 载荷（Payload）我们先将上面的添加好友的操作描述成一个JSON对象。其中添加了一些其他的信息，帮助今后收到这个JWT的服务器理解这个JWT。1234567891011121314151617&#123; \"iss\": \"John Wu JWT\", \"iat\": 1441593502, \"exp\": 1441594722, \"aud\": \"www.example.com\", \"sub\": \"jrocket@example.com\", \"from_user\": \"B\", \"target_user\": \"A\"&#125; 这里面的前五个字段都是由JWT的标准所定义的。 iss: 该JWT的签发者 sub: 该JWT所面向的用户 aud: 接收该JWT的一方 exp(expires): 什么时候过期，这里是一个Unix时间戳 iat(issued at): 在什么时候签发的 这些定义都可以在标准中找到。将上面的JSON对象进行[base64编码]可以得到下面的字符串。这个字符串我们将它称作JWT的Payload（载荷）。 eyJpc3MiOiJKb2huIFd1IEpXVCIsImlhdCI6MTQ0MTU5MzUwMiwiZXhwIjoxNDQxNTk0NzIyLCJhdWQiOiJ3d3cuZXhhbXBsZS5jb20iLCJzdWIiOiJqcm9ja2V0QGV4YW1wbGUuY29tIiwiZnJvbV91c2VyIjoiQiIsInRhcmdldF91c2VyIjoiQSJ9 如果你使用Node.js，可以用Node.js的包base64url来得到这个字符串1234567var base64url = require('base64url')var header = &#123; \"from_user\": \"B\", \"target_user\": \"A\"&#125;console.log(base64url(JSON.stringify(header)))// 输出：eyJpc3MiOiJKb2huIFd1IEpXVCIsImlhdCI6MTQ0MTU5MzUwMiwiZXhwIjoxNDQxNTk0NzIyLCJhdWQiOiJ3d3cuZXhhbXBsZS5jb20iLCJzdWIiOiJqcm9ja2V0QGV4YW1wbGUuY29tIiwiZnJvbV91c2VyIjoiQiIsInRhcmdldF91c2VyIjoiQSJ9 小知识：Base64是一种编码，也就是说，它是可以被翻译回原来的样子来的。它并不是一种加密过程。 头部（Header）JWT还需要一个头部，头部用于描述关于该JWT的最基本的信息，例如其类型以及签名所用的算法等。这也可以被表示成一个JSON对象。1234567&#123; \"typ\": \"JWT\", \"alg\": \"HS256\"&#125; 在这里，我们说明了这是一个JWT，并且我们所用的签名算法（后面会提到）是HS256算法。对它也要进行Base64编码，之后的字符串就成了JWT的Header（头部）。eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9 签名（签名）将上面的两个编码后的字符串都用句号.连接在一起（头部在前），就形成了yJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJmcm9tX3VzZXIiOiJCIiwidGFyZ2V0X3VzZXIiOiJBIn0这一部分的过程在node-jws的源码中有体现 最后，我们将上面拼接完的字符串用HS256算法进行加密。在加密的时候，我们还需要提供一个密钥（secret）。如果我们用mystar作为密钥的话，那么就可以得到我们加密后的内容rSWamyAYwuHCo7IFAgd1oRpSP7nzL7BF5t7ItqpKViM这一部分又叫做签名。 最后将这一部分签名也拼接在被签名的字符串后面，我们就得到了完整的JWTeyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJmcm9tX3VzZXIiOiJCIiwidGFyZ2V0X3VzZXIiOiJBIn0.rSWamyAYwuHCo7IFAgd1oRpSP7nzL7BF5t7ItqpKViM 于是，我们就可以将邮件中的URL改成https://your.awesome-app.com/make-friend/?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJmcm9tX3VzZXIiOiJCIiwidGFyZ2V0X3VzZXIiOiJBIn0.rSWamyAYwuHCo7IFAgd1oRpSP7nzL7BF5t7ItqpKViM这样就可以安全地完成添加好友的操作了！且慢，我们一定会有一些问题： 签名的目的是什么？ Base64是一种编码，是可逆的，那么我的信息不就被暴露了吗？让我逐一为你说明。 签名的目的最后一步签名的过程，实际上是对头部以及载荷内容进行签名。一般而言，加密算法对于不同的输入产生的输出总是不一样的。对于两个不同的输入，产生同样的输出的概率极其地小（有可能比我成世界首富的概率还小）。所以，我们就把“不一样的输入产生不一样的输出”当做必然事件来看待吧。 所以，如果有人对头部以及载荷的内容解码之后进行修改，再进行编码的话，那么新的头部和载荷的签名和之前的签名就将是不一样的。而且，如果不知道服务器加密的时候用的密钥的话，得出来的签名也一定会是不一样的。 服务器应用在接受到JWT后，会首先对头部和载荷的内容用同一算法再次签名。那么服务器应用是怎么知道我们用的是哪一种算法呢？别忘了，我们在JWT的头部中已经用alg字段指明了我们的加密算法了。 如果服务器应用对头部和载荷再次以同样方法签名之后发现，自己计算出来的签名和接受到的签名不一样，那么就说明这个Token的内容被别人动过的，我们应该拒绝这个Token，返回一个HTTP 401 Unauthorized响应。 信息会暴露？是的。 所以，在JWT中，不应该在载荷里面加入任何敏感的数据。在上面的例子中，我们传输的是用户的User ID。这个值实际上不是什么敏感内容，一般情况下被知道也是安全的。 但是像密码这样的内容就不能被放在JWT中了。如果将用户的密码放在了JWT中，那么怀有恶意的第三方通过Base64解码就能很快地知道你的密码了。 JWT的适用场景我们可以看到，JWT适合用于向Web应用传递一些非敏感信息。例如在上面提到的完成加好友的操作，还有诸如下订单的操作等等。 其实JWT还经常用于设计用户认证和授权系统，甚至实现Web应用的单点登录。","categories":[{"name":"PHP","slug":"PHP","permalink":"https://blog.zhimma.com/categories/PHP/"}],"tags":[{"name":"JWT","slug":"JWT","permalink":"https://blog.zhimma.com/tags/JWT/"}]},{"title":"搬瓦工CentOs7 实现BBR加速以及SS安装","slug":"搬瓦工Cenots7 实现BBR加速以及SS安装","date":"2018-11-29T16:00:00.000Z","updated":"2019-01-24T09:22:49.546Z","comments":true,"path":"2018/11/30/搬瓦工Cenots7 实现BBR加速以及SS安装/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/搬瓦工Cenots7 实现BBR加速以及SS安装/","excerpt":"","text":"准备环境: centos7 64位1、一键安装Shadowsock 下载脚本 wget --no-check-certificate https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks-libev.sh 增加执行权限 chmod +x shadowsocks-libev.sh 运行 ./shadowsocks-libev.sh 2&gt;&amp;1 | tee shadowsocks-libev.log 安装过程中会提示配置端口、密码、加密方式。 卸载: ./shadowsocks-libev.sh uninstall ss控制 1234启动：/etc/init.d/shadowsocks start 停止：/etc/init.d/shadowsocks stop 重启：/etc/init.d/shadowsocks restart 查看状态：/etc/init.d/shadowsocks status 2、安装BBR加速 目前支持的Linux系统包括：Ubuntu 14.04 x64、Ubuntu 16.04 x64、CentOS 6 x64、CentOS 7 x64 只支持 64 位系统，要求 glibc 版本 2.14 以上。 关闭防火墙 121、systemctl disable firewalld2、systemctl stop firewalld BBR安装脚本 1231、wget https://raw.githubusercontent.com/kuoruan/shell-scripts/master/ovz-bbr/ovz-bbr-installer.sh2、chmod +x ovz-bbr-installer.sh3、./ovz-bbr-installer.sh 安装过程中，会提示加速端口(可以更改) 判断BBR是否正常工作 ping 10.0.0.2 如果能通，则代表启动成功 控制bbr 1systemctl &#123;start|stop|restart|status&#125; haproxy-lkl 配置bbr加速端口 1vi /usr/local/haproxy-lkl/etc/port-rules 一行一个端口，可写范围 ​ 卸载BBR ​ 作者：我是你的nobita 链接：https://www.jianshu.com/p/9f27d4cabd40 來源：简书 简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。","categories":[{"name":"CentOs","slug":"CentOs","permalink":"https://blog.zhimma.com/categories/CentOs/"}],"tags":[{"name":"搬瓦工","slug":"搬瓦工","permalink":"https://blog.zhimma.com/tags/搬瓦工/"}]},{"title":"没有可用软件包nginx","slug":"没有可用软件包nginx","date":"2018-11-29T16:00:00.000Z","updated":"2019-02-01T07:22:19.330Z","comments":true,"path":"2018/11/30/没有可用软件包nginx/","link":"","permalink":"https://blog.zhimma.com/2018/11/30/没有可用软件包nginx/","excerpt":"","text":"今天安装Nginx时候使用：yum install nginx，报出了没有可用软件包 nginx，我的环境是CentOS7,在网上查询后，贴出解决方法 ngixn官方解决方法 先创建yum的一个repository文件：/etc/yum.repos.d/nginx.repo； 将下面配置粘贴保存进去 12345[nginx]name=nginx repobaseurl=http://nginx.org/packages/centos/$releasever/$basearch/gpgcheck=0enabled=1 执行命令 1234(1)安装Nginxyum install nginx(2)启动nginxservice nginx start / systemctl start nginx.service","categories":[{"name":"Nginx","slug":"Nginx","permalink":"https://blog.zhimma.com/categories/Nginx/"},{"name":"CentOS","slug":"CentOS","permalink":"https://blog.zhimma.com/categories/CentOS/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://blog.zhimma.com/tags/Nginx/"}]},{"title":"elk stack实践","slug":"elk stack实践","date":"2017-06-12T11:37:45.000Z","updated":"2019-01-24T08:17:14.328Z","comments":true,"path":"2017/06/12/elk stack实践/","link":"","permalink":"https://blog.zhimma.com/2017/06/12/elk stack实践/","excerpt":"","text":"环境说明：真实机IP：192.168.1.198，其中 服务器 IP 说明 Redis-server 172.17.0.3 nginx服务器1 Project1(logstash) 172.17.0.2 nginx服务器2 Project2(logstash) 172.17.0.3 服务器1 Elk 172.17.0.6 服务器2 1234567docker run -it -d --privileged=true --name redis-master -p 63791:6379 -p 221:22 -v /Users/zhimma/Data/www/:/data/www/ 67793c412ed1 /usr/sbin/initdocker run -it -d --privileged=true --name project1 -p 50441:5044 -p 8081:80 -p 10241:1024 -p 222:22 -v /Users/zhimma/Data/www/:/data/www/ 67793c412ed1 /usr/sbin/initdocker run -it -d --privileged=true --name project2 -p 50442:5044 -p 8082:80 -p 10242:1024 -p 223:22 -v /Users/zhimma/Data/www/:/data/www/ 67793c412ed1 /usr/sbin/initdocker run -it -d --privileged=true --name elk -p 50443:5044 -p 15602:5601 -p 224:22 -p 8083:80 -p 10243:1024 -v /Users/zhimma/Data/www/:/data/www/ 67793c412ed1 /usr/sbin/init 123456789101112docker psCONTAINER ID PORTS NAMES3f139b00a661 3306/tcp, 6379/tcp, 0.0.0.0:224-&gt;22/tcp, 0.0.0.0:8083-&gt;80/tcp, 0.0.0.0:10243-&gt;1024/tcp, 0.0.0.0:50443-&gt;5044/tcp, 0.0.0.0:15602-&gt;5601/tcp elke08ca55e124d 3306/tcp, 6379/tcp, 0.0.0.0:223-&gt;22/tcp, 0.0.0.0:8082-&gt;80/tcp, 0.0.0.0:10242-&gt;1024/tcp, 0.0.0.0:50442-&gt;5044/tcp project23b670e7a9ad1 3306/tcp, 6379/tcp, 0.0.0.0:222-&gt;22/tcp, 0.0.0.0:8081-&gt;80/tcp, 0.0.0.0:10241-&gt;1024/tcp, 0.0.0.0:50441-&gt;5044/tcp project1dbefa01b3393 80/tcp, 3306/tcp, 0.0.0.0:221-&gt;22/tcp, 0.0.0.0:63791-&gt;6379/tcp redis-master 所有服务器关闭防火墙 将软件和配置文件放在宿主机目录，各个容器就可以共享使用了 redis-master安装logstash服务器安装redis,进行配置,开机启动 project服务器安装logstash安装java环境1yum install java ###下载logstash 12345mkdir /opt/downloadsmkdir /opt/softcd /opt/downloadswget https://artifacts.elastic.co/downloads/logstash/logstash-6.4.1.tar.gztar -zxvf logstash-6.4.1.tar.gz -C /opt/soft/ 配置logstash1234vi /opt/soft/logstash-6.4.0/config/jvm.options-Xms2g-Xmx2g 安装配置supervisor 参考：https://blog.csdn.net/donggege214/article/details/80264811 vi /etc/supervisord.conf 123456[unix_http_server]file=/var/run/supervisor/supervisor.sock chmod=0700chown=root:root[include]files = supervisord.d/*.conf vi /etc/supervisord/l.conf 12345678[program:elk-l]command=/opt/soft/logstash-6.4.0/bin/logstash -r -f /data/www/elk/conf/project/*.confautostart=trueautorestart=trueuser=rootredirect_stderr=truestdout_logfile=/var/log/elk/l.logpriority=10 vi /data/www/elk/conf/project/project.conf 12345678910111213141516171819202122232425262728293031input &#123; file &#123; path =&gt; [ &quot;/data/www/project-mdl/trunk/Common/Runtime/Apps/*.log&quot; ] start_position =&gt; &quot;beginning&quot; ignore_older =&gt; 0 sincedb_path =&gt; &quot;/dev/null&quot; type =&gt; &quot;Api&quot; codec =&gt; multiline &#123; pattern =&gt; &quot;^\\[&quot; negate =&gt; true what =&gt; &quot;previous&quot; &#125; &#125;&#125;filter &#123;&#125;output &#123; if [type] == &quot;Api&quot; &#123; redis &#123; host =&gt; &apos;192.168.1.198&apos; port =&gt; &apos;63791&apos; db =&gt; &apos;1&apos; data_type =&gt; &quot;list&quot; key =&gt; &quot;project&quot; &#125; &#125; stdout &#123; codec =&gt; rubydebug &#125;&#125; 重启supervisor，如果数据写入redis-master服务器，那么就代表项目日志收集成功 ELK服务器安装java环境1yum install java 安装elk123456789mkdir /opt/downloadsmkdir /opt/softcd /opt/downloadswget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.4.1.tar.gzwget https://artifacts.elastic.co/downloads/kibana/kibana-6.4.1-linux-x86_64.tar.gzwget https://artifacts.elastic.co/downloads/logstash/logstash-6.4.1.tar.gztar -zxvf logstash-6.4.1.tar.gz -C /opt/soft/tar -zxvf elasticsearch-6.4.1.tar.gz -C /opt/soft/tar -zxvf kibana-6.4.1-linux-x86_64.tar.gz -C /opt/soft/ 配置elk创建elastic用户由于 Elasticsearch 不允许也不推荐使用 root 用户来运行，因此需要新建一个用户来启动 Elasticsearch。 12adduser elastic #创建elastic用户passwd elastic #修改elastic密码 创建ES数据日志文件夹12345cd /data/www/elkmkdir data #创建数据目录mkdir log #创建日志目录mkdir bak #创建备份目录chown -R elatic /data/www/elk/ #修改目录拥有者为 elastic 优化文件句柄数以及用户可用进程数新版 Elasticsearch 要求其可用的文件句柄至少为 65536，同时要求其进程数限制至少为 2048，可用按照下面的指令进行修改。 分别对应以下两个报错信息： max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536]； max number of threads [1024] for user [es] is too low, increase to at least [2048]。 12345678vim /etc/security/limits.conf* soft nofile 655350* hard nofile 655350* soft nproc 4096* hard nproc 8192elastic soft memlock unlimitedelastic hard memlock unlimited 修改内核交换为了避免不必要的磁盘和内存交换，影响效率，需要将 vm.swappiness 修改为 1。 此外需要修改最大虚拟内存 vm.max_map_count 防止启动时报错：max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144] 123456vim /etc/sysctl.confvm.swappiness = 1vm.max_map_count = 655360sysctl -p # 立即生效 关闭swap并且重启12swapoff -areboot 配置 Elasticsearch 内存占用12345cd /opt/soft/elasticsearch-6.4.1/config/vim jvm.options -Xms2g-Xmx2g 配置 Elasticsearch配置文件12345678[root@3f139b00a661 ~]# grep -n &apos;^[a-z]&apos; /opt/soft/elasticsearch-6.4.0/config/elasticsearch.yml17:cluster.name: elk-demo33:path.data: /data/www/elk/data37:path.logs: /data/www/logs43:bootstrap.memory_lock: false55:network.host: 0.0.0.059:http.port: 9200 安装配置supervisor grep -n &#39;^[a-z]&#39; /etc/supervisord.conf 12345678910111213144:file=/var/run/supervisor/supervisor.sock ; (the path to the socket file)5:chmod=0700 ; sockef file mode (default 0700)6:chown=root:root ; socket file uid:gid owner16:logfile=/var/log/supervisor/supervisord.log ; (main log file;default $CWD/supervisord.log)17:logfile_maxbytes=50MB ; (max main logfile bytes b4 rotation;default 50MB)18:logfile_backups=10 ; (num of main logfile rotation backups;default 10)19:loglevel=info ; (log level;default info; others: debug,warn,trace)20:pidfile=/var/run/supervisord.pid ; (supervisord pidfile;default supervisord.pid)21:nodaemon=false ; (start in foreground if true;default false)22:minfds=1024 ; (min. avail startup file descriptors;default 1024)23:minprocs=200 ; (min. avail process descriptors;default 200)37:supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface40:serverurl=unix:///var/run/supervisor/supervisor.sock ; use a unix:// URL for a unix socket129:files = /etc/supervisord.d/elk.conf grep &#39;^[a-z]&#39; /etc/supervisord.d/elk.conf 123456789101112131415161718192021command=/opt/soft/elasticsearch-6.4.0/bin/elasticsearchautostart=trueautorestart=trueuser=elasticredirect_stderr=truestdout_logfile=/var/log/elk/e.logpriority=1command=/opt/soft/logstash-6.4.0/bin/logstash -r -f /data/www/elk/conf/elk/*.confautostart=trueautorestart=trueuser=elasticredirect_stderr=truestdout_logfile=/var/log/elk/l.logpriority=10command=/opt/soft/kibana-6.4.0-linux-x86_64/bin/kibanaautostart=trueautorestart=trueuser=elasticredirect_stderr=truestdout_logfile=/var/log/elk/k.logpriority=20 cat /data/www/elk/conf/elk/elk.conf 123456789101112131415161718192021222324252627input &#123; redis &#123; host =&gt; &apos;192.168.1.198&apos; port =&gt; 63791 db =&gt; 1 data_type =&gt; &quot;list&quot; key =&gt; &quot;project&quot; &#125; stdin &#123; codec =&gt; multiline &#123; pattern =&gt; &quot;^\\[&quot; negate =&gt; true what =&gt; &quot;previous&quot; &#125; &#125;&#125;filter &#123;&#125;output &#123; elasticsearch &#123; hosts =&gt; [ &quot;127.0.0.1:9200&quot; ] index =&gt; &quot;project&quot; &#125; stdout &#123; codec =&gt; rubydebug &#125;&#125;","categories":[{"name":"ELK","slug":"ELK","permalink":"https://blog.zhimma.com/categories/ELK/"}],"tags":[{"name":"ELK Stack","slug":"ELK-Stack","permalink":"https://blog.zhimma.com/tags/ELK-Stack/"}]},{"title":"正向代理和反向代理","slug":"正向代理和反向代理","date":"2017-06-12T11:37:45.000Z","updated":"2019-02-01T07:34:15.483Z","comments":true,"path":"2017/06/12/正向代理和反向代理/","link":"","permalink":"https://blog.zhimma.com/2017/06/12/正向代理和反向代理/","excerpt":"","text":"概念正向代理正向代理是一个位于客户端和目标服务器之间的代理服务器（中间服务器）。为了从原始服务器取得内容，客户端向代理服务器发送一个请求，并且指定目标服务器，之后代理向目标服务器转交并且将获得的内容返回给客户端。正向代理的情况下客户端必须要进行一些特别的设置才能使用。 反向代理反向代理正好相反。对于客户端来说，反向代理就好像目标服务器。并且客户端不需要进行任何设置。客户端向反向代理发送请求，接着反向代理判断请求走向何处，并将请求转交给客户端，使得这些内容就好似他自己一样，一次客户端并不会感知到反向代理后面的服务，也因此不需要客户端做任何设置，只需要把反向代理服务器当成真正的服务器就好了。 区别访问形式区分：正向代理需要你主动设置代理服务器ip或者域名进行访问，由设置的服务器ip或者域名去获取访问内容并返回；正向代理是代理客户端，为客户端收发请求，使真实客户端对服务器不可见；反向代理不需要你做任何设置，直接访问服务器真实ip或者域名，但是服务器内部会自动根据访问内容进行跳转及内容返回，你不知道它最终访问的是哪些机器。反向代理是代理服务器端，为服务器收发请求，使真实服务器对客户端不可见。从上面的描述也能看得出来正向代理和反向代理最关键的两点区别： 是否指定目标服务器 客户端是否要做设置正向代理中，proxy和client同属一个LAN，对server透明； 反向代理中，proxy和server同属一个LAN，对client透明。 实际上proxy在两种代理中做的事都是代为收发请求和响应，不过从结构上来看正好左右互换了下，所以把前者那种代理方式叫做正向代理，后者叫做反向代理。用途上来区分：正向代理：正向代理用途是为了在防火墙内的局域网提供访问internet的途径。另外还可以使用缓冲特性减少网络使用率反向代理：反向代理的用途是将防火墙后面的服务器提供给internet用户访问。同时还可以完成诸如负载均衡等功能从安全性来讲：正向代理：正向代理允许客户端通过它访问任意网站并且隐蔽客户端自身，因此你必须采取安全措施来确保仅为经过授权的客户端提供服务反向代理：对外是透明的，访问者并不知道自己访问的是代理。对访问者而言，他以为访问的就是原始服务器使用场景正向代理从上面的介绍也就可以猜出来正向代理的至少一个功能（俗称翻墙），也即： 用户A无法访问facebook，但是能访问服务器B，而服务器B可以访问facebook。于是用户A访问服务器B，通过服务器B去访问facebook，，服务器B收到请求后，去访问facebook，facebook把响应信息返回给服务器B，服务器B再把响应信息返回给A。这样，通过代理服务器B，就实现了翻墙。 反向代理从上面的介绍也可以猜出来反向代理的至少一个功能（比如负载均衡），也即： 假设用户A访问 http://www.somesite.com/something.html，但www.somesite.com上并不存在something.html页面，于是接收用户请求的该服务器就偷偷从另外一台服务器上取回来，然后返回给用户，而用户并不知道something.html页面究竟位于哪台机器上。 反向代理的作用就比较多了，这里简单列举一下： 保护和隐藏原始资源服务器 加密和SSL加速 负载均衡 缓存静态内容 压缩 减速上传 安全 外网发布 下面做两个简单介绍 保护和隐藏原始资源服务器用户A始终认为它访问的是原始服务器B而不是代理服务器Z，但实用际上反向代理服务器接受用户A的应答，从原始资源服务器B中取得用户A的需求资源，然后发送给用户A。由于防火墙的作用，只允许代理服务器Z访问原始资源服务器B。尽管在这个虚拟的环境下，防火墙和反向代理的共同作用保护了原始资源服务器B，但用户A并不知情。 负载均衡当反向代理服务器不止一个的时候，我们甚至可以把它们做成集群，当更多的用户访问资源服务器B的时候，让不同的代理服务器Z（x）去应答不同的用户，然后发送不同用户需要的资源。 透明代理透明代理比较类似正向代理的功能，差别在于客户端根本不知道代理的存在，它改编你的request，并会传送真实IP（使用场景就是公司限制网络的访问）。 比如为了工作效率或者安全，A公司屏蔽了QQ软件的使用。A公司的员工接上了网络，但发现无法使用qq。这就是透明代理捣的鬼。公司在内网和外网的中间插入一个透明代理，这个代理会根据规则抓取请求内容，遇到qq的请求我就把这个请求给屏蔽掉，这样就完成了透明屏蔽。当然了，如果你明白原理，就可以自己搞个正向代理来绕过公司的屏蔽。","categories":[{"name":"CentOS","slug":"CentOS","permalink":"https://blog.zhimma.com/categories/CentOS/"}],"tags":[{"name":"CentOS","slug":"CentOS","permalink":"https://blog.zhimma.com/tags/CentOS/"}]},{"title":"设计模式-控制反转及其依赖注入(番外篇)","slug":"设计模式-控制反转及其依赖注入（番外篇）","date":"2017-05-07T09:36:10.000Z","updated":"2018-09-29T11:08:49.333Z","comments":true,"path":"2017/05/07/设计模式-控制反转及其依赖注入（番外篇）/","link":"","permalink":"https://blog.zhimma.com/2017/05/07/设计模式-控制反转及其依赖注入（番外篇）/","excerpt":"上面有2篇文章介绍了PHP依赖注入和控制反转的概念和实例,为了加深理解才有这篇文章。 首先,我们假设,我们要开发一个组件命名为SomeComponent.这个组件中现在要注入一个数据库连接。在这个例子中,数据库连接在component中被创建,这种方法不可取,这样做的话,我们将不能改变数据库连接参数及数据库类型等一些参数。","text":"上面有2篇文章介绍了PHP依赖注入和控制反转的概念和实例,为了加深理解才有这篇文章。 首先,我们假设,我们要开发一个组件命名为SomeComponent.这个组件中现在要注入一个数据库连接。在这个例子中,数据库连接在component中被创建,这种方法不可取,这样做的话,我们将不能改变数据库连接参数及数据库类型等一些参数。 1234567891011121314151617181920212223242526&lt;?php class SomeComponent&#123; /** * The instantiation of the connection is hardcoded inside * the component so is difficult to replace it externally * or change its behavior */ public function someDbTask() &#123; $connection = new Connection(array( &quot;host&quot; =&gt; &quot;localhost&quot;, &quot;username&quot; =&gt; &quot;root&quot;, &quot;password&quot; =&gt; &quot;secret&quot;, &quot;dbname&quot; =&gt; &quot;invo&quot; )); // ... &#125; &#125; $some = new SomeComponent();$some-&gt;someDbTask(); 为了解决上面所说的问题,我们需要在使用前创建一个外部连接,并注入到容器中。就目前而言,这看起来是一个很好的解决方案： 1234567891011121314151617181920212223242526272829303132333435363738&lt;?php class SomeComponent&#123; protected $_connection; /** * Sets the connection externally */ public function setConnection($connection) &#123; $this-&gt;_connection = $connection; &#125; public function someDbTask() &#123; $connection = $this-&gt;_connection; // ... &#125; &#125; $some = new SomeComponent(); //Create the connection$connection = new Connection(array( &quot;host&quot; =&gt; &quot;localhost&quot;, &quot;username&quot; =&gt; &quot;root&quot;, &quot;password&quot; =&gt; &quot;secret&quot;, &quot;dbname&quot; =&gt; &quot;invo&quot;)); //Inject the connection in the component$some-&gt;setConnection($connection); $some-&gt;someDbTask(); 现在我们来考虑一个问题,我们在应用程序中的不同地方使用此组件,将多次创建数据库连接。使用一种类似全局注册表的方式,从这获得一个数据库连接实例,而不是使用一次就创建一次。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;?php class Registry&#123; /** * Returns the connection */ public static function getConnection() &#123; return new Connection(array( &quot;host&quot; =&gt; &quot;localhost&quot;, &quot;username&quot; =&gt; &quot;root&quot;, &quot;password&quot; =&gt; &quot;secret&quot;, &quot;dbname&quot; =&gt; &quot;invo&quot; )); &#125; &#125; class SomeComponent&#123; protected $_connection; /** * Sets the connection externally */ public function setConnection($connection)&#123; $this-&gt;_connection = $connection; &#125; public function someDbTask() &#123; $connection = $this-&gt;_connection; // ... &#125; &#125; $some = new SomeComponent(); //Pass the connection defined in the registry$some-&gt;setConnection(Registry::getConnection()); $some-&gt;someDbTask(); 现在,让我们来想像一下,我们必须在组件中实现两个方法,首先需要创建一个新的数据库连接,第二个总是获得一个共享连接：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384&lt;?php class Registry&#123; protected static $_connection; /** * Creates a connection */ protected static function _createConnection() &#123; return new Connection(array( &quot;host&quot; =&gt; &quot;localhost&quot;, &quot;username&quot; =&gt; &quot;root&quot;, &quot;password&quot; =&gt; &quot;secret&quot;, &quot;dbname&quot; =&gt; &quot;invo&quot; )); &#125; /** * 单例模式 * Creates a connection only once and returns it */ public static function getSharedConnection() &#123; if (self::$_connection===null)&#123; $connection = self::_createConnection(); self::$_connection = $connection; &#125; return self::$_connection; &#125; /** * Always returns a new connection */ public static function getNewConnection() &#123; return self::_createConnection(); &#125; &#125; class SomeComponent&#123; protected $_connection; /** * Sets the connection externally */ public function setConnection($connection)&#123; $this-&gt;_connection = $connection; &#125; /** * This method always needs the shared connection */ public function someDbTask() &#123; $connection = $this-&gt;_connection; // ... &#125; /** * This method always needs a new connection */ public function someOtherDbTask($connection) &#123; &#125; &#125; $some = new SomeComponent(); //This injects the shared connection$some-&gt;setConnection(Registry::getSharedConnection()); $some-&gt;someDbTask(); //Here, we always pass a new connection as parameter$some-&gt;someOtherDbTask(Registry::getConnection()); 到此为止,我们已经看到了如何使用依赖注入解决我们的问题。不是在代码内部创建依赖关系,而是让其作为一个参数传递,这使得我们的程序更容易维护,降低程序代码的耦合度,实现一种松耦合。但是从长远来看,这种形式的依赖注入也有一些缺点。 例如,如果组件中有较多的依赖关系,我们需要创建多个setter方法传递,或创建构造函数进行传递。另外,每次使用组件时,都需要创建依赖组件,使代码维护不太易,我们编写的代码可能像这样： 12345678910111213141516171819&lt;?php //Create the dependencies or retrieve them from the registry$connection = new Connection();$session = new Session();$fileSystem = new FileSystem();$filter = new Filter();$selector = new Selector(); //Pass them as constructor parameters$some = new SomeComponent($connection, $session, $fileSystem, $filter, $selector); // ... or using setters $some-&gt;setConnection($connection);$some-&gt;setSession($session);$some-&gt;setFileSystem($fileSystem);$some-&gt;setFilter($filter);$some-&gt;setSelector($selector); 我想,我们不得不在应用程序的许多地方创建这个对象。如果你不需要依赖的组件后,我们又要去代码注入部分移除构造函数中的参数或者是setter方法。为了解决这个问题,我们再次返回去使用一个全局注册表来创建组件。但是,在创建对象之前,它增加了一个新的抽象层： 1234567891011121314151617181920212223&lt;?php class SomeComponent&#123; // ... /** * Define a factory method to create SomeComponent instances injecting its dependencies */ public static function factory() &#123; $connection = new Connection(); $session = new Session(); $fileSystem = new FileSystem(); $filter = new Filter(); $selector = new Selector(); return new self($connection, $session, $fileSystem, $filter, $selector); &#125; &#125; 这一刻,我们好像回到了问题的开始,我们正在创建组件内部的依赖,我们每次都在修改以及找寻一种解决问题的办法,但这都不是很好的做法。 一种实用和优雅的来解决这些问题,是使用容器的依赖注入,像我们在前面看到的,容器作为全局注册表,使用容器的依赖注入做为一种桥梁来解决依赖可以使我们的代码耦合度更低,很好的降低了组件的复杂性： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&lt;?php class SomeComponent&#123; protected $_di; public function __construct($di) &#123; $this-&gt;_di = $di; &#125; public function someDbTask() &#123; // Get the connection service // Always returns a new connection $connection = $this-&gt;_di-&gt;get(&apos;db&apos;); &#125; public function someOtherDbTask() &#123; // Get a shared connection service, // this will return the same connection everytime $connection = $this-&gt;_di-&gt;getShared(&apos;db&apos;); //This method also requires a input filtering service $filter = $this-&gt;_db-&gt;get(&apos;filter&apos;); &#125; &#125; $di = new Phalcon\\DI(); //Register a &quot;db&quot; service in the container$di-&gt;set(&apos;db&apos;, function()&#123; return new Connection(array( &quot;host&quot; =&gt; &quot;localhost&quot;, &quot;username&quot; =&gt; &quot;root&quot;, &quot;password&quot; =&gt; &quot;secret&quot;, &quot;dbname&quot; =&gt; &quot;invo&quot; ));&#125;); //Register a &quot;filter&quot; service in the container$di-&gt;set(&apos;filter&apos;, function()&#123; return new Filter();&#125;); //Register a &quot;session&quot; service in the container$di-&gt;set(&apos;session&apos;, function()&#123; return new Session();&#125;); //Pass the service container as unique parameter$some = new SomeComponent($di); $some-&gt;someTask(); 现在,该组件只有访问某种service的时候才需要它,如果它不需要,它甚至不初始化,以节约资源。该组件是高度解耦。他们的行为,或者说他们的任何其他方面都不会影响到组件本身。我们的实现办法 Phalcon\\DI 是一个实现了服务的依赖注入功能的组件,它本身也是一个容器。由于Phalcon高度解耦,Phalcon\\DI 是框架用来集成其他组件的必不可少的部分,开发人员也可以使用这个组件依赖注入和管理应用程序中不同类文件的实例。 基本上,这个组件实现了 Inversion of Control 模式。基于此,对象不再以构造函数接收参数或者使用setter的方式来实现注入,而是直接请求服务的依赖注入。这就大大降低了整体程序的复杂性,因为只有一个方法用以获得所需要的一个组件的依赖关系。 此外,这种模式增强了代码的可测试性,从而使它不容易出错。在容器中注册服务框架本身或开发人员都可以注册服务。当一个组件A要求调用组件B(或它的类的一个实例),可以从容器中请求调用组件B,而不是创建组件B的一个实例。 这种工作方式为我们提供了许多优点： 我们可以更换一个组件,从他们本身或者第三方轻松创建。 在组件发布之前,我们可以充分的控制对象的初始化,并对对象进行各种设置。 我们可以使用统一的方式从组件得到一个结构化的全局实例 服务可以通过以下几种方式注入到容器： 1234567891011121314151617181920&lt;?php //Create the Dependency Injector Container$di = new Phalcon\\DI(); //By its class name$di-&gt;set(&quot;request&quot;, &apos;Phalcon\\Http\\Request&apos;); //Using an anonymous function, the instance will lazy loaded$di-&gt;set(&quot;request&quot;, function()&#123; return new Phalcon\\Http\\Request();&#125;); //Registering directly an instance$di-&gt;set(&quot;request&quot;, new Phalcon\\Http\\Request()); //Using an array definition$di-&gt;set(&quot;request&quot;, array( &quot;className&quot; =&gt; &apos;Phalcon\\Http\\Request&apos;)); 在上面的例子中,当向框架请求访问一个请求数据时,它将首先确定容器中是否存在这个”reqeust”名称的服务。容器会反回一个请求数据的实例,开发人员最终得到他们想要的组件。 在上面示例中的每一种方法都有优缺点,具体使用哪一种,由开发过程中的特定场景来决定的。用一个字符串来设定一个服务非常简单,但缺少灵活性。设置服务时,使用数组则提供了更多的灵活性,而且可以使用较复杂的代码。lambda函数是两者之间一个很好的平衡,但也可能导致更多的维护管理成本。 Phalcon\\DI 提供服务的延迟加载。除非开发人员在注入服务的时候直接实例化一个对象,然后存存储到容器中。在容器中,通过数组,字符串等方式存储的服务都将被延迟加载,即只有在请求对象的时候才被初始化 123456789101112131415161718192021222324&lt;?php //Register a service &quot;db&quot; with a class name and its parameters$di-&gt;set(&quot;db&quot;, array( &quot;className&quot; =&gt; &quot;Phalcon\\Db\\Adapter\\Pdo\\Mysql&quot;, &quot;parameters&quot; =&gt; array( &quot;parameter&quot; =&gt; array( &quot;host&quot; =&gt; &quot;localhost&quot;, &quot;username&quot; =&gt; &quot;root&quot;, &quot;password&quot; =&gt; &quot;secret&quot;, &quot;dbname&quot; =&gt; &quot;blog&quot; ) ))); //Using an anonymous function$di-&gt;set(&quot;db&quot;, function()&#123; return new Phalcon\\Db\\Adapter\\Pdo\\Mysql(array( &quot;host&quot; =&gt; &quot;localhost&quot;, &quot;username&quot; =&gt; &quot;root&quot;, &quot;password&quot; =&gt; &quot;secret&quot;, &quot;dbname&quot; =&gt; &quot;blog&quot; ));&#125;); 以上这两种服务的注册方式产生相同的结果。然后,通过数组定义的,在后面需要的时候,你可以修改服务参数： 1234567&lt;?php $di-&gt;setParameter(&quot;db&quot;, 0, array( &quot;host&quot; =&gt; &quot;localhost&quot;, &quot;username&quot; =&gt; &quot;root&quot;, &quot;password&quot; =&gt; &quot;secret&quot;)); 从容器中获得服务的最简单方式就是使用”get”方法,它将从容器中返回一个新的实例： 12&lt;?php $request = $di-&gt;get(&quot;request&quot;); 或者通过下面这种魔术方法的形式调用： 123&lt;?php $request = $di-&gt;getRequest(); Phalcon\\DI //同时允许服务重用,为了得到一个已经实例化过的服务,可以使用 getShared() 方法的形式来获得服务。 具体的 Phalcon\\Http\\Request 请求示例： 12&lt;?php $request = $di-&gt;getShared(&quot;request&quot;); 参数还可以在请求的时候通过将一个数组参数传递给构造函数的方式： 12&lt;?php $component = $di-&gt;get(&quot;MyComponent&quot;, array(&quot;some-parameter&quot;, &quot;other&quot;)) 复制完毕,本文引自这里","categories":[{"name":"PHP","slug":"PHP","permalink":"https://blog.zhimma.com/categories/PHP/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://blog.zhimma.com/tags/设计模式/"}]},{"title":"设计模式-控制反转及其依赖注入(2)","slug":"设计模式-控制反转及其依赖注入（2）","date":"2017-05-07T05:50:16.000Z","updated":"2018-09-29T11:08:49.333Z","comments":true,"path":"2017/05/07/设计模式-控制反转及其依赖注入（2）/","link":"","permalink":"https://blog.zhimma.com/2017/05/07/设计模式-控制反转及其依赖注入（2）/","excerpt":"上节我们介绍了控制反转及依赖注入的实现 最后在调用测试时： 12345678//用宝剑的英雄$class = new Hero(new Gun(&apos;倚天&apos;));$class-&gt;myWeapon();//我的倚天打起来唰唰唰~//用枪的英雄$class = new Hero(new Sword(&apos;沙漠之鹰&apos;));$class-&gt;myWeapon();//我的沙漠之鹰打起来砰砰砰~ 前言 我们看到,注入时需要实例化好所依赖的对象,再传到Hero类中,虽然通过依赖注入解决了解耦问题,但是在实际使用中,比较麻烦,因为每次都需要手动实例化依赖,再传递,这对于复杂大量的依赖关系,手动解决明显力不从心。因此,项目中需要一个自动化的依赖注入管理机制,这就是IoC容器; IoC容器：一个封装了依赖注入DI的框架,实现了动态创建注入依赖对象,管理依赖关系,管理对象声明周期等功能 核心实现,一般分为绑定(注册)对象生成器和创建对象注入依赖这两个核心步骤","text":"上节我们介绍了控制反转及依赖注入的实现 最后在调用测试时： 12345678//用宝剑的英雄$class = new Hero(new Gun(&apos;倚天&apos;));$class-&gt;myWeapon();//我的倚天打起来唰唰唰~//用枪的英雄$class = new Hero(new Sword(&apos;沙漠之鹰&apos;));$class-&gt;myWeapon();//我的沙漠之鹰打起来砰砰砰~ 前言 我们看到,注入时需要实例化好所依赖的对象,再传到Hero类中,虽然通过依赖注入解决了解耦问题,但是在实际使用中,比较麻烦,因为每次都需要手动实例化依赖,再传递,这对于复杂大量的依赖关系,手动解决明显力不从心。因此,项目中需要一个自动化的依赖注入管理机制,这就是IoC容器; IoC容器：一个封装了依赖注入DI的框架,实现了动态创建注入依赖对象,管理依赖关系,管理对象声明周期等功能 核心实现,一般分为绑定(注册)对象生成器和创建对象注入依赖这两个核心步骤 IoC容器 绑定(注册)对象生成器绑定：指的是将类于生成类对象的代码记录,对应,绑定起来。这样,在需要该类对象时,直接执行类对应的生成代码,就可以得到所需的对象; 1234567891011121314151617181920/** * 绑定类的生成器 * * @param $className 类名或者映射名,类的标志 * @param $generator 对应实例化或者可生成此类对象的代码 * * @throws \\Exception * * @author mma5694@gmail.com * @date 2017年5月7日00:42:02 */ public static function bind($className, $generator) &#123; //检测参数是否为合法的可调用结构 if (is_callable($generator)) &#123; self::$generatorList[$className] = $generator; &#125; else &#123; throw new \\Exception(&apos;对象生成器不是可调用的结构！&apos;); &#125; &#125; 注意bind方法,就是上面说的绑定(注册)对象生成器的实现,一般bind方法需要两个参数： 第一个就是类的标志,通常就是带有命名空间的类名称,也可以是自定义的类对应标志 第二个参数是一个匿名函数,也就是生成器,执行new的代码,这个参数可以是匿名函数、函数、类方法或者其他可执行的结构都是可以的 这样其实就是要用户提供类和该类对象的生成代码,将其对应,需要该类对象时再执行,但是注册时,并不调用生成类的代码,而仅仅时先存储起来(真精妙呀)。下面的例子就是将生成器代码存储到$generatorList数组中; 容器绑定对象生成器示例： 123456789101112131415use ClassFile\\Hero;use IImplements\\Sword;use IImplements\\Gun;//这里我第一个参数用的不带命名空间的类名IoContainer::bind(&apos;Gun&apos;, function ($title = &apos;&apos;) &#123; return new Gun($title);&#125;);IoContainer::bind(&apos;Sword&apos;, function ($title = &apos;&apos;) &#123; return new Sword($title);&#125;);IoContainer::bind(&apos;Hero&apos;, function ($module, $params = []) &#123; return new Hero(IoContainer::make($module, $params));&#125;); 调用bind()方法,提供类名和实例化类对象的匿名函数,我们的容器就会将类与生成器记录下来,等着需要时实例化生成所需对象 创建对象注入依赖方式1先看看完善后的IoContainer类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class IoContainer&#123; //定义存放类的容器？ protected static $generatorList = []; /** * 绑定类的生成器 * * @param $className 类名或者映射名,类的标志 * @param $generator 对应实例化或者可生成此类对象的代码 * * @throws \\Exception * * @author mma5694@gmail.com * @date 2017年5月7日00:42:02 */ public static function bind($className, $generator) &#123; //检测参数是否为合法的可调用结构 if (is_callable($generator)) &#123; self::$generatorList[$className] = $generator; &#125; else &#123; throw new \\Exception(&apos;对象生成器不是可调用的结构！&apos;); &#125; &#125; /** * 生成类的对象 * * @param $className * @param array $param * * @return mixed * @throws \\Exception * * @author mma5694@gmail.com * @date 2017年5月7日00:46:00 */ public static function make($className, $param = []) &#123; if (!isset(self::$generatorList[$className])) &#123; throw new \\Exception(&apos;类还没有绑定注册！&apos;); &#125; return call_user_func_array(self::$generatorList[$className], $param); &#125;&#125; 上面代码中的make方法就是用来生成对象的方法,该方法要获取所需的类,然后调用绑定时的生成器函数,来获取对象 通过make生成类对象： 1234$hero1 = IoContainer::make(&apos;Hero&apos;,[&apos;Sword&apos;,[&apos;屠龙刀&apos;]]);$hero1-&gt;myWeapon(); //我的屠龙刀打起来唰唰唰~$hero2 = IoContainer::make(&apos;Hero&apos;,[&apos;Gun&apos;,[&apos;AK-47&apos;]]);$hero2-&gt;myWeapon(); //我的AK-47打起来砰砰砰~ 方式2我们将所有的映射一一对应写入一个配置文件中,在容器类的构造方法中,引入配置文件中的所有对应关系,自动完成绑定(注册)对象生成器 1234567config.php return [ &apos;Sword&apos;=&gt;function($title=&apos;&apos;)&#123;return new \\IImplements\\Sword($title);&#125;, &apos;Gun&apos;=&gt;function($title=&apos;&apos;)&#123;return new \\IImplements\\Gun($title);&#125;, &apos;Hero&apos;=&gt;function($module,$params = [])&#123;return new \\ClassFile\\Hero(IoContainer\\IoContainer::make($module,$params));&#125;,]; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class IoContainer&#123; //定义存放类的容器？ protected static $generatorList = []; //配置文件注册 public function __construct() &#123; $config = dirname(dirname(__FILE__)) . &quot;/Config/config.php&quot;; $config = include $config; foreach ($config as $key =&gt; $value) &#123; if (is_callable($value)) &#123; self::$generatorList[$key] = $value; &#125; else &#123; throw new \\Exception(&apos;对象生成器不是可调用的结构！&apos;); &#125; &#125; &#125; /** * 绑定类的生成器 * * @param $className 类名或者映射名,类的标志 * @param $generator 对应实例化或者可生成此类对象的代码 * * @throws \\Exception * * @author mma5694@gmail.com * @date 2017年5月7日00:42:02 */ public static function bind($className, $generator) &#123; //检测参数是否为合法的可调用结构 if (is_callable($generator)) &#123; self::$generatorList[$className] = $generator; &#125; else &#123; throw new \\Exception(&apos;对象生成器不是可调用的结构！&apos;); &#125; &#125; /** * 生成类的对象 * * @param $className * @param array $param * * @return mixed * @throws \\Exception * * @author mma5694@gmail.com * @date 2017年5月7日00:46:00 */ public static function make($className, $param = []) &#123; if (!isset(self::$generatorList[$className])) &#123; throw new \\Exception(&apos;类还没有绑定注册！&apos;); &#125; return call_user_func_array(self::$generatorList[$className], $param); &#125;&#125; 这样就当调用容器类时,会自动绑定(注册)生成器 12345$container = new IoContainer();$hero1 = IoContainer::make(&apos;Hero&apos;,[&apos;Sword&apos;,[&apos;屠龙刀&apos;]]);$hero1-&gt;myWeapon(); //我的屠龙刀打起来唰唰唰~$hero2 = IoContainer::make(&apos;Hero&apos;,[&apos;Gun&apos;,[&apos;AK-47&apos;]]);$hero2-&gt;myWeapon(); //我的AK-47打起来砰砰砰~ 这样也是可以的; 上面的例子就完成了IoC容器的两个基本步骤：绑定和创建 结语 理解了什么是IoC容器, 本文的目的就达到了. 实际使用中(例如laravel)IoC容器的方法会有很多, 例如绑定构造器, 绑定对象实例, 绑定单例, 绑定接口实现等. 具体的使用就要到具体的框架或者产品中应用了 本文示例代码在这里,引用参考文章地址在这里","categories":[{"name":"PHP","slug":"PHP","permalink":"https://blog.zhimma.com/categories/PHP/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://blog.zhimma.com/tags/设计模式/"}]},{"title":"设计模式-控制反转及其依赖注入(1)","slug":"设计模式-控制反转及其依赖注入（1）","date":"2017-05-07T03:46:17.000Z","updated":"2018-09-29T11:08:49.333Z","comments":true,"path":"2017/05/07/设计模式-控制反转及其依赖注入（1）/","link":"","permalink":"https://blog.zhimma.com/2017/05/07/设计模式-控制反转及其依赖注入（1）/","excerpt":"摘要 IoC:Inversion of Control 控制反转 DI：Dependency Injection 依赖注入 IoC与DI","text":"摘要 IoC:Inversion of Control 控制反转 DI：Dependency Injection 依赖注入 IoC与DI 先记住这句话：IoC是设计模式,而DI是IoC控制反转设计模式的典型实现 IoC控制反转是一种设计模式,用来解决对象间的过度依赖问题。 解决思路是：设法不在依赖对象中去获取(new)被依赖对象,最典型的的实现方式就是DI依赖注入了。 将对象所依赖的其他对象,在类外部生成好之后,传递到类内部的,而不是在类的内部实例化。这种解决依赖的方法就是DI依赖注入。 例如：对象Hero依赖对象Sword,我们可以选择如下定义方式： 123456789101112131415161718// 英雄依赖宝剑的定义实现class Sword&#123; private $title; public function __construct($title) &#123; $this-&gt;title = $title; &#125;&#125;class Hero&#123; private $weapon; public function __construct() &#123; // Hero依赖Sword $this-&gt;weapon = new Sword(&apos;倚天剑&apos;); &#125;&#125; 上面的代码中,Hero类对象就依赖Sword对象,但是在此例子中,Hero类对象对于Sword类对象的依赖就比较严重,一旦这个Hero使用的不再是Sword,而是Gun了,Hero的内部方法就要重写,在复杂的程序中,是不可取得,需要降低Hero对武器(无论是Sword或者Gun亦或者是其他武器)的直接依赖 解决思路就是设法不在依赖对象中去获取需要依赖的对象,这种思路就是IoC控制反转。 把原来本应在类(对象)内部完成的依赖,设法在类(对象)外部完成,这个由内到外的转化过程就是反转 所以：IoC反转最典型的实现方式就是依赖注入DI,如下代码所示 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172/** * 武器接口 * Interface Weapon */interface Weapon&#123; public function __construct($title); /** * 进攻 */ public function attack();&#125; /** * 宝剑类 */class Sword implements Weapon&#123; protected $title; public function __construct($title) &#123; $this-&gt;title = $title; &#125; public function attack() &#123; return &quot;我的&#123;$this-&gt;title&#125;打起来唰唰唰~&quot;; &#125; /** * 枪类 */class Gun implements Weapon&#123; protected $title; public function __construct($title) &#123; $this-&gt;title = $title; &#125; public function attack() &#123; return &quot;我的&#123;$this-&gt;title&#125;打起来砰砰砰~&quot;; &#125;&#125; class Hero&#123; private $weapon; /** * * 构造方法的参数是一个对象,通过类型约束限制必须为实现Weapon武器接口的对象 * 在构造方法中,直接将参数传递进来的武器对象赋值到当前对象的属性上 * 这样,英雄Hero依赖的对象不是在Hero类的内部实例化,而是在外部实例化好,传递到Hero内部的 * * 这就是《依赖注入》,通俗来讲,就是所依赖的对象在外部生成好之后,传递到类内内部的,而不是在 * 类内部实例化,这种解决依赖的方法就是DI依赖注入 * * * * Hero constructor。 * * @param Weapon $weapon */ public function __construct(Weapon $weapon) &#123; $this-&gt;weapon = $weapon; &#125; public function myWeapon() &#123; echo $this-&gt;weapon-&gt;attack(); &#125; 调用测试： 12345678//用宝剑的英雄$class = new Hero(new Gun(&apos;倚天&apos;));$class-&gt;myWeapon();//我的倚天打起来唰唰唰~//用枪的英雄$class = new Hero(new Sword(&apos;沙漠之鹰&apos;));$class-&gt;myWeapon();//我的沙漠之鹰打起来砰砰砰~ 这样,无论Hero需要宝剑还是枪,都可以通过外部注入的方式,将武器传递给Hero对象 通过构造方法传递参数,是依赖注入最常用的形式,除此之外,还有属性赋值的方法,也可以完成依赖注入,例如： 1234567// class Hero&#123; public $weapon;&#125;$hero = new Hero;$hero-&gt;weapon = new Sword(&apos;倚天&apos;); 以上就是平时所说的依赖注入,有没有理解呢？ 结语 解决了什么是依赖注入的问题, 本篇的目的就达到了,(示例代码在这里,本文参考引用这里)但还远远不够 , 注意上面的使用Hero的代码, 我们是手动将实例化好的武器对象作为参数传递给Hero的构造方法的。 此时的问题就是, 当出现大量的, 随机的需要注入的依赖如何处理? 一个个的实例化传递, 是否够自动化? 要解决这个问题, 就出现了IoC容器。 IoC容器也称为服务容器。 主要就是解决依赖和注入的问题。 实现机制是通过预先将创建对象的代码绑定或注册到IoC容器中, 然后利用该IoC容器创建对象, 在创建对象的过程中, 通过分析对象所需要的依赖(一般利用反射机制), 将注册好的创建对象的代码注入到对象的构造方法中去, 从而完成自动解决这个依赖注入的问题。 非常智能。 下篇我会接着记录","categories":[{"name":"PHP","slug":"PHP","permalink":"https://blog.zhimma.com/categories/PHP/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://blog.zhimma.com/tags/设计模式/"}]},{"title":"计算机基础","slug":"计算机基础","date":"2017-02-27T08:58:28.000Z","updated":"2019-01-24T08:16:59.910Z","comments":true,"path":"2017/02/27/计算机基础/","link":"","permalink":"https://blog.zhimma.com/2017/02/27/计算机基础/","excerpt":"","text":"计算机基础title: 计算机基础date: 2018-03-18 23:20:16 进程与线程进程和线程都是一个时间段的描述，是CPU工作时间段的描述 一个最最基础的事实：CPU太快，太快，太快了，寄存器仅仅能够追的上他的脚步，RAM和别的挂在各总线上的设备完全是望其项背。那当多个任务要执行的时候怎么办呢？轮流着来?或者谁优先级高谁来？不管怎么样的策略，一句话就是在CPU看来就是轮流着来。一个必须知道的事实：执行一段程序代码，实现一个功能的过程介绍 ，当得到CPU的时候，相关的资源必须也已经就位，就是显卡啊，GPS啊什么的必须就位，然后CPU开始执行。这里除了CPU以外所有的就构成了这个程序的执行环境，也就是我们所定义的程序上下文。当这个程序执行完了，或者分配给他的CPU执行时间用完了，那它就要被切换出去，等待下一次CPU的临幸。在被切换出去的最后一步工作就是保存程序上下文，因为这个是下次他被CPU临幸的运行环境，必须保存。串联起来的事实：前面讲过在CPU看来所有的任务都是一个一个的轮流执行的，具体的轮流方法就是：先加载程序A的上下文，然后开始执行A，保存程序A的上下文，调入下一个要执行的程序B的程序上下文，然后开始执行B,保存程序B的上下文。。。 进程就是包换上下文切换的程序执行时间总和 = CPU加载上下文+CPU执行+CPU保存上下文进程的颗粒度太大，每次都要有上下的调入，保存，调出。如果我们把进程比喻为一个运行在电脑上的软件，那么一个软件的执行不可能是一条逻辑执行的，必定有多个分支和多个程序段，就好比要实现程序A，实际分成 a，b，c等多个块组合而成。那么这里具体的执行就可能变成： 程序A得到CPU =&gt; CPU加载上下文，开始执行程序A的a小段，然后执行A的b小段，然后再执行A的c小段，最后CPU保存A的上下文。 这里a，b，c的执行是共享了A的上下文，CPU在执行的时候没有进行上下文切换的。这里的a，b，c就是线程，也就是说线程是共享了进程的上下文环境，的更为细小的CPU时间段。 开个QQ，开了一个进程；开了迅雷，开了一个进程。在QQ的这个进程里，传输文字开一个线程、传输语音开了一个线程、弹出对话框又开了一个线程。所以运行某个软件，相当于开了一个进程。在这个软件运行的过程里（在这个进程里），多个工作支撑的完成QQ的运行，那么这“多个工作”分别有一个线程。所以一个进程管着多个线程。通俗的讲：“进程是爹妈，管着众多的线程儿子”… 一个进程可以包括多个线程 每个线程可以使用进程的共享内存（互斥锁） 操作系统的设计，因此可以归结为三点： 以多进程形式，允许多个任务同时运行； 以多线程形式，允许单个任务分成不同的部分运行； 提供协调机制，一方面防止进程之间和线程之间产生冲突，另一方面允许进程之间和线程之间共享资源。 异步，非阻塞和 IO 复用https://segmentfault.com/a/1190000007614502 同步与异步同步与异步的重点在消息通知的方式上，也就是调用结果通知的方式。 同步: 当一个同步调用发出去后，调用者要一直等待调用结果的通知后，才能进行后续的执行。异步：当一个异步调用发出去后，调用者不能立即得到调用结果的返回。异步调用，要想获得结果，一般有两种方式 主动轮询异步调用的结果; 被调用方通过callback来通知调用方调用结果。 demo：同步买奶茶：小明点单交钱，然后等着拿奶茶；异步买奶茶：小明点单交钱，店员给小明一个小票，等小明奶茶做好了，再来取。 异步买奶茶: 小明要想知道奶茶是否做好了，有两种方式： 小明主动去问店员，一会就去问一下：“奶茶做好了吗？”…直到奶茶做好。这叫轮训。 等奶茶做好了，店员喊一声：“小明，奶茶好了！”，然后小明去取奶茶。这叫回调。 阻塞与非阻塞阻塞与非阻塞的重点在于进/线程等待消息时候的行为，也就是在等待消息的时候，当前进/线程是挂起状态，还是非挂起状态。 阻塞调用在发出去后，在消息返回之前，当前进/线程会被挂起，直到有消息返回，当前进/线程才会被激活.非阻塞调用在发出去后，不会阻塞当前进/线程，而会立即返回。 demo：阻塞买奶茶：小明点单交钱，干等着拿奶茶，什么事都不做；非阻塞买奶茶：小明点单交钱，等着拿奶茶，等的过程中，时不时刷刷微博、朋友圈。 总结： 同步与异步，重点在于消息通知的方式; 阻塞与非阻塞，重点在于等消息时候的行为。 demo 同步阻塞：小明在柜台干等着拿奶茶； 同步非阻塞：小明在柜台边刷微博边等着拿奶茶； 异步阻塞：小明拿着小票啥都不干，一直等着店员通知他拿奶茶； 异步非阻塞：小明拿着小票，刷着微博，等着店员通知他拿奶茶。 IO复用在一个进程处理所有的并发I/O呢?答案是有的，这就是I/O复用技术。 最初级的I/O复用所谓的I/O复用，就是多个I/O可以复用一个进程。当一个连接过来时，我们不阻塞住，这样一个进程可以同时处理多个连接了。比如一个进程接受了10000个连接，这个进程每次从头到尾的问一遍这10000个连接：“有I/O事件没？有的话就交给我处理，没有的话我一会再来问一遍。”然后进程就一直从头到尾问这10000个连接，如果这1000个连接都没有I/O事件，就会造成CPU的空转，并且效率也很低 升级版的I/O复用上面虽然实现了基础版的I/O复用，但是效率太低了。于是伟大的程序猿们日思夜想的去解决这个问题…终于！我们能不能引入一个代理，这个代理可以同时观察许多I/O流事件呢？当没有I/O事件的时候，这个进程处于阻塞状态；当有I/O事件的时候，这个代理就去通知进程醒来？于是，早期的程序猿们发明了两个代理—select、poll。select、poll代理的原理是这样的： 当连接有I/O流事件产生的时候，就会去唤醒进程去处理。 但是进程并不知道是哪个连接产生的I/O流事件，于是进程就挨个去问：“请问是你有事要处理吗？”……问了99999遍，哦，原来是第100000个进程有事要处理。那么，前面这99999次就白问了，白白浪费宝贵的CPU时间片了 注:select与poll原理是一样的，只不过select只能观察1024个连接，poll可以观察无限个连接。 上面看了，select、poll因为不知道哪个连接有I/O流事件要处理，性能也挺不好的。 那么，如果发明一个代理，每次能够知道哪个连接有了I/O流事件，不就可以避免无意义的空转了吗？ 于是，超级无敌、闪闪发光的epoll被伟大的程序员发明出来了。 epoll代理的原理是这样的： 当连接有I/O流事件产生的时候，epoll就会去告诉进程哪个连接有I/O流事件产生，然后进程就去处理这个进程。 如此，多高效！","categories":[{"name":"PHP","slug":"PHP","permalink":"https://blog.zhimma.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://blog.zhimma.com/tags/PHP/"}]},{"title":"负载均衡实践","slug":"负载均衡实践","date":"2017-02-27T08:58:28.000Z","updated":"2019-01-24T08:16:58.958Z","comments":true,"path":"2017/02/27/负载均衡实践/","link":"","permalink":"https://blog.zhimma.com/2017/02/27/负载均衡实践/","excerpt":"","text":"引用参考：https://juejin.im/post/5821c24e570c350060bef4c3 环境说明：windows 下4台服务器，真实机IP：192.168.2.107，其中 服务器 IP 说明 nginx1 172.17.0.3 nginx服务器1 nginx2 172.17.0.4 nginx服务器2 server1 172.17.0.5 服务器1 server2 172.17.0.6 服务器2 server3 172.17.0.7 服务器3 1234567C:\\Users\\mma&gt;docker psCONTAINER ID PORTS NAMES73e6f2096612 6379/tcp, 0.0.0.0:201-&gt;22/tcp, 0.0.0.0:881-&gt;80/tcp, 0.0.0.0:3361-&gt;3306/tcp, 0.0.0.0:9004-&gt;9001/tcp nginx1805ef8d42fa6 6379/tcp, 0.0.0.0:202-&gt;22/tcp, 0.0.0.0:882-&gt;80/tcp, 0.0.0.0:3362-&gt;3306/tcp, 0.0.0.0:9005-&gt;9001/tcp nginx2e91b4a662023 6379/tcp, 0.0.0.0:203-&gt;22/tcp, 0.0.0.0:883-&gt;80/tcp, 0.0.0.0:3363-&gt;3306/tcp, 0.0.0.0:9006-&gt;9001/tcp server101bb4850cc8c 6379/tcp, 0.0.0.0:204-&gt;22/tcp, 0.0.0.0:884-&gt;80/tcp, 0.0.0.0:3364-&gt;3306/tcp, 0.0.0.0:9007-&gt;9001/tcp server2e500cbd1efad 6379/tcp, 0.0.0.0:205-&gt;22/tcp, 0.0.0.0:885-&gt;80/tcp, 0.0.0.0:3365-&gt;3306/tcp, 0.0.0.0:9008-&gt;9001/tcp server3 主要需要暴露http服务端口 负载均衡概念负载均衡，英文名称为Load Balance，其意思就是分摊到多个操作单元上进行执行，例如Web服务器、FTP服务器、企业关键应用服务器和其它关键任务服务器等，从而共同完成工作任务。 我们知道单台服务器的性能是有上限的，当流量很大时，就需要使用多台服务器来共同提供服务，这就是所谓的集群。 负载均衡服务器，就是用来把经过它的流量，按照某种方法，分配到集群中的各台服务器上。这样一来不仅可以承担更大的流量、降低服务的延迟，还可以避免单点故障造成服务不可用。一般的反向代理服务器，都具备负载均衡的功能。 负载均衡功能可以由硬件来提供，比如以前的F5设备。也可以由软件来提供，LVS可以提供四层的负载均衡(利用IP和端口)， 架构图 负载均衡策略加权轮询指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 图中有两点需要注意，第一，如果可以把加权轮询算法分为先深搜索和先广搜索，那么nginx采用的是先深搜索算法，即将首先将请求都分给高权重的机器，直到该机器的权值降到了比其他机器低，才开始将请求分给下一个高权重的机器；第二，当所有后端机器都down掉时，nginx会立即将所有机器的标志位清成初始状态，以避免造成所有的机器都处在timeout的状态，从而导致整个前端被夯住。 1234567891011121314151617http &#123; upstream zhimma &#123; server 192.168.2.107:883 weight=5; server 192.168.2.107:884 weight=6; server 192.168.2.107:885 weight=7; &#125; server&#123; listen 80; server_name zhimma.ma; location / &#123; proxy_pass http://zhimma; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; &#125;&#125; 除了 weight 之外，还有别的配置项 1234upstream phpServers &#123; server 192.168.2.107:883 weight=5 max_fails=1 fail_timeout=20 max_conns=100; server 192.168.2.107:883 weight=6 backup down&#125; max_fails 默认为1。某台Server允许请求失败的次数，超过最大次数后，在failtimeout时间内，新的请求将不会分配给这台机器。如果设置为0，Nginx会将这台Server置为永久无效状态，然后将请求发给定义了proxynextupstream, fastcginextupstream, uwsginextupstream, scginextupstream, and memcachednext_upstream指令来处理这次错误的请求。 fail_timeout 默认为10秒。某台Server达到maxfails次失败请求后，在failtimeout期间内，nginx会认为这台Server暂时不可用，不会将请求分配给它 backup 备份机，所有服务器挂了之后才会生效 down 标识某一台server不可用 max_conns 限制分配给某台Server处理的最大连接数量，超过这个数量，将不会分配新的连接给它。默认为0，表示不限制。注意：1.5.9之后的版本才有这个配置 表示最多给100这台Server分配1000个请求，如果这台Server正在处理1000个请求，nginx将不会分配新的请求给到它。假如有一个请求处理完了，还剩下999个请求在处理，这时nginx也会将新的请求分配给它。 3.IP HASHip_hash(ip绑定)每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 ip hash算法的核心实现如下： 12345for(i = 0;i &lt; 3;i++)&#123; hash = (hash * 113 + iphp-&gt;addr[i]) % 6271; &#125;p = hash % iphp-&gt;rrp.peers-&gt;number; 从代码中可以看出，hash值既与ip有关又与后端机器的数量有关。经过测试，上述算法可以连续产生1045个互异的value，这是该算法的硬限制。对此nginx使用了保护机制，当经过20次hash仍然找不到可用的机器时，算法退化成轮询。因此，从本质上说，ip hash算法是一种变相的轮询算法，如果两个ip的初始hash值恰好相同，那么来自这两个ip的请求将永远落在同一台服务器上，这为均衡性埋下了很深的隐患。 12345678910111213141516171819http &#123; upstream zhimma &#123; ip_hash; server 192.168.2.107:883; server 192.168.2.107:884; server 192.168.2.107:885; &#125; server&#123; listen 80; server_name zhimma.ma; location / &#123; proxy_pass http://zhimma; #如果服务器要获取客户端真实IP，可以用下三句设置主机头和客户端真实地址 proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; &#125;&#125; fair通用hash、一致性hashsession_sticky配置详情负载均衡服务器1234567891011121314151617181920212223242526272829303132333435363738user nginx;worker_processes 1;error_log /var/log/nginx/error.log warn;pid /var/run/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include /etc/nginx/mime.types; default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; upstream zhimma &#123; server 192.168.2.107:883 weight=3; server 192.168.2.107:884 weight=4; server 192.168.2.107:885 weight=5; &#125; server&#123; listen 80; server_name zhimma.ma; root /home/www/tourism/laravel_store/public; index index.html index.htm; location / &#123; proxy_pass http://zhimma; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125; &#125;# include /etc/nginx/conf.d/*.conf;&#125; 业务服务器1234567891011121314151617181920212223242526272829303132333435363738[root@e91b4a662023 conf.d]# pwd /etc/nginx/conf.d[root@e91b4a662023 conf.d]# lszhimma.ma.confroot@e91b4a662023 conf.d]# cat zhimma.ma.conf server &#123; listen 80; server_name zhimma.ma; ## Root and index files. # 这里的路径对应自己项目路径，因为我是做了目录挂载，所以剩下2台服务器nginx的配置做了区分 # 分别是/home/www/zhimma/server2和/home/www/zhimma/server3 root /home/www/zhimma/server1; index index.php index.html index.htm; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; location / &#123; # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # location ~ \\.php$ &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125; try_files $uri $uri/ /index.php$is_args$args; &#125; &#125; 重启nginx ，配置hosts,访问zhimma.ma:881,就能看的切换的效果了 ###","categories":[{"name":"PHP","slug":"PHP","permalink":"https://blog.zhimma.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://blog.zhimma.com/tags/PHP/"}]},{"title":"面向对象的特性","slug":"面向对象的特性","date":"2017-02-27T08:58:28.000Z","updated":"2019-01-24T08:16:58.079Z","comments":true,"path":"2017/02/27/面向对象的特性/","link":"","permalink":"https://blog.zhimma.com/2017/02/27/面向对象的特性/","excerpt":"","text":"来自这里https://www.cnblogs.com/zhyunfe/p/6398581.html、 [TOC] 面向对象的特性封装对事物的封装是指，将事物进行抽象后，提供抽象概念的实现的具体方法。 PHP也只是三种封装概念：Private，Protected，Public。 私有/Private私有的概念是，仅仅对象内部可见，外部不可见 保护/Protected 保护的概念是，仅仅是自身类和继承类可见，这个关键字的用途主要是防止滥用类的派生，另外三方库编写的时候会用到，防止误用。 继承继承性就是派生类(子类)自动继承一个或多个基类(父类)中的属性与方法，并可以重写或添加新的属性和方法。继承这个特性简化了对象和类的创建，增加了代码的可重性。继承分单继承和多继承，PHP所支持的是单继承，也就是说，一个子类有且只有一个父类。 多态多态是指在面向对象中能够根据使用类的上下文来重新定义或改变类的性质和行为同一个类的不同对象，使用同一个方法可以获得不同的结果。多态性增强了软件的灵活性和重用性。 五大基本原则单一职责原则SRP(Single Responsibility Principle)是指一个类的功能要单一，不能包罗万象。如同一个人一样，分配的工作不能太多，否则一天到晚虽然忙忙碌碌的，但效率却高不起来。 开放封闭原则OCP(Open－Close Principle)一个模块在扩展性方面应该是开放的而在更改性方面应该是封闭的。比如：一个网络模块，原来只服务端功能，而现在要加入客户端功能，那么应当在不用修改服务端功能代码的前提下，就能够增加客户端功能的实现代码，这要求在设计之初，就应当将服务端和客户端分开，公共部分抽象出来。 替换原则(the Liskov Substitution Principle LSP)子类应当可以替换父类并出现在父类能够出现的任何地方。比如：公司搞年度晚会，所有员工可以参加抽奖，那么不管是老员工还是新员工，也不管是总部员工还是外派员工，都应当可以参加抽奖，否则这公司就不和谐了。 依赖原则(the Dependency Inversion Principle DIP) 具体依赖抽象，上层依赖下层。假设B是较A低的模块，但B需要使用到A的功能，这个时候，B不应当直接使用A中的具体类： 而应当由B定义一抽象接口，并由A来实现这个抽象接口，B只使用这个抽象接口：这样就达到了依赖倒置的目的，B也解除了对A的依赖，反过来是A依赖于B定义的抽象接口。通过上层模块难以避免依赖下层模块，假如B也直接依赖A的实现，那么就可能造成循环依赖。一个常见的问题就是编译A模块时需要直接包含到B模块的cpp文件，而编译B时同样要直接包含到A的cpp文件。 接口分离原则(the Interface Segregation Principle ISP)模块间要通过抽象接口隔离开，而不是通过具体的类强耦合起来","categories":[{"name":"PHP","slug":"PHP","permalink":"https://blog.zhimma.com/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"https://blog.zhimma.com/tags/PHP/"}]},{"title":"sudo: sorry, you must have a tty to run sudo","slug":"sudo-sorry-you-must-have-a-tty-to-run-sudo","date":"2017-02-07T06:47:05.000Z","updated":"2018-09-29T11:08:49.330Z","comments":true,"path":"2017/02/07/sudo-sorry-you-must-have-a-tty-to-run-sudo/","link":"","permalink":"https://blog.zhimma.com/2017/02/07/sudo-sorry-you-must-have-a-tty-to-run-sudo/","excerpt":"","text":"今天使用一个非root管理员操作服务器时，使用sudo命令报出了 sudo: sorry, you must have a tty to run sudo 错误，查找资料后记录如下解决方法： 使用不同账户，执行执行脚本时候sudo经常会碰到 sudo: sorry, you must have a tty to run sudo这个情况 其实修改一下sudo的配置就好了 vi /etc/sudoers (最好用visudo命令)注释掉 Default requiretty 一行#Default requiretty 意思就是sudo默认需要tty终端。注释掉就可以在后台执行了。","categories":[{"name":"遇到的问题","slug":"遇到的问题","permalink":"https://blog.zhimma.com/categories/遇到的问题/"}],"tags":[{"name":"遇到的问题","slug":"遇到的问题","permalink":"https://blog.zhimma.com/tags/遇到的问题/"}]},{"title":"window10下docker的应用-创建本地环境的lnmp环境","slug":"window10下docker的应用-创建本地环境的lnmp环境","date":"2016-12-30T07:41:38.000Z","updated":"2019-01-24T09:20:38.040Z","comments":true,"path":"2016/12/30/window10下docker的应用-创建本地环境的lnmp环境/","link":"","permalink":"https://blog.zhimma.com/2016/12/30/window10下docker的应用-创建本地环境的lnmp环境/","excerpt":"docker相关1.下载docker for windows2.安装docker的步骤就不多说了，下一步下一步就行,安装之前记得先将windows自带的hyper-v开启&nbsp;","text":"docker相关1.下载docker for windows2.安装docker的步骤就不多说了，下一步下一步就行,安装之前记得先将windows自带的hyper-v开启&nbsp; 3.配置docker 设置共享磁盘 配置镜像仓库地址为阿里云（非必须） 1.在阿里云镜像仓库中申请自己的加速器&nbsp;&nbsp; 2.在docker中配置仓库地址，右键右下角docker中的设置功能进行设置&nbsp;&nbsp; 上面配置成功后，接下来进行docker的常用操作 4.打开windows的cmd或者git bash; 输入docker，出现下列信息表示安装成功5.docker命令 attach Attach to a running container # 当前 shell 下 attach 连接指定运行容器 build Build an image from a Dockerfile # 通过 Dockerfile 定制镜像 commit Create a new image from a container’s changes # 提交当前容器为新的镜像 cp Copy files/folders from the containers filesystem to the host path# 从容器中拷贝指定文件或者目录到宿主机中 create Create a new container # 创建一个新的容器，同 run，但不启动容器 diff Inspect changes on a container’s filesystem # 查看 docker 容器变化 events Get real time events from the server # 从 docker 服务获取容器实时事件 exec Run a command in an existing container # 在已存在的容器上运行命令 export Stream the contents of a container as a tar archive # 导出容器的内容流作为一个 tar 归档文件[对应 import ] history Show the history of an image # 展示一个镜像形成历史 images List images # 列出系统当前镜像 import Create a new filesystem image from the contents of a tarball # 从tar包中的内容创建一个新的文件系统映像[对应 export] info Display system-wide information # 显示系统相关信息 inspect Return low-level information on a container # 查看容器详细信息 kill Kill a running container # kill 指定 docker 容器 load Load an image from a tar archive # 从一个 tar 包中加载一个镜像[对应 save] login Register or Login to the docker registry server # 注册或者登陆一个 docker 源服务器 logout Log out from a Docker registry server # 从当前 Docker registry 退出 logs Fetch the logs of a container # 输出当前容器日志信息 port Lookup the public-facing port which is NAT-ed to PRIVATE_PORT# 查看映射端口对应的容器内部源端口 pause Pause all processes within a container # 暂停容器 ps List containers # 列出容器列表 pull Pull an image or a repository from the docker registry server# 从docker镜像源服务器拉取指定镜像或者库镜像 push Push an image or a repository to the docker registry server# 推送指定镜像或者库镜像至docker源服务器 rename Rename an existing container restart Restart a running container # 重启运行的容器 rm Remove one or more containers # 移除一个或者多个容器 rmi Remove one or more images # 移除一个或多个镜像[无容器使用该镜像才可删除，否则需删除相关容器才可继续或 -f 强制删除] run Run a command in a new container# 创建一个新的容器并运行一个命令 save Save an image to a tar archive # 保存一个镜像为一个 tar 包[对应 load] search Search for an image on the Docker Hub # 在 docker hub 中搜索镜像 start Start a stopped containers # 启动容器 stats Display a stream of a containers’ resource usage statistics stop Stop a running containers # 停止容器 tag Tag an image into a repository # 给源中镜像打标签 top Lookup the running processes of a container # 查看容器中运行的进程信息 unpause Unpause a paused container # 取消暂停容器 version Show the docker version information # 查看 docker 版本号 wait Block until a container stops, then print its exit code # 截取容器停止时的退出状态值&nbsp; 6.到此windows for docker 安装了解完成，下面进行容器的运行 查找镜像docker search centos&nbsp; pull一个镜像docker pull centos&nbsp;&nbsp; 将这个镜像启动成为一个容器,并映射端口、挂在本地的目录、后台启动、赋予最高权限docker run -it -d --privileged=true --name www -p 33060:3306 -p 220:22 -p 8080:80 -v e:\\WWW\\:/home/www centos /usr/sbin/init 解释一下主要参数的意义： -d&nbsp;&nbsp;&nbsp;&nbsp;以daemon方式运行 –privileged=true&nbsp;&nbsp;&nbsp;&nbsp;增加权限的选项（本人未测试） –name &nbsp;&nbsp;&nbsp;&nbsp;给运行的容器起别名（个人理解） -p 33060:3306 -p 220:22 -p 8080:80 &nbsp;&nbsp;&nbsp;&nbsp;进行端口映射 -v e:\\WWW\\:/home/www &nbsp;&nbsp;&nbsp;&nbsp;挂载宿主机的www目录到容器内部的www目录 运行后查看容器运行列表&nbsp; 进入容器并查看&nbsp;主要是查看挂在的目录是否存在&nbsp;)和本地目录一致，挂在成功！7 安装ifconfigyum install net-tools –y8 安装压缩解压缩yum install -y unzip zip9 安装firewall-cmdyum install firewalld systemd –y10 修改密码passwd11 安装ssh服务，使用工具与容器内进行通讯和管理 1 . 安装ssh服务 yum install openssh* 2 . 配置sshvi /etc/ssh/sshd_config 这里我没有设置，因为是初学，使用的默认配置,如果需要设置，请参考下面的说明 设置這些设定是一些比较基本的首先先把port改掉port 52041再来是限定登入者AllowUsers 使用者账号1 使用者账号2 …..这一行在设置中是沒有的~请自行加入再来把这两行的注释掉PermitEmptyPasswords noPasswordAuthentication yes再来限制root账号登录 3 .重启 systemctl restart sshd.service 4 .设置为开机启动sudo systemctl enable sshd.service 5 .开放防火墙端口firewall-cmd --permanent --zone=public --add-port=22/tcp 6 . 重启防火墙firewall-cmd --reload最后输入netstat -ant 看看PORT有沒有加入监控中 CentOS 7 的最小安装未把 netstat 安装进去，所以如果执行失败请输入sudo yum install net-toolsyum install firewalld systemd -y测试环境下也可以彻底关闭防火墙 关闭防火墙CentOS 7.0默认使用的是firewall作为防火墙 firewall： systemctl start firewalld.service#启动firewall systemctl stop firewalld.service#停止firewall systemctl disable firewalld.service#禁止firewall开机启动 12.测试ssh连接记住我们的端口映射好220=&gt;22 8080=&gt;80 33060=&gt;3306直接上图 查看本机ip&nbsp; 使用220端口连接容器&nbsp; 连接成功查看&nbsp; 13.将容器保存为镜像先退出容器，拿到需要保存为镜像的容器iddocker commit -m=&#39;ssh_network_vim&#39; --author=&#39;mma&#39; f3823b442695 ssh_network_vim -m 创建的镜像的提交信息 –author指定镜像作者，接着是容器ID、目标镜像仓库、镜像名&nbsp; ok，docker在windows上面的安装设置就到这里，下面进行lnmp环境的搭建 下面进行lnmp 环境的搭建 安装lnmp 请移步此教程：CentOS7☞lnmp环境搭建 正式使用 之前我安装过一次，并且配置完成提交到阿里云的仓库中了，这次的演示，我直接从阿里云下载之前我使用的镜像1.下载镜像 镜像信息&nbsp; 下载镜像docker pull registry.cn-hangzhou.aliyuncs.com/mma/centos7-lnmp:v1.0.0&nbsp;&nbsp;2.运行一个容器，并进入进入容器docker run -it -d --privileged=true --name www -p 33060:3306 -p 220:22 -p 8080:80 -v e:\\WWW\\:/home/www 84901c8f883b/usr/sbin/init&nbsp;3.启动lnmp4.windows访问容器中的项目 ok,这篇总结就到这里，总结下，写这篇总结收获很多，第一次使用hexo，第一次在windows环境模拟linux环境（纯属zhuangb用），阿里云方面，第一次使用markdown。。。 说实在的，还是很有感触，也是个开始，继续加油下去吧！mafuntoo","categories":[{"name":"Docker","slug":"Docker","permalink":"https://blog.zhimma.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://blog.zhimma.com/tags/Docker/"}]}]}